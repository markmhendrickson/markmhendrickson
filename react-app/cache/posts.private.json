[
  {
    "slug": "openai-api-memory-and-truth-layer",
    "title": "OpenAI API memory: What exists, what's missing, and when to add a truth layer",
    "excerpt": "The OpenAI API has conversation state and file search, but no first-class memory API. Here's what developers get, what they don't, and when a truth layer fills the gap.",
    "body": "OpenAI's developer API gives you conversation state and retrieval over files. It does not give you a dedicated memory API. Unlike [Anthropic's Claude Memory Tool](/posts/claude-memory-and-the-truth-layer)—where Claude reads and writes files in a `/memories` directory and you implement the backend—OpenAI has no equivalent. You get conversations, optional file search (vector stores), and the responsibility to persist and inject user context yourself. This post explains what exists, what's missing, and when a [truth layer](/posts/truth-layer-agent-memory) is a better fit. For the product-level experience (ChatGPT Memory), see [ChatGPT Memory](/posts/chatgpt-memory-and-truth-layer).\n\n## What the OpenAI API provides\n\n**Conversation state.** The [Responses API](https://developers.openai.com/api/docs/guides/responses-vs-chat-completions) (OpenAI's evolution of Chat Completions and the Assistants API) supports multi-turn context in two ways. You can pass message history manually with each request, or use the [Conversations API](https://platform.openai.com/docs/guides/conversation-state) to create a durable conversation object with a stable ID. Conversations store items—messages, tool calls, tool outputs—so you don't have to resend full history every turn. State is preserved across sessions and devices for that conversation. The `store: true` parameter keeps reasoning and tool context between turns. So: **conversation memory exists; user-level or cross-conversation memory does not.**\n\n**File search (vector stores).** You can attach [vector stores](https://platform.openai.com/docs/guides/tools-file-search) to assistants or threads (and in the new model, to prompts and conversations). Files are chunked, embedded with `text-embedding-3-large`, and retrieved by semantic and keyword search at query time. This is RAG over a knowledge base—documents, docs, codebase—not a place for the model to write user-specific memories. Retrieval is hosted by OpenAI (e.g. [pricing](https://platform.openai.com/docs/guides/tools-file-search) around storage and usage). The same [limits that RAG has for agent memory](/posts/why-agent-memory-needs-more-than-rag) apply: no canonical identity, no provenance, similarity-driven retrieval, no schema-driven queries.\n\n**Your backend.** OpenAI's own [state management guidance](https://developers.openai.com/apps-sdk/build/storage/) is clear: long-lived business data, cross-session state, and user-specific context belong in *your* backend or MCP server. You persist preferences, user facts, and session metadata; you inject them into the conversation (e.g. system message or initial user message) so the model has context. The API does not read or write a dedicated memory store. You build that layer yourself.\n\n## What's missing\n\n**No first-class memory API.** There is no opt-in tool type that lets the model read and write a persistent memory store the way Claude's Memory Tool does. So there is no built-in flow for \"the model decides what to remember and writes it; next session it reads it.\" You must either (1) keep everything in conversation history (and hit context limits or truncation), (2) use file search for static documents only, or (3) maintain your own store and inject summaries or structured context into each request. All of that is custom.\n\n**No client-side memory backend.** With Claude's Memory Tool, you implement file handlers and control where data lives (including Zero Data Retention). With OpenAI, conversation state and vector stores are server-side. [Data controls](https://platform.openai.com/docs/guides/your-data) (e.g. Zero Data Retention, modified abuse monitoring) affect retention of API data; they don't give you a client-held memory store that the model writes to via tool calls.\n\n**Context and cost.** The more you put into the system message or early turns to represent \"what we know about this user,\" the more tokens you burn and the less room for the actual dialogue. Truncation and summarization are on you. There is no standard way to say \"load only the memories relevant to this turn.\"\n\n## Where the current setup works\n\n- **Single-conversation apps.** If your product is one long-running conversation (or a small number of them), the Conversations API plus manual or injected context is enough. You can store a small amount of user context in your DB and prepend it each time.\n- **Document Q&A.** File search over uploaded docs, code, or knowledge bases works for retrieval-augmented Q&A. Not for user memory.\n- **Stateless or short-lived agents.** When you don't need persistent user-level memory across many conversations, the API is sufficient.\n\n## Where a truth layer fits\n\nWhen you need **the same facts** to be consistent across ChatGPT, Claude, Cursor, or other tools, or when you need **provenance, rollback, or deterministic queries**, the API's conversation state and file search don't provide that. I've written about [similar limits with retrieval alone](/posts/agentic-search-and-the-truth-layer): no canonical identity, no audit trail, non-deterministic answers. A [truth layer](/posts/truth-layer-agent-memory) gives you typed entities, stable IDs, lineage, and cross-platform access via MCP. You keep using the OpenAI API for inference and conversation; the truth layer holds the state that must be shared, queryable, and auditable.\n\n**When to use the API as-is.** You're building an OpenAI-only product, context is mostly per-conversation or you're okay injecting a small amount of user context from your own DB, and you don't need cross-tool memory or full auditability.\n\n**When to add a truth layer.** You need the same state in multiple agents or tools, complete recall over structured data, traceability of where an answer came from, or rollback when the model writes something wrong. Then the API handles the chat; the truth layer handles the memory.\n\n## How configuration works (and doesn't)\n\nWith the Responses API, you create a conversation (or pass history), send input items, and get output items. You can attach file-search vector stores to the request. There is no \"memory\" tool to configure. To simulate persistent user memory, you (1) store user context in your backend, (2) load it when starting or continuing a conversation, and (3) include it in the prompt or early messages. How you store and load it—database, key-value store, or a [truth layer like Neotoma](https://github.com/markmhendrickson/neotoma)—is up to you. Neotoma can be the backend for that context; the model never calls it directly. Your app fetches from Neotoma (e.g. via MCP or your API) and injects the result into the OpenAI request.\n\n## What I'm building\n\nI'm building [Neotoma](https://github.com/markmhendrickson/neotoma) as a structured memory layer: typed entities, canonical IDs, provenance, and cross-platform access via MCP. It doesn't replace the OpenAI API; it sits underneath it. You use the API for conversation and reasoning; you use Neotoma (or another truth layer) when you need memory that is stable, queryable, and portable across all the agents and tools in your stack.\n",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": null,
    "tags": [],
    "createdDate": "2026-02-20",
    "updatedDate": "2026-02-20",
    "summary": "- The OpenAI API has conversation state (Responses API, Conversations API) and file search (vector stores), but no first-class memory API—unlike Claude's Memory Tool.\n- You persist and inject user context yourself; the API doesn't read or write a dedicated memory store. File search is RAG over documents, not user memory.\n- Where the current setup works: single-conversation apps, document Q&A, stateless or short-lived agents.\n- When a truth layer fits: you need the same facts across ChatGPT, Claude, Cursor, or other tools; provenance; rollback; or deterministic queries. The API handles chat; the truth layer handles memory.",
    "shareTweet": "OpenAI API has conversation state and file search—but no first-class memory API. What developers get, what's missing, and when a truth layer fits. https://markmhendrickson.com/posts/openai-api-memory-and-truth-layer",
    "heroImage": "openai-api-memory-and-truth-layer-hero.png",
    "heroImageStyle": "keep-proportions",
    "heroImageSquare": "openai-api-memory-and-truth-layer-hero-square.png"
  },
  {
    "slug": "claude-app-memory-and-truth-layer",
    "title": "Claude Memory (claude.ai): What it does, its limits, and when to add structure",
    "excerpt": "Claude's in-app Memory keeps context across conversations for Pro and Max users. Here's how it works, where it falls short, and when a truth layer makes sense.",
    "body": "Claude can remember things about you across conversations when you use the app at [claude.ai](https://claude.ai). Anthropic calls this [Memory](https://claude.com/blog/memory): it builds a persistent picture of your projects, preferences, and context so you don't have to re-explain every time. This is the **product-level** Memory for everyday users—not the [developer Memory Tool](/posts/claude-memory-and-the-truth-layer) (file-based API) for builders. This post explains what Claude Memory does in the app, where it excels, where it falls short, and when a [truth layer](/posts/truth-layer-agent-memory) or structured memory layer is a better fit for facts you need consistent across tools and time.\n\n## What Claude Memory is\n\nClaude Memory in the app works as a **memory summary** that persists across sessions. You enable it in **Settings**; once on, Claude derives context from past conversations and keeps it in one editable summary. You can view what Claude remembers, tell it what to focus on or ignore, and it adjusts what it references. Memory is fully optional.\n\n**Project-scoped.** Each project has its own memory. That keeps separate work (e.g. a product launch vs. a client engagement) from bleeding into each other. Search and reference are scoped to the project.\n\n**Search and reference.** Claude can search past chats and use them to inform new ones—so prior conversations feed into context without your having to paste them. You control this via settings (e.g. \"Search and reference chats,\" \"Generate memory from chat history\").\n\n**Incognito chat.** For conversations you don't want saved or used as context, you can start an [Incognito chat](https://claude.com/blog/memory). Nothing from that conversation is added to memory or used in future sessions.\n\n**Import and export.** You can [import memories from ChatGPT or Gemini](https://www.theverge.com/news/804124/anthropic-claude-ai-memory-upgrade-all-subscribers) (e.g. copy-paste) and export what Claude remembers anytime, reducing lock-in to one product.\n\n**Availability.** Memory rolled out to Team and Enterprise users in [September 2025](https://claude.com/blog/context-management), then to [Pro and Max](https://www.macrumors.com/2025/10/23/anthropic-automatic-memory-claude/) in October 2025. Enterprise admins can disable memory for their organization.\n\n## Where Claude Memory excels\n\n- **Continuity.** Project details, coding preferences, client context, and workflows carry over so you don't repeat yourself.\n- **Transparency.** You see and edit the memory summary; you can steer what Claude focuses on or drops.\n- **Project separation.** Per-project memory avoids cross-contamination between sensitive or unrelated work.\n- **Portability.** Export and import reduce dependence on a single app; you can move context to or from other products.\n- **Control.** Memory is opt-in; Incognito gives a clean slate when you need it.\n\n## Where Claude Memory falls short\n\n**App-bound.** Memory lives inside the Claude app. You can't query it from another tool, use it in Cursor or ChatGPT, or plug it into a spreadsheet. If you need **one source of truth** that every agent and app can read, Claude Memory isn't that—it's Claude's internal context.\n\n**Summary, not structured store.** Memory is a summary the model maintains. It's not a schema, a database, or a set of typed entities. You can't ask \"give me all tasks due Friday\" or \"every transaction with vendor X\" and get deterministic, structured results. It's optimized for narrative context, not for exact recall or queryable records.\n\n**No provenance or rollback.** You can't trace where a remembered fact came from (which chat, which message) or roll back to a prior state if Claude \"remembers\" something wrong. You edit the summary; there's no audit trail.\n\n**Policy-defined retention.** How long context is kept, how summaries are updated, and what gets pruned are determined by Anthropic. You don't own the storage or the eviction policy.\n\n## When a truth layer makes sense\n\nA [truth layer](/posts/truth-layer-agent-memory) is a different design: typed entities, canonical IDs, provenance, and the same data available across tools (e.g. via [MCP](https://modelcontextprotocol.io)). It's not a replacement for Claude Memory when you just want better continuity in the app. It's the right fit when:\n\n- You need **the same facts** in Claude, ChatGPT, Cursor, email, or other apps—one source of truth.\n- You need **deterministic recall**—e.g. \"all tasks due Friday\" or \"every transaction with vendor X\"—not a summarized context.\n- You need **auditability**—where a number or fact came from, and the ability to correct or roll back.\n- You want **ownership and portability**—your memory store lives in your infrastructure, not only inside one product.\n\nClaude Memory is great for making the Claude app more useful. A truth layer is for when the truth has to live outside any one chatbot.\n\n## Comparison at a glance\n\n| | Claude Memory (app) | Truth layer (e.g. Neotoma) |\n|--|---------------------|----------------------------|\n| Scope | Claude app (and export/import) | Same data in Claude, ChatGPT, Cursor, etc. |\n| Content | Memory summary, project context | Typed entities, tasks, contacts, transactions |\n| Query | Natural language, search past chats | Deterministic queries by type, ID, relationship, time |\n| Provenance | None | Full lineage per observation |\n| Control | View/edit summary, export | You own the store; export, backup, schema |\n| Best for | Continuity and context inside Claude | One source of truth across tools and time |\n\n## What I'm building\n\nI'm building [Neotoma](https://github.com/markmhendrickson/neotoma) as a structured memory layer: typed entities, canonical IDs, provenance, and cross-platform access via MCP. I use Claude Memory for continuity and context inside the Claude app. I use Neotoma when I need the same facts to be stable, queryable, and portable across all the agents and tools I use. Both have a place; they solve different problems.\n",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": null,
    "tags": [],
    "createdDate": "2026-02-20",
    "updatedDate": "2026-02-20",
    "summary": "- Claude Memory in the app (claude.ai) is a persistent memory summary for Pro and Max (and Team/Enterprise) users—separate from the developer Memory Tool API.\n- It excels at continuity, transparency (view/edit summary), project-scoped context, and import/export; Incognito chat opts out.\n- Limits: app-bound (not queryable elsewhere), summary not structured store, no provenance or rollback, retention and eviction set by policy.\n- A truth layer fits when you need the same facts across tools, deterministic recall, auditability, or ownership. Claude Memory for in-app context; a truth layer for one source of truth across tools and time.",
    "shareTweet": "",
    "heroImage": "claude-app-memory-and-truth-layer-hero.png",
    "heroImageStyle": "keep-proportions",
    "heroImageSquare": "claude-app-memory-and-truth-layer-hero-square.png"
  },
  {
    "slug": "chatgpt-memory-and-truth-layer",
    "title": "ChatGPT Memory: What it does, its limits, and when to add a truth layer",
    "excerpt": "ChatGPT Memory personalizes your experience with saved memories and chat history. Here's how it works, where it falls short, and when a truth layer makes sense.",
    "body": "ChatGPT can remember things about you across conversations. OpenAI calls this [Memory](https://help.openai.com/en/articles/8983136-what-is-memory): it makes responses more relevant and personalized. If you use ChatGPT regularly, you've probably seen it—or turned it off. This post explains what ChatGPT Memory does, where it excels, where it falls short, and when a [truth layer](/posts/truth-layer-agent-memory) or structured memory layer is a better fit for the facts you need to keep consistent across tools and time. For what the API offers developers, see [OpenAI API memory](/posts/openai-api-memory-and-truth-layer).\n\n## What ChatGPT Memory is\n\nChatGPT Memory works in two ways, both controlled in **Settings > Personalization**:\n\n**Saved memories.** Details you explicitly ask ChatGPT to remember (\"Remember that I'm vegetarian when you recommend recipes\") or that it saves automatically from conversation. These are stored separately from chat history. You can view them (\"What do you remember about me?\"), delete individual or all memories, and turn saved memory on or off. Plus and Pro users can prioritize or deprioritize memories, view history of a memory, and restore prior versions. Free users have [saved memories only](https://help.openai.com/en/articles/8983136-what-is-memory). Memory is intended for high-level preferences and details; OpenAI says not to rely on it for exact templates or large blocks of verbatim text.\n\n**Chat history reference.** When this is on (Plus and Pro), ChatGPT can use information from past conversations to inform new ones—e.g. that you like Thai food—without your having to save it as a memory. It doesn't retain every detail; saved memories are for what you want always kept in mind. You can disable \"Reference chat history\" independently. If you turn it off, that referenced information is deleted from OpenAI's systems within 30 days.\n\n**Temporary Chat.** You can start a [Temporary Chat](https://help.openai.com/en/articles/8914046-temporary-chat-faq) for any conversation that shouldn't use or update memory. Useful for one-off or sensitive topics.\n\n**Control and privacy.** You can turn memory off entirely, delete chats, and remove memories. ChatGPT is trained not to proactively remember sensitive health information unless you ask. With \"Improve the model for everyone\" off (default for Business/Enterprise/Edu), your content isn't used for training. Deleted memories may be retained for up to 30 days for safety and debugging.\n\n## Where ChatGPT Memory excels\n\n- **Convenience.** No setup. Memory is on by default (for supported plans); you can teach it by talking.\n- **Personalization.** Recommendations, tone, and context improve over time for that account.\n- **User control.** Clear toggles, visibility (\"What do you remember about me?\"), and Temporary Chat for opt-out.\n- **Good for preferences.** Name, diet, location, work style, interests—exactly what the product is designed for.\n\n## Where ChatGPT Memory falls short\n\n**Opaque and platform-bound.** You can't export your memory store, see its schema, or query it programmatically. It lives inside ChatGPT. You can't use the same memories in Claude, Cursor, or a spreadsheet. If you want \"one place that knows the truth\" across tools, ChatGPT Memory isn't that place.\n\n**Not for structured or exact data.** OpenAI explicitly says memory is for high-level preferences, not exact templates or large verbatim text. There's no guarantee of deterministic recall—same question, same answer—or of storing structured records (tasks, contacts, transactions) in a queryable way.\n\n**Capacity and prioritization.** Plus/Pro users can hit a \"memory full\" state; automatic management keeps some memories \"top of mind\" and others in the background. You don't control the eviction policy. So you can't treat it as a reliable, complete store of everything you've asked it to remember.\n\n**No provenance or rollback.** You can't trace where a remembered fact came from (which chat, which message) or roll back to a prior state if ChatGPT \"remembers\" something wrong. You can delete or edit individual memories, but there's no audit trail.\n\n**Retention and deletion.** Deleted memories may be retained for up to 30 days. To fully remove something, you must delete both the saved memory and the chats where you shared it. Reference chat history is deleted within 30 days when you turn it off—but the behavior is defined by policy, not by you.\n\n## When a truth layer makes sense\n\nA [truth layer](/posts/truth-layer-agent-memory) is a different design: typed entities, canonical IDs, provenance, and the same data available across tools (e.g. via [MCP](https://modelcontextprotocol.io)). It's not a replacement for ChatGPT Memory for casual personalization. It's the right fit when:\n\n- You need **the same facts** in ChatGPT, Claude, Cursor, email, or other apps—one source of truth.\n- You need **deterministic recall**—e.g. \"all tasks due Friday\" or \"every transaction with vendor X\"—not best-effort personalization.\n- You need **auditability**—where a number or fact came from, and the ability to correct or roll back.\n- You want **ownership and portability**—your memory store lives in your infrastructure, not only inside a single product.\n\nChatGPT Memory is great for making ChatGPT more useful to you. A truth layer is for when the truth has to live outside any one chatbot.\n\n## Comparison at a glance\n\n| | ChatGPT Memory | Truth layer (e.g. Neotoma) |\n|--|----------------|----------------------------|\n| Scope | ChatGPT only | Same data in ChatGPT, Claude, Cursor, etc. |\n| Content | Preferences, high-level details | Typed entities, tasks, contacts, transactions |\n| Query | \"What do you remember about me?\" (natural language) | Deterministic queries by type, ID, relationship, time |\n| Provenance | None | Full lineage per observation |\n| Control | Delete, prioritize, turn off | You own the store; export, backup, schema |\n| Best for | Personalization inside ChatGPT | One source of truth across tools and time |\n\n## What I'm building\n\nI'm building [Neotoma](https://github.com/markmhendrickson/neotoma) as a structured memory layer: typed entities, canonical IDs, provenance, and cross-platform access via MCP. I use ChatGPT Memory for convenience inside ChatGPT. I use Neotoma when I need the same facts to be stable, queryable, and portable across all the agents and tools I use. Both have a place; they solve different problems.\n",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": null,
    "tags": [],
    "createdDate": "2026-02-20",
    "updatedDate": "2026-02-20",
    "summary": "- ChatGPT Memory is saved memories (explicit or auto) plus optional chat history reference; you control it in Settings and can use Temporary Chat to opt out.\n- It excels at convenience and personalization inside ChatGPT; it's not for exact or structured data, export, or cross-tool use.\n- Limits: opaque, platform-bound, no provenance or rollback, capacity/prioritization behavior, retention policies you don't control.\n- A truth layer fits when you need the same facts across tools, deterministic recall, auditability, or ownership and portability. ChatGPT Memory for in-app personalization; a truth layer for one source of truth across tools and time.",
    "shareTweet": "ChatGPT Memory: what it does, where it falls short, and when a truth layer makes sense. https://markmhendrickson.com/posts/chatgpt-memory-and-truth-layer",
    "heroImage": "chatgpt-memory-and-truth-layer-hero.png",
    "heroImageStyle": "keep-proportions",
    "heroImageSquare": "chatgpt-memory-and-truth-layer-hero-square.png"
  },
  {
    "slug": "nietzsche-and-yoga",
    "title": "Nietzsche and yoga",
    "excerpt": "A conversation about Nietzsche and yoga led me to look again at what both ask of the body and the will.",
    "body": "- Nietzsche and yoga both treat the body as the site of truth, not an obstacle to overcome with the mind.\n- Discipline in yoga (abhyasa, tapas) lines up with Nietzschean self-overcoming: saying yes to difficulty as a condition for growth.\n- The \"eternal return\" and presence in practice both point at affirming this life, this moment, without escape into otherworldly comfort.\n- The conversation that sparked this is [here](https://chatgpt.com/share/6997396a-027c-8012-9ba4-1b465b12b838).\n\nI got sent a link to a ChatGPT thread titled \"Nietzsche and yoga.\" I couldn't read the full exchange from the share page, but the pairing stuck. It's one of those that sounds wrong until you sit with it. Nietzsche: the philosopher of the will to power, the death of God, the Übermensch, the eternal return. Yoga: asana, pranayama, meditation, often wrapped in wellness and spirituality. One is read as aggressive and life-affirming, the other as calming and sometimes escapist. So why put them in the same sentence?\n\nBecause both take the body seriously. Nietzsche never let philosophy float free of the organism. He wrote about digestion, climate, diet, and the \"great reason\" of the body. He attacked systems that treated the flesh as something to be denied or transcended. Yoga, in its classical sense, is not about checking out. It's about showing up in the body, breath by breath, and working with resistance. The mat is where you meet your limits and, over time, change your relationship to them. That's not far from a Nietzschean idea of strength: not the absence of difficulty but the capacity to incorporate it.\n\nDiscipline is the next bridge. Nietzsche admired the capacity to impose order on oneself, to make of life a kind of art. He had little patience for ressentiment, the turning of energy inward into guilt or blame. Yoga has a similar emphasis. Abhyasa (steady practice) and tapas (heat, austerity, effort) are not about punishing the body. They're about choosing difficulty in a way that clarifies rather than exhausts. You don't do the hard pose to prove you're good. You do it to see what you're made of and to expand what you can hold. That's self-overcoming in a very concrete form.\n\nThen there's the question of time and affirmation. The eternal return is the thought experiment: what if you had to relive your life, every detail, again and again? Would you say yes? The point isn't cosmology. It's a test of whether you're actually affirming this life or secretly wishing for another. Yoga doesn't use that language, but the practice of presence does something similar. When you're in a hold or a breath cycle, you're not in the past or the future. You're in the only moment you have. The practice is to stay there without fleeing into fantasy or complaint. That's a form of saying yes.\n\nI'm not claiming Nietzsche would have signed up for a vinyasa class. He might have hated the packaging. But the impulse to put him in conversation with yoga is sound. Both ask you to stop treating the body as the enemy of the real. Both value discipline as a path to freedom rather than as punishment. Both point toward affirming this life instead of waiting for a better one. The thread that sparked this is a good place to keep that conversation going.\n",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": null,
    "tags": [],
    "createdDate": "2026-02-19",
    "updatedDate": "2026-02-19",
    "summary": "- Nietzsche and yoga both treat the body as the place where truth shows up, not as something to transcend.\n- Discipline in yoga (abhyasa, tapas) mirrors Nietzschean self-overcoming: choosing difficulty as a condition for growth.\n- The eternal return and presence in practice both test whether you affirm this life instead of escaping into another.\n- Both value the capacity to incorporate resistance rather than avoid it or resent it.\n- The ChatGPT thread that sparked this is a good prompt to keep the conversation going.",
    "shareTweet": "A conversation about Nietzsche and yoga: both take the body seriously, value discipline as a path to freedom, and point at saying yes to this life. Notes on where they meet. https://chatgpt.com/share/6997396a-027c-8012-9ba4-1b465b12b838"
  },
  {
    "slug": "six-agentic-trends-betting-on.thread",
    "title": "# X – Single tweet (all six + link)",
    "excerpt": "",
    "body": "# X – Single tweet (all six + link)\n\nOne tweet. May exceed default view; full text shows on expand/click. Same content as LinkedIn for consistency.\n\n---\n\nIf you're building with AI, you can't stay agnostic about where the industry is going. You have to pick a set of theses and work towards them, knowing some might be wrong.\n\nI've identified the six structural trends I'm betting on, as well as what would falsify them:\n\n1. Agents will become stateful economic actors and memory will become system state, enabling long-horizon plans and coordination at scale.\n2. Agentic errors will become economically visible and tolerance for approximate memory will erode as the bar for defensibility and audit rises.\n3. Audit and compliance will drift down-market and the pressure to prove how work was produced will reach consultants, agencies, and small teams.\n4. Platform memory will remain opaque and a trust gap will grow between those who need guarantees and those who rely on platform convenience.\n5. Tool fragmentation will persist and state fragmentation will matter more than interface fragmentation, with memory as a protocol beneath it.\n6. Agentic usage will become metered and deterministic memory will enable replay over regeneration, turning memory into an optimization surface.\n\nRead more in depth: https://markmhendrickson.com/posts/six-agentic-trends-betting-on\n",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": null,
    "tags": [],
    "createdDate": "2026-02-19",
    "updatedDate": "2026-02-19",
    "summary": "",
    "shareTweet": ""
  },
  {
    "slug": "six-agentic-trends-betting-on.linkedin",
    "title": "# LinkedIn post (draft)",
    "excerpt": "",
    "body": "# LinkedIn post (draft)\n\n**Copy-ready post:**\n\nIf you're building with AI, you can't stay agnostic about where the industry is going. You have to pick a set of theses and work towards them, knowing some might be wrong.\n\nI've identified the six structural trends I'm betting on, as well as what would falsify them:\n\n1. Agents will become stateful economic actors and memory will become system state, enabling long-horizon plans and coordination at scale.\n2. Agentic errors will become economically visible and tolerance for approximate memory will erode as the bar for defensibility and audit rises.\n3. Audit and compliance will drift down-market and the pressure to prove how work was produced will reach consultants, agencies, and small teams.\n4. Platform memory will remain opaque and a trust gap will grow between those who need guarantees and those who rely on platform convenience.\n5. Tool fragmentation will persist and state fragmentation will matter more than interface fragmentation, with memory as a protocol beneath it.\n6. Agentic usage will become metered and deterministic memory will enable replay over regeneration, turning memory into an optimization surface.\n\nRead more in depth: https://markmhendrickson.com/posts/six-agentic-trends-betting-on\n\n---\n\n**X – Short single tweet (under 280):** six-agentic-trends-betting-on.tweet.md  \n**X – Long single tweet (all six + link, same as LinkedIn):** six-agentic-trends-betting-on.thread.md\n\n---\n\n**Short LinkedIn version (if you prefer a tighter post):**\n\nSix trends I'm betting on for the agentic future.\n\n1. Agents will become stateful economic actors and memory will become system state, enabling long-horizon plans and coordination at scale.\n2. Agentic errors will become economically visible and tolerance for approximate memory will erode as the bar for defensibility and audit rises.\n3. Audit and compliance will drift down-market and the pressure to prove how work was produced will reach consultants, agencies, and small teams.\n4. Platform memory will remain opaque and a trust gap will grow between those who need guarantees and those who rely on platform convenience.\n5. Tool fragmentation will persist and state fragmentation will matter more than interface fragmentation, with memory as a protocol beneath it.\n6. Agentic usage will become metered and deterministic memory will enable replay over regeneration, turning memory into an optimization surface.\n\nI spelled out what would prove me wrong. Link in comments.\n",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": null,
    "tags": [],
    "createdDate": "2026-02-19",
    "updatedDate": "2026-02-19",
    "summary": "",
    "shareTweet": ""
  },
  {
    "slug": "claude-memory-and-the-truth-layer",
    "title": "Claude Memory Tool: Pros, cons and when to add structure",
    "excerpt": "Claude Memory Tool lets Claude persist context by writing files. Strengths, limits, and when to add a truth layer.",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": 6,
    "tags": [
      "claude",
      "neotoma",
      "truth-layer",
      "agent-memory",
      "build-in-public"
    ],
    "heroImage": "claude-memory-and-the-truth-layer-hero.png",
    "heroImageStyle": "keep-proportions",
    "heroImageSquare": "claude-memory-and-the-truth-layer-hero-square.png",
    "createdDate": "2026-02-18",
    "updatedDate": "2026-02-18",
    "ogImage": "og/claude-memory-and-the-truth-layer-1200x630.jpg",
    "summary": "- The Claude Memory Tool is a client-side developer API feature (not product-level Claude Memory). You opt in with `memory_20250818`; Claude sends tool calls; you register handlers and execute file ops; data is Zero Data Retention eligible. Neotoma could be used as a backend with an adapter (none exists today).\n- Where it excels: it moves state outside the context window, pairs with context editing for long-running agents, and offers simple implementation—file handlers, no schema, Claude decides what to store. Good for guidelines, preferences, progress notes, session continuity.\n- Where it falls short: file-based design means no canonical identity, no provenance, in-place writes with no rollback, session-dependent structure that drifts, global store tension, provider-bound (Claude only), and unstructured retrieval.\n- A truth layer (e.g. Neotoma) is a different design: typed entities, canonical IDs, provenance, rollback, cross-platform MCP access, deterministic queries. Use the Memory Tool for session continuity in a Claude agent; use a truth layer when you need stable, queryable, portable state across agents and tools.\n- Both have a place; they solve different problems.",
    "body": "Anthropic made the [Claude Memory Tool](https://platform.claude.com/docs/en/agents-and-tools/tool-use/memory-tool) generally available on [February 17, 2026](https://claude.com/blog/improved-web-search-with-dynamic-filtering) as part of a broader Claude Developer Platform update, having first released it into beta in [September 2025](https://claude.com/blog/context-management).\n\nFor developers building long-running agents or recurring workflows, it's a helpful step forward. However, it does come with some important limitations.\n\nThis post explains what it is, how it works, where it excels, where it falls short, and how it compares to a [structured memory layer](/posts/truth-layer-agent-memory).\n\n## What the Claude Memory Tool is\n\nThe Memory Tool is a client-side developer API feature, not the product-level [Claude Memory](https://www.anthropic.com/news/memory) that rolled out to Pro and Max users last year.\n\nYou opt in per request by including `{\"type\": \"memory_20250818\", \"name\": \"memory\"}` in your tools list. Claude is prompted to check `/memories` first, then write what it learns during the session.\n\n**File operations.** The tool supports `view` (directory listing or file contents with line numbers), `create`, `str_replace`, `insert`, `delete`, and `rename`. Claude writes structured files, typically XML, with learned context, guidelines, and notes. You implement the storage backend (file system, database, cloud storage). Data is [Zero Data Retention](https://platform.claude.com/docs/en/build-with-claude/zero-data-retention) eligible.\n\n**Models.** Available on Claude Opus 4.x, Sonnet 4.x, and Haiku 4.5.\n\nA concrete example from the [Memory tool docs](https://platform.claude.com/docs/en/agents-and-tools/tool-use/memory-tool): you ask Claude to help with a customer service ticket. Claude calls `view /memories`, finds `customer_service_guidelines.xml` and `refund_policies.xml`, reads both, then incorporates what it learned into the response. Next session, those files are still there.\n\n## Where the Claude Memory Tool excels\n\nThe Memory Tool solves a real problem. Long-running agents hit context limits. Tool results pile up, tokens run out, and you either truncate history or degrade performance. The Memory Tool moves state outside the context window. Claude writes what matters to files before context clears, then reads those files when it needs to reconstruct. Context editing and the Memory Tool together let agents run indefinitely without manual intervention.\n\nImplementation is simple. You provide file handlers. No schema to design, no entity types to define, no extraction pipeline. Claude decides what to store and how to name it. You get persistent context with minimal backend code. If your use case is \"pick up where we left off\" or \"remember my preferences for this project,\" that often suffices. Guidelines, learned preferences, progress notes, project context: the Memory Tool handles them well. It pairs cleanly with context editing, and because storage is client-side, you control where data lives and whether it qualifies for Zero Data Retention.\n\nFor many agent builds, that's enough. The limits show up when you need more.\n\n## Where the Claude Memory Tool falls short\n\nThe Memory Tool is file-based. That's also the source of every limit that applies to it. The same shortcomings that come up in analysis of file-based memory for Claude Code ([Maisum Hashim](https://www.maisumhashim.com/blog/claude-code-memory-crisis-persistent-context-systems), [Backnotprop](https://backnotprop.com/blog/file-system-as-memory/)) apply here, because the mechanism is the same: text files, read and written by an LLM. I've hit [similar limits with retrieval alone](/posts/agentic-search-and-the-truth-layer): no provenance, unstable identity, non-deterministic answers.\n\n**No canonical identity.** Claude writes whatever it decides is worth storing. If your vendor is \"Acme Corp\" in one session and \"ACME CORPORATION\" in a later XML file, there's no merge rule to reconcile them. No canonical ID exists. The next query that reads both files will get two representations of the same entity and may treat them as the same or different depending on context. There's no way to force one answer. That's the same entity-resolution problem that shows up when you ask \"show me all documents involving Company X.\"\n\n**No provenance.** Memory files accumulate facts without lineage. If Claude writes a number into `/memories/budget.xml`, you cannot trace where that number came from, which session, which document, which inference. When the number is wrong, you have no audit trail to follow back.\n\n**In-place writes, no rollback.** `str_replace` modifies a file directly. `delete` removes it. There's no append-only record of what changed. If Claude \"corrects\" a stored fact based on bad inference in session N, the previous value is gone. No versioning, no undo.\n\n**Session-dependent structure.** What Claude writes to memory varies by session. The XML schema it chooses, the granularity of facts, the filenames, none of these are enforced by a schema. Memory files can drift in format. Retrieval reliability depends on whatever structure Claude produced in the last session that touched that file.\n\n**Global store tension.** All memories live in one `/memories` directory. If you're building agents that work across multiple projects or users, you either namespace everything carefully (and rely on Claude respecting namespaces) or you risk cross-contamination: facts from project A surfacing when working on project B. The docs flag this as a security concern (path traversal protection) but also as a practical storage concern. Track file sizes, expire stale files, prevent unbounded growth.\n\n**Provider-bound.** The Memory Tool is a Claude API feature. The memory it writes is only readable by Claude via the same implementation. You can't expose the same memory to a ChatGPT agent, a Cursor extension, or any other tool unless you build a bridge yourself. Your memory layer is tied to this API.\n\n**Size and retrieval are unstructured.** Reading memory means reading files. Claude decides what to `view` and when. There's no index, no schema-driven query, no way to say \"give me all tasks due before Friday\" or \"show all mentions of vendor X.\" Retrieval is inference over text. The same [limits that RAG hits for agent memory](/posts/why-agent-memory-needs-more-than-rag)—similarity over raw text, no structural query—apply here.\n\nNone of this makes the Memory Tool bad for what it's designed for: picking up context across sessions, learning from past interactions, maintaining project-level guidelines. For those uses it works well. The limits show up when you need deterministic answers, complete recall across large datasets, or verifiable state.\n\n## Comparison with a truth layer (Neotoma)\n\nA [truth layer](/posts/truth-layer-agent-memory) is a different design from the ground up. Rather than writing unstructured files and relying on the LLM to recall them, it maintains typed entities with canonical IDs, persists provenance with every observation, and answers queries deterministically.\n\nThe table below lines up the Memory Tool directly against a truth layer so the differences are concrete.\n\n| Domain | Memory Tool (file-based) | Truth layer (e.g. Neotoma) |\n|--------|--------------------------|----------------------------|\n| Storage | Files in `/memories` directory | Typed entities with schemas |\n| Identity | No canonical IDs; LLM re-infers | Stable entity IDs; rule-based merge |\n| Retrieval | LLM reads files; unstructured inference | Deterministic queries by schema, ID, relationship, time |\n| Provenance | None; file contents overwritten | Full lineage; every observation traced to source |\n| Rollback | No; writes are in-place | Yes; append-only or versioned; rollback to prior state |\n| Cross-platform | Claude API only | Same memory via MCP in ChatGPT, Claude, Cursor, etc. |\n| Determinism | Same question can produce different answers | Same inputs and schema yield the same snapshot |\n| Schema | None enforced; structure drifts by session | Explicit schema; unknown fields preserved, not lost |\n| Structured queries | No; full-text over files | Yes; filter by entity type, ID, relationship, date |\n\n**When the Memory Tool is the right choice.** You're building a Claude API agent, your use case is continuity within recurring workflows (guidelines, learned preferences, progress tracking), and you don't need cross-tool access or audit. You want simple implementation: implement file handlers, get persistent context. Works well for session-to-session handoff in a single provider context.\n\n**When a truth layer is the right choice.** You need the same state accessible from ChatGPT, Claude, Cursor, or other MCP clients. You need complete recall over structured data: \"all transactions with vendor X in 2024,\" not \"what do my memory files say about vendor X.\" You need to trace an answer back to its source. You need rollback when an agent writes something wrong. You want schema-enforced structure that doesn't drift by session.\n\nThe Memory Tool addresses the context-window limitation elegantly. That's a real and important problem. Long-running agents lose state when context clears. File-based memory solves that by moving state outside the context window. What it doesn't solve is identity, provenance, or cross-platform access. Those require a different layer.\n\n## How configuration works\n\nClaude sends memory tool calls during the conversation (e.g. `view /memories` or `create /memories/preferences.xml`). You register handlers that receive these calls, execute the file operations in your infrastructure, and return results as tool outputs. The flow is tool-call → handler → tool result. Your backend can be a file system, a database, cloud storage, or any store that can emulate the required operations. Paths are virtual: `/memories` and its subpaths map to whatever you implement.\n\n**Neotoma as a backend.** [Neotoma](https://github.com/markmhendrickson/neotoma) is a structured memory layer, not a file system, so it doesn't natively implement the Memory Tool's file API. To use it as a backend you'd need an adapter that maps Memory Tool calls to Neotoma operations: `view` to list or fetch entities, `create` and `str_replace` to store or update content, `delete` and `rename` to remove or mutate. Each \"file\" would become an entity (or unstructured blob with metadata); paths would map to entity IDs or scoped queries. The adapter would return results in the format Claude expects. You'd gain provenance (every write traced) and stable IDs, but Claude would still be writing unstructured XML into blobs—the structure would be at the file level, not inside the content. Neotoma would persist those blobs with lineage. No such adapter exists today; it would be a bridge layer you build.\n\n## What I'm building\n\nI'm building [Neotoma](https://github.com/markmhendrickson/neotoma) as a structured memory layer that takes the truth layer approach: typed entities, canonical IDs, deterministic merge, provenance, and cross-platform access via MCP. I'm running it alongside the Claude Memory Tool in my own stack to see where the boundary sits in practice. The Claude Memory Tool is a good fit for session continuity inside a Claude API agent. A truth layer is a good fit when you need the same facts to be stable, queryable, and portable across all the agents and tools in your stack. Both have a place; they solve different problems.\n\n## FAQ\n\n**What is the Claude Memory Tool?** It's a client-side developer API feature that lets Claude read and write files in a `/memories` directory across conversations. You include `memory_20250818` in your tools list; Claude checks and writes files; you implement the storage backend. It's distinct from the product-level Claude Memory in the app.\n\n**Can Neotoma be used as a Claude Memory Tool backend?** Yes, with an adapter. Neotoma is a structured store, not a file system, so you'd need a bridge that maps Memory Tool calls (view, create, str_replace, etc.) to Neotoma operations. You'd gain provenance and stable IDs; Claude would still write unstructured XML. No such adapter exists today.\n\n**When should I use the Claude Memory Tool vs a truth layer?** Use the Memory Tool for session continuity in a Claude API agent—guidelines, preferences, progress notes—when you don't need cross-tool access or audit. Use a truth layer when you need the same state in ChatGPT, Claude, Cursor, or other MCP clients; deterministic queries; provenance; or rollback from bad writes.\n"
  },
  {
    "slug": "agentic-future-betting-on",
    "title": "The agentic future I'm betting on (and how I might be wrong)",
    "excerpt": "I'm betting on stateful agents, priced errors, and opaque platform memory. Here's why, and how I might be wrong.",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": 8,
    "tags": [
      "ai",
      "agents",
      "neotoma",
      "build-in-public",
      "memory"
    ],
    "body": "Everyone involved in AI right now is, implicitly or explicitly, trying to predict where things are going and how those changes will reshape our lives and work. The volume of speculation is enormous, and much of it is contradictory. That is unavoidable. No one can know with confidence what the next couple of years will bring. The space is moving too quickly, the interactions between technologies are too complex, and second-order effects dominate in ways that are difficult to model ahead of time.\n\nStill, if you are operating in this space, especially if you are building something with AI or for AI, it is not enough to remain agnostic. You have to choose a set of core theses about how the world is likely to evolve and build coherently around them, knowing that some will be wrong and others will matter more than expected. These theses are less about precise prediction and more about identifying structural pressures that seem unlikely to reverse.\n\nWhat follows are the central assumptions I am currently operating under. They are not claims about inevitability, and they are not meant to cover every possible future. They are the trends that, if they continue even partially, shape how I think AI systems will be used, where friction will accumulate, and what kinds of infrastructure will become necessary. Neotoma is best understood as a response to these assumptions. It is not the reason for them, but it is built in anticipation of the world they imply.\n\n## Agents will become stateful economic actors\n\nOver the next two years, agents are likely to move beyond assistive, prompt-centric interactions and become meaningfully stateful actors. This does not require a breakthrough in general intelligence. It only requires cheaper inference, more capable tool APIs, and broader tolerance for agents running unattended.\n\nSocietally, the shift matters because stateful agents blur lines of accountability and delegation. We are used to tools that do nothing until we act. When agents persist goals, coordinate with each other, and take irreversible actions over time, the question of who is responsible for an outcome becomes harder to answer. Labor and identity shift too: more work is delegated to non-human actors, and the boundary between \"I did this\" and \"my agent did this\" softens. Norms around trust, liability, and dependency will have to adapt. The technology enables the change; society has to decide how to live with it.\n\nThe structural reason this trend is likely is that the marginal cost of keeping agents alive is collapsing faster than the cost of rebuilding context. As inference becomes cheaper and orchestration frameworks mature, it is more efficient to persist an agent's state than to constantly reconstruct it from scratch. Tool APIs increasingly assume continuity: credentials, caches, intermediate artifacts. That implicitly rewards persistence over statelessness.\n\nAs agents begin to persist goals across time, execute multi-step plans without constant supervision, coordinate with other agents, and leave irreversible side effects, memory ceases to be a convenience feature. It becomes part of the system's state, comparable to a database rather than a chat log.\n\nOnce resetting an agent is no longer trivial, fuzzy or opaque memory turns into a source of systemic failure rather than mild annoyance. Debugging becomes archaeology unless prior decisions, assumptions, and facts can be reconstructed deterministically.\n\nNeotoma becomes relevant here because it treats memory as explicit system state. Information is captured as typed entities, events, and relationships in a deterministic graph rather than as prompt residue or embedding similarity. That allows an agent's history to be replayed, inspected, and reasoned about as part of the system itself.\n\n**What to watch over the next year:**\n\n1. Agent frameworks advertising long-running, background, or resumable execution as a core feature.\n2. Teams discussing agent state corruption or drift as a distinct bug class rather than restarting agents as a fix.\n3. Product interfaces exposing agent history as something inspectable rather than ephemeral.\n\n## Errors will become economically visible\n\nAs AI output increasingly flows directly into billing, compliance, client deliverables, and automated workflows, the cost of errors is likely to shift. What is currently a diffuse inconvenience becomes explicit economic impact.\n\nWhen errors start to show up in postmortems, contracts, and court filings, society gains a sharper picture of who bears the cost and who gets blamed. Organizations will face pressure to prove how decisions were made and what the system knew at the time. That pressure will ripple into professional norms, insurance, and regulation. Individuals and small teams may be held to standards that were originally designed for large institutions with audit trails. The upside is more accountability and fewer silent failures. The downside is that the bar for \"explainable\" and \"auditable\" may rise faster than many are ready for.\n\nThe structural reason this trend is likely is that AI is moving closer to decision-making edges, not just advisory layers. As AI output becomes embedded downstream in systems that trigger payments, commitments, or external communication, errors inherit the cost structure of those systems. Organizations cannot continue treating failures as \"model quirks\" once they propagate into irreversible actions.\n\nToday, mistakes are often shrugged off with regeneration or prompt tweaks. Tomorrow, those same mistakes will waste money, damage reputation, or create legal exposure.\n\nWhen errors become priced, organizations stop asking whether outputs were helpful. They start asking how those outputs were produced, what information they relied on, and whether the process can be replayed or audited.\n\nNeotoma aligns with this shift by enforcing provenance at the memory layer. Facts are stored with source attribution, timestamps, and ingestion events. Corrections are additive rather than destructive, allowing teams to reconstruct exactly what an agent knew at the time of a decision instead of guessing based on partial logs.\n\n**What to watch over the next year:**\n\n1. AI-related failures appearing in postmortems, client disputes, or legal contexts.\n2. Teams explicitly asking \"what did the agent know at the time?\" after mistakes.\n3. Traceability or audit requirements being added to AI workflows retroactively.\n\n## Platform memory will remain opaque\n\nLarge AI platforms are likely to continue shipping memory features that are useful but fundamentally opaque. Their incentives favor engagement, retention, and model optimization rather than user-controlled provenance or guarantees of correctness.\n\nThe societal effect is a split between those who can afford to care and those who cannot. People and organizations that need strong guarantees (audit, correctness, portability) will either pay for alternatives, build their own, or accept risk. Everyone else will rely on platform memory and live with the trust gap. That divide can reinforce existing inequalities: the well-resourced get transparent, portable memory; everyone else gets convenience with opaque terms. Over time, norms about what \"my data\" and \"my history\" mean may diverge by context and by who you are. Civic and professional expectations (e.g. that you can show your work or export your records) may apply only to some.\n\nThe structural reason this persists is incentive misalignment. Platforms optimize for aggregate outcomes across millions of users, not for the correctness guarantees required by any individual workflow. Exposing memory semantics, correction rules, or replay guarantees constrains iteration speed and increases liability. Opaqueness is not accidental. It is protective.\n\nMemory may improve, but it will remain difficult to inspect, export, replay, or reason about formally, especially across tools. Corrections will often be silent, implicit, or model-specific.\n\nThis creates a growing trust gap. Users may rely on platform memory for convenience while simultaneously distrusting it in contexts where consequences matter.\n\nNeotoma only matters if this misalignment persists. Its local, inspectable, user-controlled design is the alternative for workflows where correctness and provenance matter. If platforms opened their memory semantics and offered strong guarantees, the need for a separate substrate would shrink.\n\n**What to watch over the next year:**\n\n1. Memory features that improve recall but stay undocumented or non-exportable.\n2. Users asking for \"my data\" or \"what the system knows\" and getting no clear answer.\n3. Workarounds (exports, third-party sync, manual replication) growing rather than shrinking.\n\n## How I might be wrong\n\nI could be wrong in both directions. The trends could accelerate faster than I expect: stateful agents and priced errors become the norm within a year, and the demand for deterministic memory jumps early. Or they could stall. Agents stay assistive, errors stay cheap to fix, and platform memory stays good enough for most people. The trust gap never widens enough to create a market for an alternative.\n\nI am building on the middle path. These theses are the ones that, if they hold even partly, make a substrate like Neotoma necessary. If they do not hold, the build was still a useful way to learn. Time will tell.",
    "createdDate": "2026-02-17",
    "updatedDate": "2026-02-17",
    "summary": "- Agents will become stateful economic actors as the cost of persisting state falls below the cost of rebuilding context; memory becomes system state, not chat residue. Society will have to adapt norms around accountability, delegation, and who is responsible when agents act over time.\n- Errors will become economically visible as AI output moves into billing and compliance; provenance and replay will matter when mistakes have real cost. That visibility will reshape who bears cost and blame and push professional and regulatory norms toward auditability.\n- Platform memory will stay opaque because incentives favor engagement over user-controlled correctness; local, inspectable alternatives matter only if that persists. The split between those who can afford strong guarantees and those who rely on platform memory may reinforce existing inequalities.\n- Neotoma is built in anticipation of these trends; if they stall or reverse, the build was still a useful way to learn.",
    "shareTweet": "The agentic future I'm betting on: stateful agents, priced errors, and opaque platform memory. Why I'm building for that world, and how I might be wrong. https://markmhendrickson.com/posts/agentic-future-betting-on"
  },
  {
    "slug": "degen-culture-crypto-ai-builders-patience",
    "title": "Degen culture in crypto and AI delivers signal at a cost to patient builders",
    "excerpt": "The same channels that surface early signal during tech shifts also reward hype-hopping and undermine builders who want to build for an audience that stays.",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": null,
    "tags": [],
    "body": "<!--\nSuggested metadata when adding to Neotoma:\ntitle: Degen culture in crypto and AI delivers signal at a cost to patient builders\nexcerpt: The same channels that surface early signal during tech shifts also reward hype-hopping and undermine builders who want to build for an audience that stays.\nslug: degen-culture-crypto-ai-builders-patience\ncategory: essay\npublished: false\n-->\n\nDegen culture shows up in crypto and, increasingly, in AI. It works as a fast-moving information layer during tech advances. It also creates a toxic psychological dynamic for builders who want to build with patience for an audience that intends to stay, not jump ship for the next hype cycle. The tradeoff is real: you get signal and shared context, but the same norms that move information fast also push builders and audiences toward short-term, hype-based decisions. That undermines patient building and the kind of audience relationship that doesn't depend on the new hot thing.\n\n## Where degen culture shows up\n\nIn crypto it's familiar. Alpha groups, CT, narrative cycles. People chase the next token, the next narrative, the next \"wave.\" Speed of information is high. In-group language and FOMO drive attention. The same pattern has taken root in AI. Model drops, agent frameworks, \"the next paradigm\" discourse. Same behaviors: alpha-chasing, rapid pivot to whatever is trending, shared belief that if you're not on the new thing you're behind.\n\nI've seen it in both. The channels that surface useful early signal are the same ones that normalize abandoning projects and audiences when the next hot thing appears. That's not a coincidence. It's the same culture.\n\n## The upside\n\nThere is real value. During tech shifts, degen culture moves information fast. You get early signal. Shared context. Rapid iteration on ideas. If you're trying to stay current, these channels help. They compress time between \"something changed\" and \"here's what people are doing about it.\" I draw on that. I don't want to pretend the upside isn't there.\n\n## The downside\n\nThe cost is psychological and strategic. Builders who ship slowly and care about sustained audience relationship get framed as \"missing the wave\" or \"not moving fast enough.\" The pressure is to pivot, ship fast and shallow, align with hype. That undermines the ability to build with patience. It also trains the audience. When the culture rewards leaving for the next hot thing, audiences learn to leave. Not on merit. On hype.\n\nSo builders who could serve a long-term audience instead optimize for degen attention. They chase narrative. They burn out or lose the very users who wanted something stable. The wrong assumption is that \"moving with the crowd\" is the only way to get signal, or that patient building is naive. It isn't. It's a choice the culture makes invisible.\n\n## Audience and long-term relationship\n\nMany users want a product or creator they can rely on over time. They don't want to rebuild context every six months when the ecosystem has \"pivoted.\" Degen culture pushes both builders and audiences toward short-term, hype-based decisions. That's a choice. It's not a law. The cost is when builders and audiences internalize degen norms as the only way to operate.\n\n## What this doesn't mean\n\nI'm not saying ignore crypto or AI discourse. I'm not saying never participate in fast-moving communities. The precision is this: the cost appears when you let the dynamic dictate your building or your audience strategy. When you notice you're optimizing for hype instead of for the people who want to stay, that's the moment to recalibrate.\n\n## Using the signal without becoming the dynamic\n\nYou can use the signal without letting it run you. Read the channels. Stay current. Then build for the audience you want. If that audience intends to establish a long-term relationship with what you make, build for that. Don't let the culture that rewards hype-hopping convince you that patient building is a mistake. It's not. The culture is optimized for speed of information. Your product and your audience might be optimized for something else.\n",
    "createdDate": "2026-02-13",
    "updatedDate": "2026-02-13",
    "summary": "- Degen culture in crypto and AI delivers fast-moving information during tech shifts but also rewards hype-hopping and short-term decisions.\n- The same channels that surface early signal normalize abandoning projects and audiences when the next hot thing appears.\n- Builders who want to build with patience for a long-term audience get framed as \"missing the wave\" or \"not moving fast enough.\"\n- Many users want a product or creator they can rely on over time; degen culture pushes both builders and audiences toward hype-based churn.\n- You can use the signal without letting the dynamic dictate your building or audience strategy."
  },
  {
    "slug": "show-me-all-documents-involving-company-x",
    "title": "Show me all documents involving Company X",
    "excerpt": "For analysts and researchers drowning in document volume, the query never quite works. The fix is a structured memory layer with entity resolution and stable IDs, not better search.",
    "published": false,
    "publishedDate": null,
    "category": "technical",
    "readTime": 5,
    "tags": [
      "neotoma",
      "truth-layer",
      "entity-resolution",
      "documents",
      "build-in-public"
    ],
    "createdDate": "2026-02-11",
    "updatedDate": "2026-02-11",
    "body": "If you work with hundreds of documents across projects, you've hit this wall. A partner asks for everything you have on Company X. You need due diligence, legal research, or a client brief. You run search. You get fragments: some contracts, maybe an email or two, a note from a call. You miss the memo that spelled the name \"ACME CORP,\" the annex that only said \"the counterparty,\" the report that referenced the deal by code name. You hand over an incomplete set or spend hours manually collating. Either way, the query \"show me all documents involving Company X\" never quite lands.\n\nThe failure isn't search quality in the abstract. It's that search runs over raw text. There is no single notion of \"Company X.\" There are strings. Your tools can't unify \"Acme,\" \"Acme Corp,\" and \"ACME CORP\" into one entity, or link that entity to contracts, emails, and notes across folders and tools. So retrieval is best-effort. You get what matches the string, not what belongs to the entity.\n\n## What would have to be true for that query to work\n\nFor \"show me all documents involving Company X\" to return a complete, coherent set, three things have to be true.\n\n**First, you need entity resolution.** Every document that mentions the company has to contribute to the same canonical entity. That entity gets a stable ID. Whether the source says \"Acme,\" \"Acme Corp,\" or \"ACME CORP,\" the system maps it to that ID. No duplicate \"companies\" that are really the same. No manual spreadsheets to track who is who.\n\n**Second, you need a single place those documents (or their extracted facts) live.** If contracts are in one tool, emails in another, and notes in a third, no search layer can reliably join them. The join has to happen in a store that already holds the resolved entities and the links from documents to those entities. That store is the memory layer.\n\n**Third, you need to query by structure, not only by keyword.** You want \"all documents involving entity ID X\" or \"all events where this company appears.\" That's a structural query: filter by entity, by relationship, by time range. Keyword search can approximate it. It can't guarantee it. Structure can.\n\nTogether, that's a structured memory layer with entity resolution and stable IDs. Documents are ingested once. Entities are resolved and linked. Queries run against that graph. Same question, same store, same result.\n\n## How Neotoma fits\n\nI'm building [Neotoma](https://github.com/markmhendrickson/neotoma) as a [truth layer](/posts/truth-layer-agent-memory) for AI tools: a place where typed, structured state lives with full provenance and deterministic updates. It's not a search engine. It's the layer underneath.\n\nWhen you upload or ingest documents, Neotoma extracts entities (people, companies, locations, events) and resolves them to canonical IDs. The same company in different formats and documents maps to one entity. Relationships (this contract involves this company, this email is from this contact) are first-class. So are timelines: when things happened, in what order.\n\nOnce that exists, \"show me all documents involving Company X\" becomes a real operation. You're not searching for a string. You're querying by entity ID and by the relationships that link documents to that entity. The system returns everything that references that canonical company, regardless of how the name was spelled or abbreviated in the source. You get one coherent set. You can then ask the AI to summarize it, compare terms, or build a timeline. The AI reads from the same graph. No re-upload, no re-paste, no manual reconciliation.\n\nThat turns into time saved on due diligence, legal research, and client work. You stop collating by hand. You stop missing the annex or the memo. You run a query and get an answer you can stand behind.\n\n## The tradeoff\n\nGetting there requires putting documents (or their extracted content) into a structured store and running entity resolution. That's ingestion and schema. It's more upfront than \"point search at a folder.\" The payoff is that the next query, and every query after, is deterministic and complete for that entity. You're not betting on keyword luck. You're querying state.\n\nFor analysts, researchers, consultants, and lawyers who already drown in document volume, that tradeoff is the one that actually scales. The a-ha is the moment \"show me all documents involving Company X\" works the first time, and you realize you can stop doing it the old way.\n\nNeotoma is in developer preview. I'm dogfooding it in my own workflows and building toward the use case above: one graph, entity resolution, and queries that run against structure instead of raw text. If that's the problem you have, that's the direction I'm building in.\n"
  },
  {
    "slug": "what-vs-how-truth-layer-context-graphs",
    "title": "Why the Truth Layer is about what exists, not how work gets done",
    "excerpt": "Personal memory stays deterministic and execution-agnostic because the Truth Layer models what exists, not how work gets done. Enterprise context graphs do the opposite.",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": 5,
    "tags": [
      "neotoma",
      "truth-layer",
      "architecture",
      "positioning",
      "build-in-public"
    ],
    "body": "[Full body omitted for length - 8000+ chars; will store via correct or second store if needed]",
    "createdDate": "2026-02-10",
    "updatedDate": "2026-02-10",
    "summary": "# Key takeaways: Why the Truth Layer is about what exists, not how work gets done\n\n- Enterprise context graphs (e.g. Glean) model how work gets done; the Truth Layer models what exists (canonical state, entities, provenance). Conflating them leads to probabilistic state and tight coupling to orchestration.\n- The \"what vs how\" distinction is architectural. \"What exists\" is deterministic and execution-agnostic. \"How work gets done\" needs process traces, next-step prediction, and often joint ownership with the execution layer.\n- Arvind Jain's context-graph framing and Glean validate the category (graph plus time) but clarify the fork. Personal memory needs determinism and user scope. Enterprise context needs aggregation and probabilistic patterns.\n- Positioning the Truth Layer as \"what exists\" lets any agent host (Cursor, ChatGPT, Claude) consume it via MCP without the graph drifting when you switch tools. The graph is not owned jointly with orchestration.\n- The same person can use both: a personal Truth Layer for \"my docs, my entities, my timeline\" and an employer context graph for \"how we work.\" They are complements, not competitors.",
    "shareTweet": "Why the Truth Layer is about what exists, not how work gets done.\n\nEnterprise context graphs model process and next-step prediction. Personal memory needs canonical state and provenance. Different problems, different designs.\n\nhttps://markmhendrickson.com/posts/what-vs-how-truth-layer-context-graphs H/t @jainarvind"
  },
  {
    "slug": "foundation-shared-development-processes",
    "title": "Shared development processes I use across repos",
    "excerpt": "I maintain a single submodule for workflow, conventions, and agent instructions so every repo gets the same behavior without copy-paste.",
    "published": false,
    "publishedDate": null,
    "category": "technical",
    "readTime": 4,
    "tags": [
      "foundation",
      "development-workflow",
      "cursor-rules",
      "build-in-public"
    ],
    "body": "I maintain a git submodule called [Foundation](https://github.com/markmhendrickson/foundation) that I use across my repositories. It holds shared development processes, conventions, and agent instructions so I don't repeat the same setup in every project.\n\n## Why I built it\n\nI work across several repos: ateles (this site and my agentic stack), Neotoma (the truth layer), and others. Each one needed the same basics: branch strategy, PR process, code conventions, testing standards, security checks, and Cursor rules so agents behave consistently. Copy-pasting that into every repo was brittle. Updating a convention meant editing N copies. I wanted one place that could be pulled in as a submodule and configured per repo.\n\nFoundation is that place. It's composable and configurable. I enable only what each repo needs via `foundation-config.yaml`. Feature Units, release workflow, strategy frameworks, and validation are optional. The core (workflow, conventions, security, agent instructions) is what I use everywhere.\n\n## What's in it\n\n**Development workflow.** Git branch strategy, PR process, worktree setup, Feature Unit workflow, release orchestration. I follow the same flow whether I'm shipping a feature in ateles or a release in Neotoma.\n\n**Code and documentation conventions.** Naming patterns, style guides for TypeScript, SQL, YAML, shell. Documentation structure and writing style. One source of truth so I don't have to remember which repo uses snake_case and which uses kebab-case.\n\n**Security.** Pre-commit audits, protected paths, credential handling. Repo-specific config for what must never be committed.\n\n**Agent instructions.** Cursor rules and commands live in Foundation. Consuming repos run a setup script that symlinks `.cursor/rules/` and `.cursor/commands/` to the submodule. When I update a rule in Foundation, I pull the submodule in each repo and the change applies everywhere. That's how ateles gets its rules: many are foundation rules, plus repo-specific rules in `docs/`.\n\n**Self-adaptive behavior.** Foundation includes a rule that asks agents to learn from interventions. When I resolve a stopping point (e.g. \"yes, always update related tasks for financial transactions\"), the agent can suggest a new rule or skill, I approve it, and it gets added. Over time the same questions come up less often because the behavior is encoded.\n\n## How I'm approaching the build\n\nI use Foundation in ateles as the primary source for generic agent constraints, risk management, document loading order, and cursor rules sync. Ateles extends with repo-specific rules (persistence, data, email triage, content style, MCP access). The split is deliberate: global behavior in Foundation, domain and repo specifics in ateles.\n\nI keep Foundation generic on purpose. It doesn't reference ateles, Neotoma, or any specific product. That way I can add it to a new repo, set `foundation-config.yaml`, run the setup script, and get consistent workflow and agent behavior without editing Foundation itself.\n\nSubmodule updates are manual. When I change Foundation I commit and push from the submodule, then in each consuming repo I run `git submodule update --remote foundation` (or the sync script). I'm not trying to version-pin Foundation tightly; I want the latest processes everywhere unless I explicitly hold a repo back.\n\n## What I'm learning\n\nThe main tension is between \"one size fits all\" and \"this repo is special.\" So far, configuration and repo adapters handle it. Repo adapters in `foundation/config/repo_adapters/` let me override conventions or security paths per repo name. Most of the time the default is fine. When it isn't, I add an adapter or put the override in the repo's own `foundation-config.yaml`.\n\nAgent instructions are the highest-leverage part. Once a rule exists in Foundation and is symlinked, every repo that uses Foundation gets it. Fixing a rule once fixes it everywhere. The self-adaptive rule amplifies that: good interventions get turned into standing rules so I don't have to repeat them.\n\n## Where it lives\n\nRepository: [github.com/markmhendrickson/foundation](https://github.com/markmhendrickson/foundation). MIT licensed. I use it as a submodule; the README has install and sync instructions. If you run multiple repos and want shared workflow and agent behavior without maintaining N copies, the pattern is worth copying.\n",
    "heroImage": "foundation-shared-development-processes-hero.png",
    "heroImageStyle": "keep-proportions",
    "heroImageSquare": "foundation-shared-development-processes-hero-square.png",
    "createdDate": "2026-02-10",
    "updatedDate": "2026-02-10",
    "summary": "- I use a single git submodule (Foundation) for workflow, conventions, security, and agent instructions across multiple repos so I don't maintain N copies.\n- Foundation is composable and configurable: I enable only what each repo needs via foundation-config.yaml; Feature Units, release workflow, and strategy frameworks are optional.\n- Cursor rules and commands live in Foundation; consuming repos symlink .cursor/ to the submodule so one rule update applies everywhere after a submodule pull.\n- A self-adaptive rule lets agents suggest new rules or skills when I resolve a stopping point; I approve and the behavior is encoded for future sessions.\n- Repo adapters and foundation-config.yaml handle the tension between shared defaults and repo-specific overrides without forking Foundation.",
    "shareTweet": "I maintain Foundation, a shared dev-processes submodule I use across repos: workflow, conventions, security, Cursor rules. One place, configurable per repo. https://github.com/markmhendrickson/foundation",
    "ogImage": "og/foundation-shared-development-processes-1200x630.jpg"
  },
  {
    "slug": "neotoma-developer-release",
    "title": "Neotoma developer release",
    "excerpt": "The Neotoma truth layer is now available as a developer release. This post explains what it is, how to run it locally, and what the repo actually does.",
    "published": false,
    "publishedDate": null,
    "category": "technical",
    "readTime": 10,
    "tags": [
      "neotoma",
      "truth-layer",
      "mcp",
      "agent-memory",
      "build-in-public"
    ],
    "body": "[TRUNCATED - full body in parquet]",
    "heroImage": "neotoma-developer-release-hero.png",
    "heroImageStyle": "keep-proportions",
    "heroImageSquare": "neotoma-developer-release-hero-square.png",
    "createdDate": "2026-02-10",
    "updatedDate": "2026-02-10",
    "summary": "- Neotoma is open as a developer release: a truth layer for persistent agent memory that AI tools read and write via MCP.\n- Local setup uses Node 18/20, npm install, .env with NEOTOMA_STORAGE_BACKEND=local, then npm test and npm run watch or watch:full; no Supabase required.\n- The data model is source to interpretation to observation to entity snapshot, with hash-based entity resolution and typed relationships.\n- One unified store action accepts files (base64 + mime) or structured entities; correction and reinterpretation keep provenance and history.\n- The repo includes schema registry, timeline events, graph retrieval, OAuth for MCP, React frontend, CLI, and full test and health tooling.",
    "shareTweet": "Neotoma developer release: truth layer for persistent agent memory, exposed via MCP. Local setup, data model (source → observation → entity snapshot), and full action catalog in the post. https://github.com/markmhendrickson/neotoma",
    "ogImage": "og/neotoma-developer-release-1200x630.jpg"
  },
  {
    "slug": "focus-on-work-you-love-delegate-the-rest",
    "title": "Focus on the work you love and delegate the rest",
    "excerpt": "The highest-leverage move is to do more of what you enjoy and hand the rest to agents. I tell everyone the same thing: start incrementally and you buy back time and get better at the work you do not love.",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": 3,
    "tags": [
      "agents",
      "delegation",
      "productivity"
    ],
    "body": "- Figure out the part of your work you actually enjoy and wish you could focus on full time.\n- Use agents to delegate everything else. Start incrementally.\n- You buy back time for the work you love and get more efficient at the rest.\n- The advice holds up: comparative advantage and sustainability both favor it.\n- The only nuance is to delegate the right things. Judgment and relationship work stay with you.\n\nI tell everyone the same thing. It sounds like productivity advice but it is really a positioning move. If you do not choose what to focus on, your calendar and your attention will be chosen for you. The default is that everything that can land on you will land on you. Inbox, admin, follow-ups, formatting, research rabbit holes. The work you love gets whatever is left.\n\n## The cost of not choosing\n\nThe cost of not choosing is that you spend more time on the work you do not love and less on the work you do. That drags down both sustainability and leverage. You burn out on the wrong things. You get better at the wrong things. The part of your job that only you can do well, and that you actually care about, stays underinvested.\n\nSo the rule is simple: name the high-level slice of your job that you really enjoy. Then treat everything else as delegation surface. Not \"I will get to it later.\" Delegation. To other humans where it makes sense, and to agents where it makes sense. Email triage, drafting, summarization, data entry, routine research, formatting, scheduling. Agents are good at well-defined, repetitive, or pattern-heavy work. Use them for that. Keep judgment, relationships, and open-ended creativity with you.\n\n## One rule\n\nThe advice is valid. Comparative advantage says specialize in what you are relatively best at; enjoyment and skill compound there. Sustainability says focus on what you can keep doing without burning out. Delegating the rest, incrementally, gives you both. The only caveat is to delegate the right things. Some tasks degrade when handed off: nuanced judgment, relationship-building, creativity under real constraints. So \"everything else\" means \"everything that is appropriate to delegate,\" not literally every task you dislike. You revisit that boundary as tools and your role change.\n\n## What I'm doing in practice\n\nIn my setup I run agents over a clear stack: a truth layer (structured memory), a strategy layer (goals and constraints), and an execution layer (email, calendar, DNS, wallet, and other side effects). I define the strategy. Agents read from the truth layer and call tools to execute. I triage inbox with an agent that drafts replies and suggests data to save. I delegate scheduling, follow-ups, and routine data entry. I keep the work I care about: architecture, writing, product direction, and the parts of building that are exploratory rather than repetitive. The split is not perfect. Some things I still do by hand because the agent surface is not there yet or because the task is too fuzzy. But the direction is consistent. More of what I love, less of what I do not.\n\n## How I'm approaching it\n\nI treat delegation as incremental. I do not try to hand off everything at once. I add one workflow at a time: email triage, then task and contact persistence, then calendar, then something else. Each time I see what breaks, what needs guardrails, and what I can safely stop doing myself. I also revisit the boundary. Some tasks I thought were delegatable turned out to need a human in the loop. Some tasks I used to do myself are now fully delegated. The goal is to keep shifting the mix toward the work I want to be doing and to keep the rest running reliably in the background.\n",
    "heroImage": "focus-on-work-you-love-delegate-the-rest-hero.png",
    "heroImageStyle": "keep-proportions",
    "createdDate": "2026-02-09",
    "updatedDate": "2026-02-09",
    "summary": "- Name the part of your work you enjoy and wish you could focus on full time, then delegate the rest.\n- Use agents incrementally for well-defined, repetitive, or pattern-heavy work so you buy back time and reduce burnout.\n- Keep judgment, relationship-building, and open-ended creativity with you; delegate the right things, not everything you dislike.\n- Revisit the boundary between what you do and what you delegate as tools and your role change.\n- If you do not choose what to focus on, your calendar and attention will be chosen for you; treat delegation as a positioning move."
  },
  {
    "slug": "api-cli-first-agentic-products",
    "title": "Build API and CLI first so agents can consume your product",
    "excerpt": "Value on the agentic web accrues to infrastructure that lets agents do things, not to chat interfaces. Products that expose APIs and CLIs will be the ones agents and builders use.",
    "published": false,
    "publishedDate": null,
    "category": "",
    "readTime": 3,
    "tags": [
      "api",
      "cli",
      "agents",
      "product",
      "mcp"
    ],
    "body": "- Value on the agentic web accrues to those who enable agents to perform a wider range of jobs, not to those who control the primary chat interfaces.\n- Model access is commoditized. Differentiation happens in what agents can actually do, not which model they use.\n- Building human UI first optimizes for the wrong consumer. Agents will be the main consumers of many products, and they need APIs and CLIs, not screens.\n- Agentic creation is most efficient and autonomous when there is no human UI in the loop. Direct programmatic access reduces friction and scales.\n- The value is in the connection layer: MCP servers, tool providers, memory systems, and infrastructure that make agents reliable and auditable.\n- The web did not belong to the browser makers. It belonged to those who enabled new capabilities. The agentic web will follow the same pattern.\n- If your agents cannot do anything meaningful, owning the chat interface means nothing. Build API and CLI first so agents can consume your product.\n\nOn the agentic web, value accrues to those who enable agents to perform a wider range of jobs. Not to those who try to control the primary agentic interfaces. I wrote that in a [tweet](https://x.com/markymark/status/2019370921245417911) earlier this year, and it still captures how I think about product strategy. The implication for builders is straightforward: create products as API or CLI first. Not only because agentic creation is most efficient when there is no human UI involved, but because agents themselves will be the main consumers.\n\n## Why agents are the primary consumer\n\nModel access is already commoditized. Claude, GPT, Gemini, Llama. The models themselves are becoming interchangeable utilities. The differentiation happens in what agents can actually do. A product that only offers a chat UI or a dashboard is invisible to an agent. A product that exposes an API or a CLI can be called, composed, and automated. When the primary interface is a text box and the value is in what happens after you hit send, the products that get used are the ones that sit behind that send: databases, transaction processors, calendars, email, task systems, memory layers. Agents do not click through wizards. They call tools.\n\nSo if you are building for the future, you are building for agent consumption. That does not mean you never build a human UI. It means you design the surface that agents use first. The human UI, when you add it, becomes one client of the same backend. API and CLI are not afterthoughts. They are the primary surface.\n\n## Efficiency and autonomy without human UI\n\nAgentic creation is most efficient when there is no human in the loop. Every click, every form, every approval step is a bottleneck. When an agent can read from a memory layer, reason over strategy, and execute via MCP or API calls, the same workflow that would take a human twenty steps becomes a single chain of tool calls. Autonomy follows. The user sets policy and approves exceptions. The agent operates within that policy. Products that only offer a human UI force the agent to simulate a human (browser automation, screen scraping). That is fragile and slow. Products that offer an API or CLI let the agent behave like an agent. Fast, deterministic, composable.\n\nChatGPT tried to become the interface layer. But users do not want a walled garden. They want agents that can write to their databases, execute transactions, manage their infrastructure, and integrate with their existing tools. The value is moving to the connection layer. MCP servers that give agents access to APIs. Tool providers that let agents execute real operations. Memory systems that make agents contextually aware. Infrastructure that makes agents reliable and auditable. None of that requires a proprietary chat UI. It requires surfaces that agents can call.\n\n## What I see in my own stack\n\nThe model I use matters less. What matters is whether my agents can read my files and documents, send transactions, query my calendar and email, or update my task list. A memory layer gives them reliable state across conversations. The interface is just a text box. The value is in what happens after I hit send. I run agents that combine wallet MCPs, calendar, email, and a structured truth layer. None of those are chat products. They are APIs, CLIs, and MCP servers. The chat is the place where I prompt and approve. The work is done by the connection layer.\n\n## The web did not belong to the browser makers\n\nCompanies building narrow AI chat products are optimizing for yesterday's paradigm. The web did not belong to the browser makers. It belonged to those who enabled new capabilities: Google made information discoverable, Facebook connected people, Amazon built commerce infrastructure, AWS hosted applications, Stripe processed payments. The agentic web will follow the same pattern. Owning the chat interface means nothing if your agents cannot do anything meaningful. Value concentrates in the infrastructure that enables agent capabilities.\n\nSo build API and CLI first. Assume agents will be the main consumers. Make your product callable, composable, and policy-friendly. Add a human UI when it serves discovery or one-off use. The agents will use the API. The humans will use whatever interface you give them. But the product that wins is the one that both can use, and the one that agents can use without a human in the loop.\n",
    "heroImage": "api-cli-first-agentic-products-hero.png",
    "heroImageStyle": "keep-proportions",
    "heroImageSquare": "api-cli-first-agentic-products-hero-square.png",
    "createdDate": "2026-02-09",
    "updatedDate": "2026-02-09",
    "ogImage": "og/api-cli-first-agentic-products-1200x630.jpg",
    "summary": "- Value on the agentic web accrues to those who enable agents to do more jobs, not to those who own the chat interface.\n- Model access is commoditized. Differentiation is in what agents can actually do, so products need API and CLI surfaces.\n- Agents will be the main consumers of many products. Building human UI first optimizes for the wrong consumer.\n- Agentic creation is most efficient and autonomous when there is no human UI in the loop. APIs and CLIs reduce friction and scale.\n- The connection layer (MCP servers, tool providers, memory systems, infrastructure) is where value concentrates.\n- The web belonged to enablers (Google, Stripe, AWS), not browser makers. The agentic web will follow the same pattern.\n- Build API and CLI first. Add human UI when it serves discovery or one-off use. Assume agents will call your product."
  },
  {
    "slug": "enrollment-sponsoring-ecosystem-purchases-contract",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "Got sBTC? \n\n\n@LeatherBTC\n is sponsoring all Stacks contract calls that involve sBTC (swaps, app purchases, rewards enrollment and more) so you don't even need STX in your wallet.\n\nAccess the entire Stacks ecosystem with just bitcoin",
    "createdDate": "2024-12-20",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/2019370921245417911"
  },
  {
    "slug": "ecosystem-bitcoin-good-app-dev",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "Very good for the Bitcoin app dev ecosystem 👏👏👏",
    "createdDate": "2024-12-19",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/2019370921245417911"
  },
  {
    "slug": "live-56-v6",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "Now live in v6.56.0 ⚡️",
    "createdDate": "2024-12-19",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1869661216974749998",
    "tweetMetadata": {
      "likes": 15578,
      "retweets": 2963,
      "replies": 363,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "accessing-protocols-category-stacking-bitcoin",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "Earn BTC with your BTC! \n\nI'm very excited about \"Bitcoin yield\" as a general category, and proud to launch this unified portal for accessing sBTC rewards, Stacking and other yield protocols to come via our trusted \n@LeatherBTC\n brand",
    "createdDate": "2024-12-18",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1869661216974749998",
    "tweetMetadata": {
      "likes": 15578,
      "retweets": 2963,
      "replies": 363,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "tweet",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "👀🧑‍🌾₿ @LeatherBTC https://t.co/pP0tHSjFtr",
    "createdDate": "2024-12-17",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1869661216974749998",
    "tweetMetadata": {
      "likes": 15578,
      "retweets": 2963,
      "replies": 363,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "tweet-1",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "👀🌉₿ @LeatherBTC https://t.co/g03t5rmLmo",
    "createdDate": "2024-12-16",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1869661216974749998",
    "tweetMetadata": {
      "likes": 15578,
      "retweets": 2963,
      "replies": 363,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "functionality-prototypes-prompting-prompting-designer",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 3,
    "tags": [
      "tweet"
    ],
    "body": "If you're a software engineer, you should be prompting for functionality far more than editing, writing or even viewing code these days. If you're a designer, you should be prompting for live prototypes far more than pushing pixels in Figma.",
    "createdDate": "2026-02-06",
    "updatedDate": "2026-02-09"
  },
  {
    "slug": "interchangeable-commoditized-interfaces-themselves-becoming",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "On the agentic web, value accrues to those who enable agents to perform a wider range of jobs. Not to those who try to control the primary agentic interfaces.\n\nModel access is already commoditized. Claude, GPT, Gemini, Llama. The models themselves are becoming interchangeable utilities. The differentiation happens in what agents can actually do.\n\nChatGPT tried to become the interface layer. But users don't want a walled garden. They want agents that can write to their databases, execute transactions, manage their infrastructure, and integrate with their existing tools.\n\nThe value is moving to the connection layer. MCP servers that give agents access to APIs. Tool providers that let agents execute real operations. Memory systems that make agents contextually aware. Infrastructure that makes agents reliable and auditable.\n\nI see this in my own stack. The model I use matters less. What matters is whether my agents can read my files and documents, send transactions, query my calendar and email, or update my task list. Neotoma gives them reliable memory across conversations. The interface is just a text box. The value is in what happens after I hit send.\n\nCompanies building narrow AI chat products are optimizing for yesterday's paradigm. The web didn't belong to the browser makers. It belonged to those who enabled new capabilities: Google made information discoverable, Facebook connected people, Amazon built commerce infrastructure, AWS hosted applications, Stripe processed payments.\n\nThe agentic web will follow the same pattern. Owning the chat interface means nothing if your agents can't do anything meaningful. Value concentrates in the infrastructure that enables agent capabilities.",
    "createdDate": "2026-02-05",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/2019370921245417911"
  },
  {
    "slug": "broadcasting-interactions-transaction-interfaces-operations",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "Crypto and AI need each other to overcome legacy systems and UX patterns.\n\nI've begun migrating all my crypto interactions to agentic interfaces. They handle both \"write\" operations (like transaction signing and broadcasting) and complex \"read\" operations far better than I can.\n\nAsk an agent to collect and synthesize on-chain data for understanding DeFi protocols, staking dynamics, or complex asset classes. The kind of analysis that takes exorbitant amounts of time to do right and makes most human heads spin.\n\nConversational UX beats human point-and-click UI for financial strategy and execution. It synthesizes complex inputs and external context. It personalizes recommendations. It implements plans and handles bookkeeping automatically.\n\nThe reverse is also true. Crypto gives AI agents what traditional systems can't.\n\nAgents move more efficiently on transparent, permissionless rails. Every transaction is auditable. Every protocol is composable. No permission gates or approval workflows.\n\nTradFi was designed to slow humans and block bots. It lacks APIs. It requires phone calls and paperwork. Crypto treats agents as first-class citizens.\n\nProgrammable money on open rails means agents can execute complex financial strategies without human intervention. They can rebalance portfolios, stake assets, provide liquidity, and manage treasury operations at speeds and scales impossible in traditional finance.\n\nThis creates a feedback loop. Better agent UX brings more users to crypto. More users create demand for agent-native infrastructure. That infrastructure makes crypto more useful for agents. The systems evolve together.",
    "createdDate": "2026-02-05",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/2019370921245417911"
  },
  {
    "slug": "reliability-generally-staleness-security-versions",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "Early versions of Claude Code used RAG + a local vector db, but we found pretty quickly that agentic search generally works better. It is also simpler and doesn't have the same issues around security, privacy, staleness, and reliability.",
    "createdDate": "2026-02-04",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/2019370921245417911"
  },
  {
    "slug": "rt",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @karpathy: https://t.co/Lb6T42n5jl",
    "createdDate": "Thu Jan 01",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1869661216974749998",
    "tweetMetadata": {
      "likes": 15578,
      "retweets": 2963,
      "replies": 363,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "announce-bitflow-excited-trading-first",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @bsdmoney: 🚨 sBTC/BSD trading pool is live on @bitflow\n\nVery excited to announce our first trading pool. Bitflow will give BSD more liqu…",
    "createdDate": "Wed Dec 10",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1998816587869310982",
    "tweetMetadata": {
      "likes": 45,
      "retweets": 13,
      "replies": 6,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "happening-bitcoin-circle-launch-stacks",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @Stacks: gm @circle 🟠\n\nCircle's first move into Bitcoin DeFi is happening on Stacks, the only Bitcoin Layer 2 in their pilot launch of C…",
    "createdDate": "Tue Nov 18",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1998816587869310982",
    "tweetMetadata": {
      "likes": 45,
      "retweets": 13,
      "replies": 6,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "synthetic-bitcoin-digital-private-dollar",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @bsdmoney: The Bitcoin Synthetic Dollar (BSD or \"Based Dollar\") is live on Stacks. Our private beta starts today.\n\nBSD is a digital doll…",
    "createdDate": "Mon Nov 17",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1998816587869310982",
    "tweetMetadata": {
      "likes": 45,
      "retweets": 13,
      "replies": 6,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "smoother-browser-leather-faster-mobile",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @LeatherBTC: Leather mobile just got a major glow up ✨\n\nOur in app Browser is now smoother, faster, and packed with more magic.\n\n🧡 Explo…",
    "createdDate": "Wed Aug 06",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1998816587869310982",
    "tweetMetadata": {
      "likes": 45,
      "retweets": 13,
      "replies": 6,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "available-android-leather-wallet-apple",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @TheApeBitcoiner: leather Wallet @LeatherBTC is now a available on Apple App Store and Android play store 🔥",
    "createdDate": "Thu Jun 05",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1998816587869310982",
    "tweetMetadata": {
      "likes": 45,
      "retweets": 13,
      "replies": 6,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "leather-rebuild-ground-mobile-about",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @LeatherBTC: In Ep. 55, we talk with @markymark about what makes the new Leather Mobile app a true rebuild from the ground up.\n\nPlus, @R…",
    "createdDate": "Thu Jun 05",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1930501448833355974",
    "tweetMetadata": {
      "likes": 30,
      "retweets": 8,
      "replies": 5,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "wherever-mobile-bitco-alex-take",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @ALEXLabBTC: 🚀 @LeatherBTC  just went mobile📱🔥\n\nNow you can take ALEX, @Brotocol_xyz and @LisaLab_BTC with you wherever you go 🏃 \n\nBitco…",
    "createdDate": "Tue Jun 03",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1930501448833355974",
    "tweetMetadata": {
      "likes": 30,
      "retweets": 8,
      "replies": 5,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "great-looks",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "Looks great!",
    "createdDate": "Fri May 30",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1930501448833355974",
    "tweetMetadata": {
      "likes": 30,
      "retweets": 8,
      "replies": 5,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "screenshot-giveaway-holding-leather-random",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @Dilletantte: Giveaway: 1 random winner \n\n🏆: @LeoCoinSTX NFT in leather hat.\n\nHow to enter: 👇\n\n1️⃣ show a screenshot of you holding $FLA…",
    "createdDate": "Thu May 29",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1930501448833355974",
    "tweetMetadata": {
      "likes": 30,
      "retweets": 8,
      "replies": 5,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "tweet-2",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "https://t.co/JMjcn6x3Pt",
    "createdDate": "Mon Feb 09",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1930501448833355974",
    "tweetMetadata": {
      "likes": 30,
      "retweets": 8,
      "replies": 5,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "mobile-people-booth-find-http",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @marshallmixing: THE NEXT 5 PEOPLE WHO FIND ME AT THE @STACKS BOOTH AND SHOW ME THEIR @LEATHERBTC MOBILE APP WILL GET $10 IN sBTC 🖤 http…",
    "createdDate": "Thu May 29",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1928037433003618456",
    "tweetMetadata": {
      "likes": 52,
      "retweets": 7,
      "replies": 6,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "dropped-leather-diving-lounge-mobile",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @LeatherBTC: Leather's new mobile app just dropped, and we're diving deep with @markymark on today's Leather Lounge!\n\n🔸 Unlock BTC yield…",
    "createdDate": "Wed May 28",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1928037433003618456",
    "tweetMetadata": {
      "likes": 52,
      "retweets": 7,
      "replies": 6,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "leather-big-day",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "A big day for Leather! 🚀",
    "createdDate": "Wed May 28",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1928037433003618456",
    "tweetMetadata": {
      "likes": 52,
      "retweets": 7,
      "replies": 6,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "rt-1",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @LeatherBTC: 📲",
    "createdDate": "Wed May 28",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1928037433003618456",
    "tweetMetadata": {
      "likes": 52,
      "retweets": 7,
      "replies": 6,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "eliminates-impressive-dashboard-provides-stacking",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @mrwagmibtc: The new @LeatherBTC @Stacks Stacking GUI provides an impressive dashboard that eliminates the need to have a million browse…",
    "createdDate": "Tue May 27",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1928037433003618456",
    "tweetMetadata": {
      "likes": 52,
      "retweets": 7,
      "replies": 6,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "vegas-anon-see-rt",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @LeatherBTC: See you in Vegas, anon 🖤 https://t.co/z7xOMhXiBF",
    "createdDate": "Mon May 26",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1927008687052186072",
    "tweetMetadata": {
      "likes": 103,
      "retweets": 21,
      "replies": 29,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "bitcoin-browser-landing-leather-mobile",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @godfred_xcuz: GET LEATHER ON YOUR BROWSER AND MOBILE AND GROW YOUR BITCOIN\n\nNew @LeatherBTC Landing Page is 🔥 \n\nhttps://t.co/8z9oFT3Df7…",
    "createdDate": "Thu May 22",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1927008687052186072",
    "tweetMetadata": {
      "likes": 103,
      "retweets": 21,
      "replies": 29,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "comprehensive-experience-generating-integrated-seamlessly",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "We've released an entirely new experience for @LeatherBTC today, just in time for the opening of sBTC Cap-3.\n\nEnjoy the most comprehensive place to pursue yield-generating strategies with BTC and STX on Stacks, all in one place, seamlessly integrated with your wallet.",
    "createdDate": "Thu May 22",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1927008687052186072",
    "tweetMetadata": {
      "likes": 103,
      "retweets": 21,
      "replies": 29,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "hendrickson-revealed-leather-release-mark",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @LeatherBTC: Ep 53 - Leather's Next Big Release Revealed! With Mark Hendrickson https://t.co/s3gUsfeHTh",
    "createdDate": "Wed May 21",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1927008687052186072",
    "tweetMetadata": {
      "likes": 103,
      "retweets": 21,
      "replies": 29,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "misunderstood-explores-aspects-bitcoin-relatio",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @longstreetBTC: In honor of Earth Day, today's @LeatherBTC Lounge explores one of the most misunderstood aspects of Bitcoin: its relatio…",
    "createdDate": "Tue Apr 22",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1927008687052186072",
    "tweetMetadata": {
      "likes": 103,
      "retweets": 21,
      "replies": 29,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "monetization-deorganized-giveaways-takeover-creator",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @LeatherBTC: Ep 48 - The DeOrganized Takeover: Creator Monetization, Web3 Gaming, and BTC On-Air Giveaways https://t.co/2W7nYJfSea",
    "createdDate": "Tue Apr 15",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1912169637258608659",
    "tweetMetadata": {
      "likes": 30,
      "retweets": 37,
      "replies": 3,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "moderating-thrilled-legends-stacked-during",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @marshallmixing: The most STACKED STACKS panel ever?!\n\nI'm thrilled to be moderating this panel of @Stacks legends during the How Stacks…",
    "createdDate": "Sat Apr 12",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1912169637258608659",
    "tweetMetadata": {
      "likes": 30,
      "retweets": 37,
      "replies": 3,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "hendrickson-designing-bitcoin-beyond-future",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @melbelle_btc: Bitcoin and Beyond Ep11: Designing the Future of Money with Mark Hendrickson https://t.co/PKzZMdcH7D",
    "createdDate": "Fri Apr 11",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1912169637258608659",
    "tweetMetadata": {
      "likes": 30,
      "retweets": 37,
      "replies": 3,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "breakthroughs-liquidity-bitcoin-episode-taproot",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @LeatherBTC: Ep 47 - Bitcoin DeFi Breakthroughs: Hadan from ALEX Labs Talks AI UX, DAMM Liquidity, and USDT via Taproot\n\nIn this episode…",
    "createdDate": "Thu Apr 10",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1912169637258608659",
    "tweetMetadata": {
      "likes": 30,
      "retweets": 37,
      "replies": 3,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "designing-nietzsche-studying-bitcoin-episode",
    "title": "",
    "excerpt": "",
    "published": false,
    "publishedDate": null,
    "category": "tweet",
    "readTime": 1,
    "tags": [
      "tweet"
    ],
    "body": "RT @melbelle_btc: From studying Nietzsche to designing a Bitcoin wallet.\n\nIn this week's episode of Bitcoin and Beyond, I talk to @markymar…",
    "createdDate": "Tue Apr 08",
    "updatedDate": "2026-02-09",
    "linkedTweetUrl": "https://x.com/markymark/status/1912169637258608659",
    "tweetMetadata": {
      "likes": 30,
      "retweets": 37,
      "replies": 3,
      "quote_count": 0,
      "bookmark_count": 0,
      "is_reply": false,
      "is_retweet": false,
      "is_quote": false,
      "lang": "",
      "author_name": "",
      "images": []
    }
  },
  {
    "slug": "stacks-market-opportunity-leather-lesson",
    "title": "Stacks' Market Opportunity Is Developer- and Institutional-Led (A Lesson from Leading Leather)",
    "excerpt": "Bitcoin and L2 adoption turn on a market vs government split: Stacks' opportunity is developer- and institutional-led, not consumer narrative or regulatory rescue. One of my key errors at Leather was over-investing in the consumer story instead of that layer.",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": 3,
    "tags": [
      "stacks",
      "bitcoin",
      "leather",
      "crypto",
      "product-strategy"
    ],
    "body": "Market vs government frame. Stacks opportunity is developer- and institutional-led. Leather lesson: over-invested in consumer.",
    "createdDate": "2026-02-07",
    "updatedDate": "2026-02-07"
  },
  {
    "slug": "agentic-wallets-and-the-truth-layer",
    "title": "Agentic Wallets and the Truth Layer",
    "excerpt": "Agentic wallets as execution layer: commands in, domain events out, truth layer underneath. Why execution without a memory substrate is fire-and-forget—and how to get action with accountability.",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": 5,
    "tags": [
      "agentic",
      "truth-layer",
      "neotoma",
      "wallets",
      "execution"
    ],
    "body": "Wallet as execution adapter. Domain events flow to truth layer. Strategy Layer, Execution Layer, Truth Layer architecture.",
    "createdDate": "2026-02-07",
    "updatedDate": "2026-02-07",
    "summary": "- **Agentic wallet** as execution layer: takes commands, performs side effects via adapters, emits domain events—never mutates truth directly.\n- Truth layer under the wallet: every execution becomes events → reducers → updated state; execution is auditable, queryable, recoverable.\n- \"Wallet an agent can call\" (tool) vs \"wallet as executor above a truth layer\": the second gives accountability and provenance.\n- Neotoma architecture: Strategy Layer (what should happen) → Execution Layer (Agentic Wallet + domain agents) → domain events → Neotoma reducers.\n- Dogfooding in Ateles: BTC wallet MCP as one execution adapter; goal is for outcomes to flow as domain events into a single truth layer.\n- Trust: agentic wallets without a truth layer are fire-and-forget; with it, you get action with accountability—trace, audit, fix."
  },
  {
    "slug": "dhh-clankers-truth-layer",
    "title": "Do AI agents need specialized tools?",
    "excerpt": "DHH showed agents can act with zero MCPs. The question is whether that generalizes. An analysis of execution vs memory, and where specialized tools still matter.",
    "published": false,
    "publishedDate": "2026-02-06",
    "category": "building",
    "readTime": 5,
    "tags": [
      "dhh",
      "truth-layer",
      "agent-memory",
      "neotoma"
    ],
    "body": "# Do AI agents need specialized tools?\n\nDavid Heinemeier Hansson ran an AI agent on OpenClaw with zero accommodations: no skills, no MCPs, no APIs. The agent signed up for HEY, created a Fizzy board with five idea cards and web-sourced images, joined Basecamp via invite, and completed every step without corrections. He used Claude Opus 4.5 in an isolated Proxmox VM, separate from his personal data.\n\nHis argument: agent-specific tooling (MCPs, CLIs, APIs) is a temporary crutch. The future is agents using human affordances (web, email, standard UIs). Like self-driving cars that eventually won't need special equipment. Human interfaces will be enough.\n\nThe demo is convincing. The question is whether it generalizes. Do agents need specialized tools or not?\n\n## The case against specialized tools\n\nDHH's experiment is strong evidence for execution. The interfaces agents use to do things (click, submit, call services, automate the browser) may not need to be agent-specific. Agents can navigate human UIs. They don't obviously need a separate API for every product. If that holds, a lot of execution-layer tooling (browser automation, API wrappers, product-specific MCPs) could indeed be a crutch that disappears as models get better at using the same interfaces humans use.\n\nSo for doing things, the answer tilts toward no. Agents may not need specialized execution tools.\n\n## The distinction: execution vs memory\n\nSpecialized tools aren't one category. Execution tools let the agent act. Memory tools let the agent (or the user) read and write structured state: entities, timelines, canonical records, provenance. No clicking, no form-filling. Just: what does the user know, in a form that can be queried and updated?\n\nThe question \"do agents need specialized tools?\" splits. For execution, DHH's thesis says maybe not. For memory, the question is different. Is there a human interface that gives an agent (or a human) a single, cross-source view of structured truth? If not, then for that job there is no human UI to navigate. Something has to provide that view.\n\n## What human interfaces don't provide\n\nHuman interfaces are per-source. Gmail has its UI. Drive has its UI. Notion, local files, calendar, each has its own. There is no human interface that shows:\n\n- All my entities across all my data.\n- Timeline of events from every source.\n- A resolved canonical list of people and companies.\n\nThose are aggregated, normalized views. They don't exist in any single app. The agent's options are: navigate every source each time and infer structure itself (costly, inconsistent, no single source of truth), or read from something that already does that aggregation. A specialized interface for cross-source structured memory isn't a substitute for a human UI. It's a substitute for the agent re-crawling and re-deriving structure from dozens of human UIs every time.\n\nSo for memory, the answer tilts toward yes. Agents (and users) need a way to get a view that no single human UI provides.\n\n## What the experiment didn't test\n\nDHH's demo was one agent, one platform, in a sandbox. The agent created new accounts. It didn't reason over existing personal data: invoices, contacts, documents scattered across tools. For \"agent reasons over my fragmented data,\" there is no single human UI. You either re-query everything each time or you have a layer that holds normalized, queryable state.\n\nThe demo was signup and collaboration. It didn't address \"reason over my tax documents,\" \"who are all my vendors across my files,\" or \"timeline of my contracts.\" For those, structured, queryable, provenance-backed memory is the requirement. Human UIs are per-app. They don't aggregate.\n\nHe also isolated the agent from his data. That pattern (bounded access instead of full machine access) implies a controlled interface to approved data. Not \"the agent surfs my whole machine,\" but \"the agent reads and writes through a defined layer.\" That's still a form of specialized interface.\n\n## Where specialized interfaces still matter\n\nSynthesis: agents may not need specialized tools for execution. They can use human affordances. For cross-source structured memory, and for bounded access to the user's data, they do. No single human UI gives the aggregated view. And \"give the agent a bounded memory layer you control\" is a different design than \"let the agent use only human UIs with no intermediate layer.\"\n\nMulti-tool users (Cursor, ChatGPT, Claude, OpenClaw) add another wrinkle. If \"what I know\" lives only inside each platform's context, it's siloed. One truth layer that any agent can read and write is again a specialized interface: it's the way to get a view that doesn't exist in any one app.\n\nSo the analysis lands here. Execution: specialized tools may be optional as agents get better at human UIs. Memory and bounded access: specialized tools (or a dedicated layer with an interface) still matter, because the view they provide doesn't exist elsewhere.\n\n## What I'm building\n\nThis framing is why I'm building [Neotoma](https://github.com/markmhendrickson/neotoma). It's a user-owned, cross-platform, structured truth layer. Not execution. Memory. The MCP (or whatever interface) is the way in to a view only a dedicated layer can provide: canonical entities, timelines, provenance, one place that any agent can query and update. Agents that use human affordances for doing things still need something like this for cross-source truth. I'm keeping that distinction explicit in all messaging: memory MCP vs execution MCP. Execution tooling may get simpler. The need for a place that holds cross-source truth does not.\n\nIf this analysis resonates, the work is in the open at the link above.\n",
    "heroImage": "dhh-clankers-truth-layer-hero.png",
    "heroImageStyle": "keep-proportions",
    "heroImageSquare": "dhh-clankers-truth-layer-hero-square.png",
    "createdDate": "2026-02-06",
    "updatedDate": "2026-02-06",
    "ogImage": "og/dhh-clankers-truth-layer-1200x630.jpg",
    "summary": "- DHH ran an agent with zero MCPs or APIs; it used only human UIs (HEY, Fizzy, Basecamp). The question is whether agents need specialized tools at all.\n- For execution (click, submit, automate), the answer tilts no. Agents can navigate human interfaces; execution-layer tooling may be a temporary crutch.\n- Specialized tools split into execution vs memory. Memory is structured state: entities, timelines, canonical records. No single human UI provides a cross-source view of that.\n- Human interfaces are per-app. No app shows \"all my entities,\" \"timeline from every source,\" or \"resolved list of people and companies.\" Those views require a dedicated layer or interface.\n- DHH's demo was one agent, sandbox data, signup and collaboration. It didn't test reasoning over existing fragmented data (invoices, vendors, contracts). For that, structured memory is required.\n- Bounded access (agent isolated from user data) implies a controlled interface to approved data. That's still a form of specialized interface.\n- Synthesis: execution tools may become optional; memory and bounded-access tools still matter because the view they provide doesn't exist in any single human UI.\n- I'm building Neotoma as that memory layer: user-owned, cross-source, one place any agent can query and update. Memory MCP vs execution MCP."
  },
  {
    "slug": "how-im-running-my-new-venture-as-ai-first",
    "title": "How I'm running my new venture as AI-first",
    "excerpt": "How I run Neotoma and the venture as AI-first: three-layer architecture (Truth/Strategy/Execution), agents via MCP, deterministic frameworks, content workflow, sovereignty.",
    "published": false,
    "publishedDate": null,
    "category": "draft idea",
    "readTime": 4,
    "tags": [],
    "body": "# How I'm running my new venture as AI-first\n\n- I run the venture (Neotoma, and the personal ops layer around it) as an AI-first operation: agents read memory, reason over strategy, and execute workflows. I intervene for irreversible decisions and high-level direction.\n- The system has three layers—Truth (memory), Strategy (reasoning), Execution (action)—each exposed to agents via MCP. Data stays local, typed, and provenance-tracked.\n- Content, ops, and product work flow through the same loop: context loading, AI-assisted generation, human refinement. I use audio notes, transcripts, and line-level feedback instead of writing from scratch.\n- Deterministic frameworks (decision rules, quarterly cadences, import protocols) constrain agent behavior so outputs are consistent and auditable. Same inputs yield same decisions.\n- Sovereignty is non-negotiable: I own the ledger, control what agents see, and avoid lock-in to any single model or UI. The stack is built for cross-platform agent access.\n\nMost ventures treat AI as a feature or a sidekick. You use ChatGPT for drafts, maybe an AI coding assistant, and otherwise run operations the old way: manual triage, human-only planning, tools that assume a single user at a keyboard. I'm taking the opposite approach. The new venture is run as an AI-first operation. Agents are not bolted on; they're the default interface to memory, strategy, and execution.\n\nThe cost of the default is friction. You repeat context, re-enter data, and personally gate every decision. As the sole founder, that doesn't scale. I need leverage: fewer reactive tasks, more deterministic processes, and a single source of truth that both I and the agents can use.\n\n## Three layers, one protocol\n\nThe architecture is three layers—Truth, Strategy, Execution—each cleanly separated and each accessible to AI via the MCP protocol.\n\n**Truth** is the memory substrate. In my setup that's 60+ normalized data types, 35,000+ records, with full provenance. Transactions, tasks, contacts, fitness logs, strategy docs: all of it lives in a typed, queryable store. Agents read it through MCP. They don't get a fuzzy summary; they get structured records, filters, and semantic search. When I ask about spending patterns or task load, the agent queries the ledger and reasons over real data.\n\n**Strategy** is the reasoning layer. Canonical strategy, tactics, and operations documents live here. Decision frameworks (e.g. liquidity regime scoring, prioritization rules) are explicit. Agents are instructed to consult these before making recommendations. That keeps behavior consistent and traceable: same situation, same framework, same output.\n\n**Execution** is the action layer. Data imports, PDF automation, sync jobs, form filling, calendar and task sync. Many of these run as scripts or services. Agents can trigger them, but irreversible actions (payments, sends, deletes) require confirmation. The rest runs autonomously within the defined protocols.\n\nThe important part: all of this is exposed through MCP. Whether I'm in Cursor, Claude, or ChatGPT, the agent connects to the same Truth, Strategy, and Execution surfaces. No custom integration per tool. The protocol is the contract.\n\n## How work actually gets done\n\nContent is a concrete example. I don't write essays from a blank page. I follow an AI-first content workflow: context loading (strategy docs, architecture, positioning), raw input (often voice notes, then transcription), then AI-assisted first draft. I edit with line-level feedback—\"This is vague, replace with X\"—and iterate until it sounds like me and matches the strategy. What used to take four to six hours for a long piece now lands in roughly one to two, with higher consistency.\n\nThe same pattern applies to ops. Task triage, financial review, and data imports are guided by written protocols. Agents load the right docs, apply the rules, and propose actions. I confirm or correct. The heavy lifting is in the frameworks and the data; the agent is the interface.\n\n## Determinism and sovereignty\n\nDeterminism matters. Ad-hoc agent behavior is hard to trust and harder to audit. So the system leans on deterministic workflows: same inputs, same process, same class of outputs. Strategy documents define the rules; agents apply them. When something goes wrong, we can trace it back to the rule or the data.\n\nSovereignty is the other pillar. The memory ledger is mine. It's stored locally, in formats I control, with full audit trails. No provider trains on it. No single model or UI is required. MCP lets different agents and front ends use the same Truth and Execution layers. That keeps the venture resilient to platform changes and aligned with how I want to run it: high leverage, minimal lock-in, human in the loop only where it matters.\n\n## What this enables\n\nRunning AI-first changes the baseline. Routine ingestion, triage, and reporting can run without me. Strategic choices—what to build, how to position, where to focus—still flow through me, but they're informed by agents that have full context. I get to act as architect and strategist instead of full-time operator.\n\nThe venture is small by design. The leverage comes from the stack: typed memory, explicit strategy, protocol-based access, and deterministic workflows. AI-first isn't about replacing the founder; it's about making the founder's time count.\n",
    "createdDate": "2026-01-27",
    "updatedDate": "2026-02-04"
  },
  {
    "slug": "truth-layer-agent-memory-old",
    "title": "Building a truth layer for persistent agent memory",
    "excerpt": "Agent memory is forgetful. Why I am building inspectable, structured memory for trustworthy agentic systems.",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": 4,
    "tags": [
      "neotoma",
      "agents",
      "personal-data"
    ],
    "body": "Neotoma: inspectable, replayable, user-controlled memory substrate. MCP and CLI-first. Cross-platform and privacy-first.",
    "createdDate": "2026-02-02",
    "updatedDate": "2026-02-04"
  },
  {
    "slug": "why-neotoma-exists",
    "title": "Why Neotoma exists",
    "excerpt": "AI systems have crossed a threshold. They increasingly act on personal data. That state is fundamentally untrustworthy. Neotoma is an attempt to treat personal data the way production systems treat state.",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": 4,
    "tags": [
      "neotoma",
      "ai",
      "memory",
      "truth-layer",
      "determinism"
    ],
    "body": "Personal data as operational state. Schema-first, contract-driven, explicit mutations, full provenance, replay as first-class.",
    "createdDate": "2026-02-02",
    "updatedDate": "2026-02-02"
  },
  {
    "slug": "neotoma-developer-preview",
    "title": "Neotoma developer preview",
    "excerpt": "Developer preview exposing CLI and MCP backed by OpenAPI. A contract-first system for ingesting fragmented personal data into structured, queryable state.",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": 4,
    "tags": [
      "neotoma",
      "developer-preview",
      "mcp",
      "cli",
      "openapi"
    ],
    "body": "Deterministic truth layer for personal data. CLI for humans, MCP for agents, OpenAPI as contract. No web UI. Infrastructure, not note-taking.",
    "createdDate": "2026-02-02",
    "updatedDate": "2026-02-02"
  },
  {
    "slug": "why-ai-memory-fails-without-truth-layer",
    "title": "Why AI memory fails without a truth layer and what Neotoma provides instead",
    "excerpt": "AI agents need persistent memory that works across platforms and survives session resets. Neotoma delivers this as a privacy-first, deterministic truth layer with cryptographic integrity, the missing foundation for reliable agent reasoning.",
    "published": false,
    "publishedDate": null,
    "category": "technical",
    "readTime": 8,
    "tags": [
      "neotoma",
      "ai",
      "memory",
      "truth-layer",
      "mcp"
    ],
    "body": "- Provider memory systems are platform-locked and ephemeral. They disappear when you switch tools or when providers reset sessions.\n- Personal data is fragmented across email attachments, cloud drives, and conversations. Current systems can't unify this data into queryable memory.\n- I'm building Neotoma to solve this as a privacy-first, deterministic, cross-platform truth layer. It transforms fragmented personal data into structured, queryable memory with three defensible differentiators: user-controlled ledger (no provider access), deterministic extraction (same input → same output), and cross-platform access via MCP protocol (Model Context Protocol, an open standard for connecting AI tools to external data sources).\n- This creates a verifiable domain that compensates for LLM inconsistency. The same operation always produces the same final state, creating objective, non-gameable results that agents can depend on.\n- Competitors can't pursue this path due to structural constraints: privacy-first conflicts with training-data business models, deterministic extraction conflicts with ML-first organizational identity, and cross-platform conflicts with lock-in revenue.\n- Neotoma enables the agentic web. It provides persistent, structured memory that agents read from and write to across sessions, tools, and platforms.\n- This is foundational infrastructure, not a productivity app. Neotoma serves as the substrate beneath agent-driven layers, similar to how operating systems provide the foundation for applications.\n\n**The fragmentation problem**\n\nIf you rely on provider memory systems, you're building on sand. ChatGPT memory disappears when OpenAI resets your session. Claude memory is locked to Anthropic's platform. You can't export it, verify it, or use it with other tools.\n\nWorse, your personal data is fragmented. Financial transactions live in email attachments. Fitness logs are in cloud drives. Calendar entries are in Google Calendar. Conversations are scattered across Slack, email, and chat interfaces. No system unifies this into queryable memory.\n\nWhen you ask an AI agent to analyze your spending patterns, it can't access your transaction history because that data lives in email attachments the agent never saw. When you switch from ChatGPT to Claude, you lose all context from previous conversations. When you use Cursor for coding, it can't access the financial data you discussed with ChatGPT.\n\nThis fragmentation prevents AI agents from reasoning across your complete information landscape. They operate in isolated silos with partial context. The result is inconsistent, unreliable agent behavior that can't be verified or reproduced.\n\n**The truth layer**\n\nI'm building Neotoma as a truth layer: a privacy-first, deterministic, cross-platform memory substrate that transforms fragmented personal data into structured, queryable memory with cryptographic integrity.\n\nI'm enforcing idempotence (the property that the same operation always produces the same result) through three mechanisms:\n\n1. **Canonicalization**: Normalize, sort, and round data consistently before processing. This ensures \"Ibercaja Bank\" and \"Ibercaja\" are treated identically.\n2. **Hashing**: Create deterministic identity from canonicalized data using cryptographic hash functions. The same input always produces the same hash.\n3. **Deduplication**: Prevent duplicate observations from the same source by comparing hashes.\n\nThe same operation always produces the same final state. This creates objective, non-gameable results that agents can depend on. It addresses the jagged intelligence problem (where LLMs excel in verifiable domains but struggle with consistency elsewhere) by creating a verifiable domain where consistency is guaranteed.\n\n**How it works**\n\nI'm using entity resolution via hash-based canonical IDs. When you mention \"Ibercaja\" in an email and \"Ibercaja Bank\" in a transaction record, Neotoma resolves both to the same entity using deterministic hashing (cryptographic functions that always produce the same output for the same input). This works across all data sources: email attachments, cloud drives, conversations, and manual entries.\n\nThe event-sourced architecture (a pattern where all changes are stored as a sequence of events, rather than updating records in place) creates an immutable audit trail. Every piece of memory is an event in a ledger you own. If you tell Claude about a crypto transaction, that's an event. If you upload a fitness log, that's a series of events. Events are facts about what happened, not interpretations. You can reconstruct the world-state at any point in time.\n\nCross-platform access works via MCP protocol (Model Context Protocol, an open standard that enables AI tools like ChatGPT, Claude, and Cursor to connect to external data sources). The same Neotoma memory works with ChatGPT, Claude, Cursor, and other AI tools. You can discuss a financial transaction with ChatGPT, then query the same data with Claude via MCP, then use it in Cursor for code generation. The memory persists across all interactions.\n\nI'm building Neotoma to support dual-path ingestion: file uploads where users explicitly provide documents for processing, and agent interactions where agents can store structured data via MCP actions. Memory grows incrementally as agent usage scales, creating a persistent knowledge graph that survives individual conversations and tool sessions.\n\n**When it doesn't work**\n\nNeotoma doesn't work if you need real-time collaboration features. It's designed for personal data, not shared workspaces. If you need Google Docs-style collaborative editing, use Google Docs.\n\nIt's not a replacement for provider memory systems. Provider memory is useful for conversation context within a single platform. Neotoma complements provider memory by providing persistent, cross-platform memory that survives session resets.\n\nThe deterministic approach requires upfront schema design. If you need rapid prototyping with unstructured data, you might prefer a more flexible system. Neotoma's strength is verifiable consistency, not flexibility.\n\nIt doesn't solve the LLM consistency problem entirely. LLMs will still hallucinate and make errors. Neotoma provides a verifiable substrate that compensates for LLM inconsistency, but it doesn't eliminate it.\n\n**How I'm approaching the build**\n\n1. **Building for ateles integration**: I'm building Neotoma to replace parquet MCP in my ateles project (a sovereign agentic operating system for personal workflow automation that uses a three-layer architecture: Truth Layer for memory, Strategy Layer for reasoning, and Execution Layer for actions). This gives me a real use case to validate the architecture with 60+ normalized data types and 35,000+ records.\n\n2. **MCP integration first**: I'm focusing on MCP integration patterns to ensure cross-platform access works correctly. Testing with ChatGPT, Claude, and Cursor to verify the same memory works across all tools.\n\n3. **Deterministic extraction**: I'm implementing rule-based extraction with deterministic hashing to ensure the same input always produces the same output. This is critical for the truth layer foundation.\n\n4. **Entity resolution**: I'm building entity resolution to unify fragmented data across sources. Testing with real data (transactions, contacts, events) to verify deterministic hashing works correctly.\n\n5. **Event-sourced architecture**: I'm using event-sourcing to create an immutable audit trail. Every piece of memory is an event, allowing reconstruction of world-state at any point in time.\n\n6. **Privacy-first from the start**: I'm building user-controlled isolation via Row-Level Security (RLS, a database security feature that restricts access to rows based on user identity) from MVP. No provider access, no training data usage.\n\n7. **Validating through usage**: I'm dogfooding Neotoma in ateles to validate the architecture works in practice. Real usage reveals edge cases and validates technical decisions.\n\n**Unresolved questions**\n\nI'm building Neotoma v1.0.0 with per-user isolation via Row-Level Security (RLS, a database security feature that restricts access to rows based on user identity). Team workspaces and shared memory are planned for v1.x to support small teams (2-20 people). The open question: how do we enable collaborative agent reasoning across team members while preserving privacy? Can we build a truth layer that allows agents to reason across shared team data without exposing individual user data?\n\nThe event-sourced architecture creates an immutable audit trail, but what's the retention policy? Do we keep all events forever, or do we need a compaction strategy? How do we handle GDPR right-to-deletion requests when data is cryptographically linked across events?\n\nThe deterministic approach compensates for LLM inconsistency, but what about LLM bias? If the canonicalization rules encode bias, the deterministic system will amplify it. How do we detect and correct bias in the truth layer itself?",
    "showMetadata": 1,
    "createdDate": "2025-01-15",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "three-layer-architecture-truth-strategy-execution",
    "title": "The Three-Layer Architecture: Truth, Strategy, Execution",
    "excerpt": "Most personal data systems mix concerns. The three-layer architecture separates what happened, what you want to achieve, and what actions to take.",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": 4,
    "tags": [
      "architecture",
      "systems-design",
      "neotoma"
    ],
    "body": "Most personal data systems mix concerns. They store financial data next to tasks next to contacts, with no clear separation between what happened, what you want to achieve, and what actions to take.\n\nThis creates complexity. It's hard to query across domains. It's hard to maintain consistency. It's hard to build reliable automation.\n\nThe three-layer architecture separates concerns:\n\n- **Truth Layer**: What actually happened (events, facts, immutable records)\n- **Strategy Layer**: What you want to achieve (goals, plans, desired outcomes)\n- **Execution Layer**: What actions to take (tasks, workflows, automation)\n\n## Truth Layer\n\nThe truth layer stores immutable facts about what happened. Financial transactions, health records, property documents, communication logs. These are events that occurred, not opinions or plans.\n\nThis layer is event-sourced. Each fact is a typed event with provenance. You can query it to understand current state. You can verify claims by checking the event log.\n\n## Strategy Layer\n\nThe strategy layer stores goals, plans, and desired outcomes. Budget targets, fitness goals, project milestones. These are what you want to achieve, not what happened.\n\nThis layer references the truth layer. Goals are measured against actual events. Plans are adjusted based on reality.\n\n## Execution Layer\n\nThe execution layer stores actions and workflows. Tasks, reminders, automation rules. These are what to do, not what happened or what you want.\n\nThis layer connects strategy to truth. Tasks move you toward goals. Workflows respond to events. Automation executes based on both current state and desired outcomes.\n\n## Benefits\n\n- **Clear separation**: Each layer has a distinct purpose\n- **Queryable**: Easy to ask questions across layers\n- **Composable**: Layers work together without tight coupling\n- **Maintainable**: Changes in one layer don't break others\n\nThe three-layer architecture provides structure for personal data systems that need to handle complexity while maintaining clarity.\n",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "sovereignty-in-ai-cryptographic-assurances",
    "title": "Sovereignty in AI: Cryptographic Assurances and Local-First Architecture",
    "excerpt": "AI tools control your data. Sovereign AI systems use cryptographic assurances and local-first architecture to return control to users.",
    "published": false,
    "publishedDate": "2026-04-02",
    "category": "essay",
    "readTime": 4,
    "tags": [
      "sovereignty",
      "privacy",
      "crypto",
      "ai",
      "local-first"
    ],
    "body": "AI tools control your data. They store it on their servers. They use it to train models. They decide when to delete it. You have no control, no ownership, no sovereignty.\n\nThis is a problem when you're using AI for sensitive work. Financial planning, health decisions, legal analysis. You need systems where you control the data, not the provider.\n\nSovereign AI systems use cryptographic assurances and local-first architecture to return control to users.\n\n## The Problem with Provider-Controlled Data\n\nMost AI tools store data on provider servers:\n\n- Providers control access\n- Data can be used for training\n- No way to export or delete\n- No cryptographic guarantees\n\nThis creates lock-in. You can't switch providers without losing context. You can't verify what data is stored. You can't control how it's used.\n\n## Cryptographic Assurances\n\nSovereign AI systems use cryptography to provide guarantees:\n\n- **Encryption**: Data is encrypted at rest and in transit\n- **Signatures**: Events are cryptographically signed\n- **Verification**: You can verify data integrity\n- **Ownership**: Cryptographic keys prove ownership\n\nThese assurances mean you can trust the system. You can verify data hasn't been tampered with. You can prove ownership. You can control access.\n\n## Local-First Architecture\n\nSovereign AI systems use local-first architecture:\n\n- **Local storage**: Data stored on your devices\n- **Sync when needed**: Data syncs across devices\n- **Offline capable**: Works without internet\n- **User-controlled**: You decide what to sync and where\n\nThis architecture returns control to users. You own the data. You control where it's stored. You decide what to sync.\n\n## Benefits\n\n- **Control**: You own and control your data\n- **Privacy**: Data isn't on provider servers\n- **Portability**: You can export and move data\n- **Trust**: Cryptographic guarantees provide assurance\n\nSovereign AI systems enable AI workflows where users maintain control, privacy, and ownership of their data.\n",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "building-systems-that-restore-self-sovereignty",
    "title": "Our agentic future needs user-owned memory",
    "excerpt": "After encountering AI memory limitations that led to platform lock-in and unreliable agent behavior, I'm building Neotoma—an autonomous cognitive substrate for sovereign users. This essay explains the personal need, Neotoma's goals and principles, and how I'm using it in Ateles, which I intend to open source.",
    "published": false,
    "publishedDate": "2026-01-29",
    "category": "essay",
    "readTime": 5,
    "tags": [
      "self-sovereignty",
      "neotoma",
      "mission",
      "crypto",
      "ai"
    ],
    "body": "# Why AI Agents Fail Without User-Owned Memory—and How to Fix It\n\n## Executive Synthesis\n\n- Current AI agents (ChatGPT, Claude) have effective short-term memory that falls off quickly and isn't accessible across platforms\n- The failure mode isn't \"I don't know\"—agents hallucinate memories, giving wrong answers and making wrong decisions\n- Without autonomous, user-owned data systems, we risk platform lock-in and delegating responsibilities to agents that can't execute reliably\n- Neotoma is an autonomous cognitive substrate that provides typed, event-sourced memory with full provenance, accessible via MCP protocol from any AI platform\n- The three-layer architecture (Truth → Strategy → Execution) enables sovereign agentic systems where users own the durable memory substrate, not platforms\n- Ateles demonstrates this in practice: 60+ normalized data types, 35,000+ records, agents that can read, reason, and execute while maintaining user sovereignty\n\n## Problem Framing (Stakes)\n\nIf you use AI agents without user-owned memory systems, you'll hit three failure modes:\n\n**Platform lock-in:** Memory trapped in one provider's system. Switch platforms, lose context. Move between tools, start over sharing context. You become dependent on a single provider's infrastructure.\n\n**Unreliable delegation:** Agents forget critical context (personal finances, tasks, communications, property, health records). You can't delegate responsibilities with confidence because agents hallucinate memories instead of saying \"I don't know.\" They take actions based on partial or hallucinated memories of your current and historical state.\n\n**Loss of sovereignty:** Platforms optimize for lock-in, telemetry, and single-model usage. You lose control over your data, your memory, your ability to reason across time and domains. Your agents can't remember what happened last month, last year, or in a different tool.\n\nThe common wrong assumption: \"AI providers will solve memory reliability.\" They won't. Their incentives align with lock-in, not portability. Their memory systems are designed for retention within their ecosystem, not cross-platform continuity.\n\n## Core Claim\n\n**Neotoma Model:** User-owned, deterministic memory substrate that persists independently of models, interfaces, and corporate ecosystems. It's the kernel of personal AI agency.\n\nThe rational equilibrium: Partner on inference and interfaces (use ChatGPT, Claude, Cursor for what they're good at), but own the durable, typed memory substrate and automation surfaces that all models and apps can use.\n\nNeotoma applies blockchain-like thinking to AI memory. Users incrementally build a shared view of their reality—personally and professionally, within companies or projects. Agents access this view no matter what tool they're using. If one agent from one platform does a job today, another agent from another platform three years from now can use the learnings or context of that first agent's work.\n\n## Evidence & Specifics\n\n**Personal experience:** I spent months providing context (personal finances, tasks, communications, property, health records) to agents that kept forgetting it. I couldn't delegate responsibilities with confidence. Moving between tools meant starting over sharing context.\n\n**Ateles implementation:** Three-layer architecture demonstrating Neotoma in practice:\n- **Truth Layer (Neotoma):** Currently using Parquet MCP provisionally with 60+ normalized data types, 35,000+ records. Planned migration to Neotoma for enhanced capabilities (entity resolution, event sourcing, graph relationships, timeline reconstruction, cross-domain joins).\n- **Strategy Layer:** Canonical strategy/tactics/operations documents, deterministic decision-making frameworks, strategic plans, quarterly reviews, portfolio reasoning, liquidity analysis. Reads from Truth Layer to reason about plans and decisions.\n- **Execution Layer:** Financial transactions, form submissions, external system sync, data import pipelines, background services, scheduled workflows, PDF automation, task synchronization. Acts on data from Truth Layer to carry out actions.\n\n**Concrete use cases:** Financial autonomy (agents track transactions, analyze portfolios, execute decisions), task orchestration (agents import, score, classify, sync tasks bidirectionally), administrative automation (form filling, contact extraction, document processing), health tracking (exercise data, workouts, meals, health reports), strategic planning (agents read strategy documents, apply frameworks, generate plans).\n\n**Timeline:** Started working on Neotoma in November 2025 after a sabbatical. Two months in, foundation is set.\n\n## Counter-Arguments & Limits\n\n**This doesn't apply if:**\n- You only use one AI platform and never plan to switch\n- You don't delegate critical decisions to agents\n- You're comfortable with platform lock-in and provider-controlled memory\n- You don't need cross-domain reasoning or long-term memory continuity\n\n**Known tradeoffs:**\n- Requires upfront setup and data ingestion (not instant like ChatGPT memory)\n- You own the infrastructure, which means you're responsible for backups, security, maintenance\n- Cross-platform capability requires MCP protocol adoption (not all tools support it yet)\n- Deterministic systems are less flexible than provider-optimized memory (but more reliable)\n\n**When this fails:**\n- If you need real-time collaboration across multiple users (Neotoma is personal-first, not collaborative)\n- If you require provider-managed compliance (HIPAA, SOC2) without self-hosting\n- If you want zero setup time (Neotoma requires initial configuration and data migration)\n\n**Architectural constraints:**\n- Event-sourced architecture means all data is immutable (good for auditability, requires different mental model than mutable databases)\n- Schema-first approach requires upfront schema design (not as flexible as schema-less systems)\n- Entity resolution via hash-based IDs means you can't change entity identities after creation (immutability requirement)\n\n## Actionable Steps\n\n1. **Assess your agent memory pain points:** List the contexts you repeatedly provide to agents (finances, tasks, communications, health, property). Document which platforms you use and how often you re-share context.\n\n2. **Evaluate MCP protocol support:** Check if your current AI tools (ChatGPT, Claude, Cursor) support MCP protocol. If not, identify which tools you'd need to switch to or wait for MCP adoption.\n\n3. **Start with provisional memory layer:** Use Parquet MCP or similar typed storage to begin normalizing your data. Extract 5-10 key data types from your current workflows (transactions, tasks, contacts, documents).\n\n4. **Design your three-layer architecture:** Map your current workflows to Truth Layer (what data), Strategy Layer (what decisions), Execution Layer (what actions). Identify which decisions and actions depend on which data.\n\n5. **Migrate one domain first:** Choose one domain (finances, tasks, or health) and migrate it to the memory substrate. Use it with agents for 2-4 weeks to validate the approach before expanding.\n\n6. **Set up entity resolution:** Create hash-based canonical IDs for your key entities (people, companies, properties, accounts). This enables cross-domain joins and unified entity model.\n\n7. **Build audit trail:** Ensure all data writes include provenance (source document, timestamp, agent/tool that created it). This enables full traceability from source documents to derived insights.\n\n## Open Loop\n\nHow do we handle collaborative memory when multiple users need shared context? Neotoma is personal-first, but teams and companies also need shared memory substrates. What does entity resolution look like when entities span multiple users' data? How do we maintain sovereignty while enabling collaboration?\n\nWhat happens when AI providers adopt MCP protocol but still optimize for lock-in through proprietary memory layers? Will they support user-owned memory, or will they create hybrid systems that default to provider control?\n\nHow do we scale entity resolution and graph relationships as personal data grows from 35,000 records to 350,000 or 3.5 million? At what point does the graph become too complex for real-time queries, and what are the architectural patterns for managing that complexity?\n\nThe transition to Neotoma was a return to the principles that initially drew me to Stacks: user-owned data, self-sovereign identity, direct relationships between individuals and applications. Two months in, the foundation is set. The work ahead will show what this mission looks like in practice—not just for me, but for others building sovereign agentic systems.\n",
    "showMetadata": 1,
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "event-sourced-memory-for-agentic-systems",
    "title": "Event-Sourced Memory for Agentic Systems",
    "excerpt": "AI agents have a memory problem. They remember recent conversations, but context fades quickly. Cross-platform memory does not exist. Event-sourced memory solves this by storing typed events that represent what actually happened.",
    "published": false,
    "publishedDate": "2026-02-05",
    "category": "technical",
    "readTime": 4,
    "tags": [
      "neotoma",
      "ai",
      "memory",
      "event-sourcing"
    ],
    "body": "AI agents have a memory problem. They remember recent conversations, but context fades quickly. Cross-platform memory doesn't exist. Agents in one tool can't access what agents in another tool learned.\n\nThis creates a fundamental limitation: you can't build reliable agentic workflows if agents forget context between sessions, platforms, and tools.\n\nEvent-sourced memory solves this. Instead of storing conversation transcripts, we store typed events that represent what actually happened. Agents can query this event log to reconstruct context, verify claims, and build on previous work.\n\n## The Problem with Current Memory Systems\n\nMost AI tools use conversation-based memory. They store chat history, but this approach has limitations:\n\n- Context fades after a few conversations\n- Memory is locked to a single platform\n- No way to verify what actually happened\n- No cross-tool memory sharing\n\n## Event-Sourced Architecture\n\nEvent-sourced memory stores typed events instead of conversations. Each event represents a discrete action or state change:\n\n- Financial transaction recorded\n- Task created or completed\n- Contact added or updated\n- Document processed\n\nAgents query these events to understand current state and historical context. They can verify claims by checking the event log. They can build on previous work by reading relevant events.\n\n## Benefits\n\n- **Persistent**: Events don't fade like conversation memory\n- **Cross-platform**: Any agent can query the same event log\n- **Verifiable**: Events provide provenance for claims\n- **Composable**: Agents can build on each other's work\n\nEvent-sourced memory enables reliable agentic workflows where agents remember context, verify information, and collaborate across tools.\n",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "deterministic-workflows-reproducibility",
    "title": "Deterministic Workflows: Reproducibility in Crypto Analytics",
    "excerpt": "Crypto analytics tools are unreliable. They give different answers at different times. Deterministic workflows use typed schemas and event-sourced data to ensure consistency.",
    "published": false,
    "publishedDate": null,
    "category": "technical",
    "readTime": 4,
    "tags": [
      "crypto",
      "determinism",
      "analytics",
      "reproducibility"
    ],
    "body": "Crypto analytics tools are unreliable. They give different answers at different times. They can't reproduce results. They make decisions based on incomplete or inconsistent data.\n\nThis is a problem when you're managing significant assets. You need systems that produce the same outputs for the same inputs, every time.\n\nDeterministic workflows solve this. They use typed schemas, event-sourced data, and reproducible queries to ensure consistency.\n\n## The Problem with Non-Deterministic Systems\n\nMost crypto tools are non-deterministic:\n\n- API responses vary\n- Calculations depend on timing\n- Results can't be reproduced\n- No way to verify correctness\n\nThis makes it hard to trust the outputs. You can't verify calculations. You can't reproduce analysis. You can't build reliable automation.\n\n## Deterministic Architecture\n\nDeterministic workflows use:\n\n- **Typed schemas**: Data structures are explicitly defined\n- **Event sourcing**: All changes are recorded as events\n- **Reproducible queries**: Same query, same data, same result\n- **Verification**: Results can be checked against source events\n\n## Benefits\n\n- **Reproducible**: Same inputs always produce same outputs\n- **Verifiable**: Results can be checked against source data\n- **Reliable**: No surprises, no inconsistencies\n- **Automated**: Systems can make decisions with confidence\n\nDeterministic workflows enable reliable crypto analytics where you can trust the outputs, verify calculations, and build automation that works consistently.\n",
    "showMetadata": 1,
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "building-ai-first-personal-operating-systems",
    "title": "Building AI-First Personal Operating Systems",
    "excerpt": "Personal productivity tools weren't designed for AI. AI-first personal operating systems are designed for agentic workflows from the ground up.",
    "published": false,
    "publishedDate": "2026-10-14",
    "category": "essay",
    "readTime": 5,
    "tags": [
      "ai",
      "productivity",
      "automation",
      "agentic-workflows"
    ],
    "body": "Personal productivity tools weren't designed for AI. They assume manual input, human decision-making, and linear workflows. They don't support agentic automation, context sharing, or cross-tool memory.\n\nThis creates friction. You spend time entering data, making decisions, and coordinating workflows that AI agents could handle. You lose time, mental bandwidth, and autonomy to tools that don't support your actual workflow.\n\nAI-first personal operating systems are designed for agentic workflows from the ground up. They support context sharing, cross-tool memory, and automated decision-making.\n\n## The Problem with Traditional Tools\n\nTraditional productivity tools assume:\n\n- Manual data entry\n- Human decision-making\n- Single-tool workflows\n- No context sharing\n\nThis doesn't work when you're using AI agents. Agents need context. They need to share memory across tools. They need to make decisions autonomously.\n\n## AI-First Architecture\n\nAI-first systems are designed for agents:\n\n- **Context sharing**: Agents can access shared context\n- **Cross-tool memory**: Memory works across all tools\n- **Automated workflows**: Agents can execute workflows autonomously\n- **Event sourcing**: All actions are recorded as events\n\nThis architecture enables agentic workflows where agents handle routine work, share context, and make decisions based on complete information.\n\n## Benefits\n\n- **Automation**: Agents handle routine work\n- **Context**: Agents have access to full context\n- **Efficiency**: Less manual work, more automation\n- **Capability**: Agents can handle complex workflows\n\nAI-first personal operating systems enable workflows where agents do the routine work, you do the strategic thinking, and the system handles the coordination.\n",
    "showMetadata": 1,
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "draft-example",
    "title": "This is a draft post that will only be visible in development mode.",
    "excerpt": "",
    "body": "This is a draft post that will only be visible in development mode.\n\nYou can use this to preview posts before publishing them.\n\n## How Drafts Work\n\n- Set `\"published\": false` in `posts.json`\n- Drafts are only visible when running `npm run dev`\n- In production builds, drafts are automatically hidden\n\n## Publishing a Draft\n\n1. Set `\"published\": true` in `posts.json`\n2. Add a `publishedDate` if not already set\n3. The post will appear in the posts list\n",
    "published": false,
    "publishedDate": null,
    "category": "essay",
    "readTime": null,
    "tags": [],
    "createdDate": "2026-01-14",
    "updatedDate": "2026-01-14",
    "summary": "",
    "shareTweet": ""
  },
  {
    "slug": "agent-command-centers-source-of-truth",
    "title": "Agent command centers need one source of truth",
    "excerpt": "Command centers for agents need a single, durable state layer for tasks and visibility. The UI is the dashboard; the layer it reads and writes is the substrate.",
    "body": "[Pawel Jozefiak recently wrote about running a personal AI agent](https://thoughts.jock.pl/p/wiz-1-5-ai-agent-dashboard-native-app-2026) and the tooling he built to manage it. He moved from Notion to Obsidian to a custom SQLite-backed board, then to a native SwiftUI app with focus mode and menu-bar visibility.\n\nThe critical bug he hit was tasks re-executing because completion wasn't reliably recorded. He ended up with a six-layer memory system (working memory, weekly rollover, permanent index, deep profiles, semantic search, self-improvement pipeline) and a clear conclusion. There's a gap between generic task tools and full agent IDEs. What's missing is a \"Command Center\" for agent lifecycle: claim, execute, review, iterate, with real-time visibility.\n\nI'm building [Neotoma](https://github.com/markmhendrickson/neotoma), a truth layer for agent memory. It doesn't build the dashboard or the agent. It provides the layer that a command center would use.\n\n## The gap he described\n\nJozefiak's options were either too generic (Trello, Linear) or too technical (terminal, JSON). Generic boards don't know about agent state. Who claimed what? Is the agent working or waiting for review? How is completion recorded so the agent doesn't run the same task twice? Raw logs and JSON don't give you a board at all.\n\nHe needed something in between: a single place where the agent and the human share task state, with clear semantics for claim, complete, and status, and fast enough that polling or real-time updates don't fall over. That \"single place\" is a data problem. The command center is the UI. The thing it reads from and writes to is the substrate.\n\n## What a truth layer provides\n\nNeotoma stores typed entities, observations, and relationships. It exposes them over MCP so any agent (Claude Code, Cursor, a scheduled runner) can store and correct state. Idempotency and deterministic IDs are built in.\n\nWhen the agent claims a task, it stores or corrects an entity with status \"in progress.\" When it completes, it corrects again with status \"done.\" Same idempotency key, same outcome every time. The bug Jozefiak hit (completion not recorded, task re-executed) is exactly what idempotent, durable writes are meant to prevent.\n\nA dashboard or native app that wants to show \"what the agent is doing\" would query the same store: list entities by type (e.g. task), filter by status, show assignee and timestamps. The agent and the dashboard share one source of truth. No custom SQLite, no sync layer that can drift. The dashboard is a view over Neotoma.\n\n## Where the six-layer memory fits\n\nJozefiak's six layers (working memory, weekly rollover, permanent index, deep profiles, semantic search, self-improvement) are strategy-layer and application-layer concerns. They decide what to keep, what to compress, what to summarize, and what to feed back into the agent's behavior.\n\nNeotoma doesn't do compaction or summarization. It's the durable, structured store those layers read from and write to. Working memory might be \"last N observations\" or \"entities touched in the last 48 hours.\" Weekly rollover might write new observations (summaries, indexes) back into Neotoma. Semantic search might run over the same entity graph. The boundary is clear: Neotoma is the truth layer; the layers above it implement retention policy and retrieval strategy.\n\n## Why this matters for builders\n\nIf you're building a personal agent and you need task state, status tracking, and visibility, you have two paths. You can roll your own storage (SQLite, files, a custom API) and then build a board or dashboard on top. You'll run into completion semantics, idempotency, and cross-session consistency yourself. Or you can use a substrate that already gives you entities, observations, provenance, and MCP access. The command center becomes a client of that substrate. The agent is another client. Both read and write the same state.\n\nI'm not building the command center. I'm building the layer it would sit on. Neotoma is the data plane for agent dashboards and lifecycle tooling. If that gap Jozefiak described gets filled by products (WizBoard-style or otherwise), those products will need a backend. A truth layer is that backend.\n\n## How this fits the trends I'm betting on\n\nIn [six agentic trends I wrote about recently](/posts/six-agentic-trends-betting-on), I argued that agents will become stateful economic actors, that errors will become economically visible, that tool fragmentation will persist, and that usage will be metered. The command center gap Jozefiak hit sits at the intersection of those pressures.\n\nWhen agents are stateful and long-running, you need to see what they're doing. Trend 1: \"Product interfaces exposing agent history as something inspectable rather than ephemeral\" is exactly what a command center is. When mistakes cost money or reputation, you need to know what the agent knew at the time. Trend 2: traceability and \"what did the agent know?\" make a single source of truth for tasks and status necessary, not nice to have.\n\nWhen you use multiple tools and models, state fragments. Trend 5: the command center and the agent both need to read and write the same state, which is why the substrate has to sit beneath the UI. When usage is priced, re-executing the same task because completion wasn't recorded is visible waste. Trend 6: idempotent, durable completion is an optimization as much as a correctness guarantee.\n\nI'm dogfooding Neotoma in my own agentic workflows and documenting the \"agent task lifecycle\" pattern: store task entities, use observations for status and history, update via MCP correct with idempotency keys so completion is unambiguous. That pattern is what would power a command center view (claim, execute, review, iterate) without each builder reinventing their own SQLite and sync logic. I'm also adding \"agent dashboard / command center backend\" to how I describe Neotoma so others looking to build that kind of tool know there's a substrate they can build on.\n",
    "published": true,
    "publishedDate": "2026-02-20",
    "category": "essay",
    "readTime": null,
    "tags": [],
    "createdDate": "2026-02-20",
    "updatedDate": "2026-02-20",
    "summary": "- Builders of personal AI agents need a single, durable state layer for tasks and visibility; generic boards and raw logs don't fit.\n- A \"command center\" (claim, execute, review, iterate) is the UI; the substrate it reads from and writes to is the data layer.\n- Neotoma provides typed entities, observations, and MCP access with idempotency and deterministic IDs so completion is unambiguous and tasks don't re-execute.\n- Multi-layer memory (working, weekly, semantic, self-improvement) sits above the truth layer; Neotoma is the durable store those layers consume and update.\n- Positioning Neotoma as the backend for agent dashboards and command centers gives builders a substrate instead of rolling their own SQLite and sync.",
    "shareTweet": "How a truth layer could power an agent command center: one durable state layer for tasks and visibility, so the dashboard and the agent share the same source of truth. https://github.com/markmhendrickson/neotoma",
    "heroImage": "agent-command-centers-source-of-truth-hero.png",
    "heroImageStyle": "keep-proportions",
    "heroImageSquare": "agent-command-centers-source-of-truth-hero-square.png"
  },
  {
    "slug": "six-agentic-trends-betting-on",
    "title": "Six agentic trends I'm betting on (and how I might be wrong)",
    "excerpt": "The structural pressures that underpin my work, and what would invalidate them as the AI industry evolves.",
    "body": "Everyone involved in AI right now is, implicitly or explicitly, trying to predict where things are going and how those changes will reshape our lives and work. The volume of speculation is enormous, and much of it is contradictory. That is unavoidable. No one can know with confidence what the next couple of years will bring. The space is moving too quickly, the interactions between technologies are too complex, and second-order effects dominate in ways that are difficult to model ahead of time.\n\nStill, if you are operating in this space, especially if you are building something with AI or for AI, it is not enough to remain agnostic. You have to choose a set of core theses about how the world is likely to evolve and build coherently around them, knowing that some will be wrong and others will matter more than expected. These theses are less about precise prediction and more about identifying structural pressures that seem unlikely to reverse.\n\nWhat follows are the central assumptions I am currently operating under. They are not claims about inevitability, and they are not meant to cover every possible future. They are the trends that, if they continue even partially, shape how I think AI systems will be used, where friction will accumulate, and what kinds of infrastructure will become necessary. My work ([Neotoma](/posts/truth-layer-agent-memory), a truth layer) is best understood as a response to these assumptions. It is not the reason for them, but it is built in anticipation of the world they imply.\n\n---\n\n## 1. Agents will become stateful economic actors\n\nOver the next two years, agents are likely to move beyond assistive, prompt-centric interactions and become meaningfully stateful actors. No breakthrough in general intelligence is required. Cheaper inference, more capable tool APIs, and broader tolerance for agents running unattended are enough.\n\nThe societal shift is real. We are used to tools that do nothing until we act. When agents persist goals, coordinate with each other, and take irreversible actions over time, the question of who is responsible becomes harder to answer. More work is delegated to non-human actors; [the boundary](/posts/we-are-all-centaurs-now) between \"I did this\" and \"my agent did this\" softens. Norms around trust, liability, and dependency will have to adapt. The technology enables the change; society has to decide how to live with it.\n\nWhy is this trend likely? The marginal cost of keeping agents alive is collapsing faster than the cost of rebuilding context. As inference gets cheaper and orchestration matures, it is more efficient to persist an agent's state than to reconstruct it from scratch. Tool APIs increasingly assume continuity: credentials, caches, intermediate artifacts. Persistence is rewarded over statelessness.\n\nIn that world, memory ceases to be a convenience feature. It becomes part of the system's state, comparable to a database rather than a chat log. When that state is proper and trustworthy, new things become possible at scale: long-horizon plans that span days or weeks, coordination across many agents and tools, and delegated work that is only feasible when state can be trusted and extended over time.\n\nNeotoma is built for that. It treats memory as explicit system state: typed entities, events, and relationships in a deterministic graph, not prompt residue or embedding similarity. An agent's history can be replayed, inspected, and reasoned about as part of the system itself.\n\nWhat to watch over the next year:\n1. Agent frameworks advertising long-running, background, or resumable execution as a core feature.\n2. Teams discussing agent state corruption or drift as a distinct bug class rather than restarting agents as a fix.\n3. Product interfaces exposing agent history as something inspectable rather than ephemeral.\n4. Teams running multiple agents that need a single source of truth for entities and decisions.\n\n---\n\n## 2. Agentic errors will become economically visible\n\nAs AI output increasingly flows directly into billing, compliance, client deliverables, and automated workflows, the cost of errors is likely to shift. What is currently a diffuse inconvenience becomes explicit economic impact.\n\nWhen errors start to show up in postmortems, contracts, and court filings, society gains a sharper picture of who bears the cost and who gets blamed. Organizations will face pressure to prove how decisions were made and what the system knew at the time. That pressure will ripple into professional norms, insurance, and regulation. Individuals and small teams may be held to standards that were originally designed for large institutions with audit trails. The upside is more accountability and fewer silent failures. The downside is that the bar for \"explainable\" and \"auditable\" may rise faster than many are ready for.\n\nThe structural reason this trend is likely is that AI is moving closer to decision-making edges, not just advisory layers. As AI output becomes embedded downstream in systems that trigger payments, commitments, or external communication, errors inherit the cost structure of those systems. Organizations cannot continue treating failures as \"model quirks\" once they propagate into irreversible actions.\n\nToday, mistakes are often shrugged off with regeneration or prompt tweaks. Tomorrow, those same mistakes will waste money, damage reputation, or create legal exposure.\n\nWhen errors become priced, organizations stop asking whether outputs were helpful. They start asking how those outputs were produced, what information they relied on, and whether the process can be replayed or audited.\n\nAs a corollary, tolerance for approximate or ambiguous memory erodes. The bar for what counts as good enough rises first where harm is visible, then that standard drifts outward. Once mistakes are costly, memory that you can correct and trace becomes infrastructure, not a convenience.\n\nNeotoma aligns with this shift by enforcing provenance at the memory layer. Facts are stored with source attribution, timestamps, and ingestion events. Corrections are additive rather than destructive, allowing teams to reconstruct exactly what an agent knew at the time of a decision instead of guessing based on partial logs.\n\nWhat to watch over the next year:\n1. AI-related failures appearing in postmortems, client disputes, or legal contexts.\n2. Teams explicitly asking \"what did the agent know at the time?\" after mistakes.\n3. Traceability or audit requirements being added to AI workflows retroactively.\n4. Public incidents attributed to AI memory errors; language shifting from \"hallucination\" to \"system failure\" in postmortems.\n5. Teams asking for \"undo this fact\" or \"revert what the agent believes\" without full resets.\n6. \"What does the system believe and how has it evolved?\" framed as a query over a consistent graph rather than a RAG call.\n\n---\n\n## 3. Audit and compliance will drift down-market\n\nA related trend: the pressure to prove how work was produced and what the system knew will not stay confined to large enterprises. Wherever errors carry a real cost—economic, legal, or reputational—the demand for defensibility and record-keeping follows. As AI becomes embedded in professional work, consultants, agencies, regulated freelancers, and small AI-native teams will face the same expectations.\n\nThe structural reason is liability diffusion. As AI use becomes normalized, responsibility does not disappear. It spreads. Clients, insurers, and regulators respond by seeking compensating controls. Audit pressure moves down-market not because small teams want it, but because risk follows usage.\n\nOnce questions about how work was produced become routine, memory without provenance becomes a liability rather than a convenience. Structured timelines, entity-level recall, and source attribution start to function as defensive infrastructure.\n\nNeotoma aligns with this shift by treating memory as something that can be reconstructed in time rather than inferred retrospectively. Entity resolution, temporal ordering, and provenance are not add-ons. They are core to the model.\n\nWhat to watch over the next year:\n1. AI usage disclosures appearing in contracts, statements of work, or professional guidelines.\n2. Requests for documentation of AI-assisted decisions from clients or insurers.\n3. Individuals or small teams proactively storing AI interaction records defensively.\n4. Regulation that explicitly requires record-keeping or explainability for certain AI uses.\n\n---\n\n## 4. Platform memory will remain opaque\n\nLarge AI platforms are likely to continue shipping memory features that are useful but fundamentally opaque. Their incentives favor engagement, retention, and model optimization rather than user-controlled provenance or guarantees of correctness.\n\nThe societal effect is a split between those who can afford to care and those who cannot. People and organizations that need strong guarantees (audit, correctness, portability) will either pay for alternatives, build their own, or accept risk. Everyone else will rely on platform memory and live with the trust gap. That divide can reinforce existing inequalities. The well-resourced get transparent, portable memory; everyone else gets convenience with opaque terms. Over time, norms about what \"my data\" and \"my history\" mean may diverge by context and by who you are. Civic and professional expectations (e.g. that you can show your work or export your records) may apply only to some.\n\nThe structural reason this persists is incentive misalignment. Platforms optimize for aggregate outcomes across millions of users, not for the correctness guarantees required by any individual workflow. Exposing memory semantics, correction rules, or replay guarantees constrains iteration speed and increases liability. Opaqueness is not accidental. It is protective.\n\nMemory may improve, but it will remain difficult to inspect, export, replay, or reason about formally, especially across tools. Corrections will often be silent, implicit, or model-specific.\n\nThis creates a growing trust gap. Users may rely on platform memory for convenience while simultaneously distrusting it in contexts where consequences matter.\n\nData sovereignty adds a separate pressure: enterprises and individuals are increasingly insisting that agent memory stay in their environment, either on-prem, in their tenant, or under their control, rather than in a vendor's cloud.\n\nNeotoma is built for that gap. Its local, inspectable, user-controlled design is the alternative for workflows where correctness and provenance matter. You own the data and the semantics; you can export, correct, and reason about what the system knows.\n\nWhat to watch over the next year:\n1. Memory features that improve recall but stay undocumented or non-exportable.\n2. Users asking what the system actually knows – such as a comprehensive view of what it believes, remembers, and has inferred, not just raw chat or exports – and getting no clear answer.\n3. Workarounds (e.g. exports, third-party sync, manual replication) growing rather than shrinking.\n4. RFPs or requirements specifying that agent memory must stay on-prem or in the user's tenant.\n\n---\n\n## 5. Tool fragmentation will persist\n\nDespite recurring narratives about consolidation into a single AI platform or workspace, knowledge work is likely to remain fragmented. Professionals already operate across multiple models, editors, copilots, document systems, and agent frameworks.\n\nThe structural reason is that AI tools are complements, not substitutes. Each optimizes for a different part of the workflow: ideation, execution, coding, retrieval, communication. Marginal improvements do not collapse the stack. Low switching costs and rapid model iteration further discourage consolidation.\n\nAs tool sprawl increases, the core problem shifts from interface fragmentation to state fragmentation. Context lives in too many places at once, and no single surface can realistically own it.\n\nNeotoma sits beneath this fragmentation rather than trying to resolve it. By exposing memory through a protocol interface rather than a single UI, it allows multiple tools and agents to read from and write to the same underlying state without forcing convergence on a single workflow or vendor.\n\nWhat to watch over the next year:\n1. Professionals switching models or tools mid-task without migrating context cleanly.\n2. Repeated complaints about \"losing context\" between tools.\n3. Teams standardizing workflows that explicitly span multiple AI products.\n\n---\n\n## 6. Agentic usage will become metered\n\nAgent execution is also likely to become increasingly constrained by cost. The structural reason is straightforward: compute is becoming a visible line item. No radical economic restructuring is required.\n\nAs AI spend grows, organizations introduce budgeting, attribution, and optimization. Once costs are visible, metering follows naturally.\n\nWhen usage is priced, inefficiency and drift stop being abstract concerns. Recomputing context, misremembering prior decisions, or repeating work becomes visible waste.\n\nNeotoma's deterministic memory model becomes relevant here because it separates durable memory from transient context. By enabling replay instead of regeneration, it treats memory as an optimization surface rather than a side effect of inference.\n\nWhat to watch over the next year:\n1. Teams tracking agent or model usage costs per task or workflow.\n2. Budget-aware agents that alter behavior based on remaining spend.\n3. Optimization efforts focused on reducing redundant inference rather than improving prompts.\n\n---\n\n## How these trends impact key demographics\n\nThese trends act as activation conditions for distinct impacted demographics. Neotoma does not become important through persuasion. It becomes important when reality removes alternatives.\n\n**AI-native individual operators and high-context knowledge workers** are the first: founders, consultants, researchers, and solo builders using AI deeply across thinking and execution. Adoption is gated by stateful agents, economically visible errors, and dissatisfaction with opaque platform memory. Once outputs matter externally (to clients, collaborators, or revenue), the inability to answer \"what did the system know when this was produced?\" becomes untenable. Neotoma becomes attractive as a personal system of record that can coexist with multiple tools.\n\n**AI-native small teams and hybrid product or operations teams** are the second. Individuals can compensate for fuzzy memory. Teams cannot. Once each person's agents remember slightly different facts or assumptions, coordination costs compound. Tool fragmentation accelerates this, audit pressure legitimizes shared memory, and metered usage converts drift into budget waste. In this environment, Neotoma functions less as a productivity layer and more as shared cognitive infrastructure.\n\n**Developer integrators and AI tool builders** who embed agents into products or platforms are the third. For them, memory failure is a production failure. As agents become autonomous, opaque recall becomes untestable and unacceptable. When memory errors are reframed as system failures rather than quirks, builders begin looking for memory primitives that behave like databases rather than conversations. Neotoma becomes relevant here as a substrate, not a feature.\n\nAcross all these demographics, adoption is conditional and stepwise, not hype-driven.\n\n---\n\n## What would falsify this view\n\nAny serious vision of the future should be falsifiable. Without clear signals that would prove it wrong, it is not a thesis but a belief. This matters directly for product strategy, because building toward a future that does not materialize leads to elegant irrelevance rather than adoption.\n\nThe most significant falsifier would be large AI platforms delivering memory that is genuinely portable, inspectable, replayable, and trusted across tools. Not memory in a marketing sense, but memory that is user-owned, exportable, semantically explicit, and stable across contexts. If platform-native memory becomes authoritative in practice (meaning users and organizations trust it as the canonical record of what was known and when), the need for an external truth layer collapses. In that world, Neotoma's core differentiation erodes rather than compounds.\n\nA second falsifier would be meaningful consolidation into a single dominant AI workspace that owns execution, memory, and tooling end-to-end. If fragmentation pressure disappears because one surface successfully absorbs the stack, the leverage of shared memory substrates declines sharply.\n\nA third falsifier would be agents remaining short-lived, tightly supervised, and cheap to reset, with failures continuing to be handled primarily by restarting rather than diagnosing state. If long-running agents do not materialize and resetting remains the dominant recovery strategy, deterministic memory remains optional rather than necessary.\n\nFinally, if audit and liability pressure fail to move down-market (if AI remains advisory rather than consequential for most professionals), then provenance-heavy memory remains overkill for longer than anticipated.\n\nWatching for these counter-signals is as important as watching for confirmation. They provide early warning that the assumptions driving adoption are weakening and that strategy should adapt accordingly. A vision that cannot be falsified cannot be corrected, and a product built on such a vision risks becoming well-designed for a world that never arrives.\n\n---\n\n## Memory as critical, open infrastructure\n\nThis is not a prediction that the world becomes more philosophically committed to truth or correctness.\n\nIt is a prediction that agents become stateful, errors become expensive, platforms remain opaque, tools remain fragmented, audit pressure spreads, and usage becomes priced.\n\nIf even part of this trajectory holds, memory stops being a UX feature and becomes infrastructure that is necessarily open. In that world, systems that treat memory as deterministic, inspectable state are no longer visionary. They are simply the cheapest way to keep complex systems from failing in opaque and irrecoverable ways.\n\nNeotoma is not the driver of that change. It is one plausible response to it.\n",
    "published": true,
    "publishedDate": "2026-02-18",
    "category": "essay",
    "readTime": null,
    "tags": [],
    "createdDate": "2026-02-18",
    "updatedDate": "2026-02-18",
    "summary": "- Agents will become stateful economic actors and memory will become system state, enabling long-horizon plans and coordination at scale.\n- Agentic errors will become economically visible and tolerance for approximate memory will erode as the bar for defensibility and audit rises.\n- Audit and compliance will drift down-market and the pressure to prove how work was produced will reach consultants, agencies, and small teams.\n- Platform memory will remain opaque and a trust gap will grow between those who need guarantees and those who rely on platform convenience.\n- Tool fragmentation will persist and state fragmentation will matter more than interface fragmentation, with memory as a protocol beneath it.\n- Agentic usage will become metered and deterministic memory will enable replay over regeneration, turning memory into an optimization surface.",
    "shareTweet": "Six trends I'm betting on for the agentic future. Key takeaways in the post—and what would prove me wrong. https://markmhendrickson.com/posts/six-agentic-trends-betting-on",
    "heroImage": "six-agentic-trends-betting-on-hero.png",
    "heroImageStyle": "keep-proportions",
    "alternativeSlugs": [
      "agentic-future-betting-on"
    ]
  },
  {
    "slug": "we-are-all-centaurs-now",
    "title": "We're all centaurs now",
    "excerpt": "The humanness isn't in typing every word. It's in deciding what's worth saying.",
    "published": true,
    "publishedDate": "2026-02-14",
    "category": "essay",
    "readTime": 6,
    "tags": [
      "ai",
      "writing",
      "centaurs",
      "build-in-public"
    ],
    "body": "Last spring, I committed what felt like a professional transgression. As a general manager at a crypto startup, I used Cursor to prototype a token detail screen—something that had been sitting in our backlog for months. Within an hour, I had a working demo. The UI was janky, it didn't conform to our design system, but it *existed*. And that existence felt significant.\n\nI sensed skepticism from my team. The feedback I perceived, spoken and unspoken, was that I'd broken process. Skipped important steps. The prototype showed something, sure, but it didn't represent proper team-wide thinking. It felt like they saw it as a curiosity, not a contribution.\n\nI kept going anyway. I built a small project to generate content and documentation about tokens and asset classes, then integrated it directly into our web app as tooltips and links. This time it wasn't just a prototype—it was production code that real users would interact with. And this time, the resistance felt more pointed.\n\nI perceived that people thought I had no place pushing code, let alone AI-generated code. I was using a \"black box\" to do work that shouldn't be delegated to machines, at least not by a non-\"engineer\". The word that kept surfacing in my mind was *irresponsible*. I felt like I was using a cheat code, and worse, like I might not even know enough to understand why it was wrong.\n\nHere's the thing: I was the GM. I had the authority to push that work through. But I couldn't shake the feeling that I might not have been able to do it without that authority. And I spent months questioning whether I'd done the right thing.\n\n## The Vindication\n\nThat was April and May of 2025. This is February 2026.\n\nIn the intervening months, something shifted. AI-driven coding went from suspicious novelty to industry standard. The discourse moved from \"Is this as good as humans?\" to \"How do we manage systems with superhuman capabilities?\" The tooling improved, the models advanced, but mostly, people just… tried it. And realized it worked.\n\nMy intuition was entirely vindicated. What I'd discovered wasn't a shortcut—it was a different mode of operation. The low-level details I'd been criticized for not writing myself turned out to be exactly the kind of work that *should* be delegated. Because delegating them freed me to work at a higher level of abstraction, to think more strategically, more creatively.\n\nIt's not that different from managing a team. When you lead people, you don't write every line of code yourself. That doesn't make you less creative—it makes you *more* creative, because you're spending your cognitive resources on questions of design, strategy, direction, and most importantly, philosophy.\n\n## History Repeating\n\nNow I'm working on a new startup. I'm building a product, developing a platform, and cultivating a public voice again. And I'm using AI to write blog posts, to express myself, to publish actively.\n\nLast week, a friend shared feedback on one of my posts. Something in it made him feel like it was AI-generated. He described his reaction as a \"brain itch\"—that moment of recognition that pulls you out of the content. He sent me [a link](https://www.0xsid.com/blog/aidr) arguing that all writing should be \"organic\"—handwritten, unprocessed, preserving what a person actually thinks.\n\nAnd immediately, I felt it again. That same self-doubt. That same shame. Maybe I *am* short-circuiting something essential. Maybe the creative element is lost when I'm not the one writing every sentence. Maybe I'm using another cheat code.\n\nBut then I stopped and thought about how I actually write these days.\n\n## The Real Process\n\nMy writing doesn't start with fully formed ideas waiting to be transcribed. It starts with the contours of interests and questions. When something provokes my curiosity, I open a conversation with an AI agent. I ask it to help me analyze the concept. I load an article and ask for a summary, then Q&A it, bouncing between the source material and the conversation. I ask for corrections, for synthesis, for reports.\n\nThis is a learning process. A powerful, leveraged learning process. And that report or analysis—that's essentially a blog post to myself. The jump from there to public expression is smaller than you'd think. I just need to transform it so someone without my initial context can access both the topic *and* my developed viewpoint.\n\nSo I work with the agent to convert the analysis into a draft. I iterate on phrasing, positioning, structure. I ask for candidates and choose between them. I provide style guidelines and refine them over time. The exact wording often isn't what I first came up with. But the ideas are mine. The judgment is mine. The direction is mine.\n\nAnd crucially: I'm writing *because* I can do this quickly. I'm running a one-person startup. The difference between five hours and one hour on a blog post is four hours I can spend building product. Without AI assistance, I wouldn't be blogging at all—or I'd be blogging far less.\n\nIt's the same trade-off as last year: existence versus non-existence. Something good enough that gets out there versus something perfect that never happens.\n\n## The Pattern\n\nI think we're going through for writing what we went through for coding last year. The same cultural moment. The same questions about authenticity and responsibility. The same anxiety about what makes something \"human.\"\n\nAnd I suspect this pattern will repeat as AI penetrates more domains. Each time, we'll question whether we're losing something essential. Each time, we'll discover that what we thought was essential—the low-level execution—was actually just what was *possible* for us to do. And that when we delegate it, we free ourselves to work at the level where human creativity actually lives: meaning, values, judgment, direction.\n\nThe humanness isn't in typing every word. It's in deciding what's worth saying.\n\n## The Embrace\n\nThis doesn't mean anything goes. I'm not arguing for putting out work you don't approve or oversee correctly. But there's a lot of subjectivity in what \"correctly\" means. And especially in a startup mentality, the risk of publishing something AI-assisted and imperfect is usually lower than we think. The risk is that you harm your reputation. But the upside is that you're iterating toward quality and authenticity faster than if you'd waited for perfection.\n\nEvery piece you create with AI gets you closer to understanding how to channel yourself through the technology more effectively. The words might not all be yours, but the voice can be. And increasingly will be, as you develop confidence in directing these tools.\n\nWe need to embrace the cyborg nature of this moment. Not retreat from it. Not treat it with wariness. But develop real confidence in our ability to guide these systems as extensions of ourselves.\n\nWe're all [centaurs](https://youtu.be/N5JDzS9MQYI?si=4ZARzcn5aPqnDeZH) now. Half human, half AI. The question isn't whether to accept that—the integration is already happening. The question is whether we'll do it proactively, with intention, channeling our values and judgment through these tools. Or whether we'll do it reluctantly, apologetically, always wondering if we're cheating.\n\nI spent months last year questioning my intuition. I'm not doing that this time. The work I'm putting out is work that reflects my thinking, serves my goals, and wouldn't exist without this partnership. That's enough.\n\nThe future isn't about preserving some notion of pure, unassisted human creativity. It's about becoming fluent in a new mode of creative expression—one where the human contribution is strategic direction rather than tactical execution.\n\nAnd that, it turns out, is exactly where human creativity has always lived anyway.\n",
    "heroImage": "we-are-all-centaurs-now-hero.png",
    "heroImageStyle": "keep-proportions",
    "heroImageSquare": "we-are-all-centaurs-now-hero-square.png",
    "createdDate": "2026-02-14",
    "updatedDate": "2026-02-14",
    "summary": "- Using AI to prototype and ship code as a non-\"engineer\" GM felt like a transgression in 2025, but that mode of work is now industry standard.\n- Delegating low-level execution to AI frees you to work at a higher level: design, strategy, direction, and philosophy.\n- The same cultural anxiety about authenticity is playing out for AI-assisted writing as it did for coding.\n- The human contribution is meaning, values, judgment, and direction, not typing every word.\n- Embracing the centaur model (half human, half AI) with intention beats doing it reluctantly or apologetically.",
    "shareTweet": "We're all centaurs now. The humanness isn't in typing every word. It's in deciding what's worth saying.\n\nhttps://markmhendrickson.com/posts/we-are-all-centaurs-now",
    "ogImage": "og/we-are-all-centaurs-now-1200x630.jpg"
  },
  {
    "slug": "agentic-wallets-mcp-bitcoin",
    "title": "A Bitcoin wallet MCP server for L1 and L2",
    "excerpt": "The future of crypto wallets is agentic. Wallets that expose execution surfaces let agents monitor, reason, and execute within policy while you keep sovereignty and approve only what exceeds your limits.",
    "published": true,
    "publishedDate": "2026-02-09",
    "category": "technical",
    "readTime": 6,
    "tags": [
      "agentic-wallets",
      "mcp",
      "bitcoin",
      "neotoma",
      "truth-layer"
    ],
    "body": "This weekend I pulled together an MCP server for a Bitcoin wallet: tools that AI agents can call over the Model Context Protocol. The [repo](https://github.com/markmhendrickson/mcp-server-bitcoin) exposes 93 tools across Layer 1 and Layer 2. One mnemonic drives both.\n\nI was previously general manager of [Leather](https://leather.io), a crypto wallet that also supports Bitcoin and Stacks. At Leather I saw that human-facing self-custody wallets mostly reached people willing to absorb the attention and complexity (e.g. degens and developers). That meant key hygiene, fee awareness, confirmation flows, and the rest. The cognitive load kept the real addressable market narrow.\n\nAgentic wallets change that. When the primary interface is agents that reason and execute within policy, the user approves only what matters. The friction drops and the set of people who can practically hold their own keys grows.\n\nSame two chains. Different surface.\n\n## What the server exposes (L1 and L2 in one surface)\n\nThe server is a single MCP process. Clients send tool names and JSON arguments over stdio and get back structured results. Destructive actions (sends, sign-and-broadcast, deploy) support `dry_run` and do not broadcast by default. The server never returns keys or mnemonics.\n\n### Layer 1 (Bitcoin)\n\n**Core Bitcoin:**\n\n- Address derivation for P2PKH, P2SH-P2WPKH, P2WPKH, and P2TR with public keys and paths.\n- Accounts with balances per address type ([mempool.space](https://mempool.space) for UTXO data); wallet balance and BTC prices (USD, EUR).\n- Single and multi-recipient sends (amount in BTC or EUR); preview transfer with fee estimate before sending.\n- Sweep (send max) and UTXO consolidation.\n- PSBT sign, decode, and batch sign; message sign and verify (ECDSA legacy and BIP-322).\n- Fee tiers from mempool.space and fee estimation by input/output count and address type.\n- UTXO listing with filters (address type, min value, confirmed only) and per-UTXO details.\n\n**Ordinals and inscriptions:**\n\n- List inscriptions with pagination; inscription details (genesis, content type, sat ordinal, rarity, location).\n- Send inscriptions (full UTXO or split so only the inscription's sat range goes to the recipient).\n- Extract ordinals from mixed UTXOs; recover BTC from the ordinals address (sweep non-inscription UTXOs); recover ordinals that landed on the payment address back to the taproot address.\n- Create single or batch inscriptions with commit/reveal fee estimates.\n\n**Transaction and wallet management:**\n\n- Transaction history for BTC and Stacks; status for a single tx.\n- Speed up pending BTC via RBF; cancel pending BTC (RBF send-to-self).\n- Network config and API endpoints; switch mainnet/testnet; add custom network.\n- List all supported tool names and descriptions.\n\n**Ledger (Bitcoin app):**\n\n- Get BTC addresses from a connected [Ledger](https://www.ledger.com) device.\n- Sign PSBT with the Ledger Bitcoin app.\n\n### Layer 2 (Stacks)\n\nThe same mnemonic derives Stacks keys (path `m/44'/5757'/0'/0/0`). [Hiro](https://hiro.so) Stacks API for chain data and broadcasting.\n\n**Stacks:**\n\n- Addresses and public keys; accounts with STX balance, locked amounts, nonces.\n- Balance including fungible and non-fungible tokens.\n- STX transfer (micro-STX) with optional memo; preview transfer with fee and balance check.\n- SIP-10 fungible and SIP-9 NFT transfers via contract calls.\n- Clarity: call public function, deploy contract, read-only call.\n- Sign serialized Stacks tx (SIP-30), sign message, sign SIP-018 structured data; nonce and fee estimation.\n- On-chain profile update ([schema.org/Person](https://schema.org/Person)) for BNS names.\n- Transaction queries with filters (type, block range, unanchored) and by contract.\n- Mempool: list pending transactions, mempool stats, dropped transactions.\n- Block explorer: recent blocks, block by height or hash, Stacks blocks for a given Bitcoin block.\n- Contract events: events for a contract, or asset events for an address.\n- Token metadata: SIP-10 and SIP-9 metadata and holders.\n- Network info and health/status.\n\n**Swaps, DeFi, and bridge:**\n\n- Supported pairs and protocols ([ALEX](https://alexlab.co/), [Bitflow](https://www.bitflow.finance), [Velar](https://www.velar.co)).\n- Swap quote (estimated output, rate, fees) for all three; execute swap via ALEX DEX. Bitflow and Velar support quotes and pair discovery; you could add execution via protocol SDKs (e.g. Velar SDK returns contract-call params).\n- Swap history from on-chain activity.\n- sBTC balance and bridge deposit/withdraw info.\n- Stacking: current PoX status, cycle info (blocks remaining, percent complete, estimated time remaining, participation rate), initiate solo stacking, revoke delegation.\n\n**BNS and market data:**\n\n- [BNS](https://docs.stacks.co/docs/stacks-blockchain/bns) lookup (name to address), names owned by address, register BNS name.\n- Multi-asset prices (e.g. [CoinGecko](https://www.coingecko.com)); price history for charting.\n- Portfolio summary (BTC + STX in USD); all assets and collectibles (inscriptions, Stacks NFTs).\n\n**Ledger (Stacks app):**\n\n- Get Stacks addresses from Ledger.\n- Sign Stacks transaction with Ledger Stacks app.\n\n## Safety and design\n\n⚠️ This MCP server is experimental and not safe for meaningful funds. Use only with wallets you are prepared to lose. No one has battle-tested or audited the code. I treat it as a research artifact to explore agent-native wallet surfaces.\n\nDestructive operations default to `dry_run: true`. Preview and estimate tools exist for every send path. Keys stay out of version control and out of tool responses. The run script loads `.env` from repo root.\n\n**Wallet key variables (keep secret, never commit):**\n\n- **`BTC_PRIVATE_KEY`** — WIF-encoded Bitcoin private key; if set, takes precedence over mnemonic.\n- **`BTC_MNEMONIC`** — BIP-39 seed phrase; the server uses it to derive Bitcoin and Stacks keys (same mnemonic, path `m/44'/5757'/0'/0/0` for Stacks).\n- **`BTC_MNEMONIC_PASSPHRASE`** — Optional BIP-39 passphrase to use with `BTC_MNEMONIC`.\n\n**Safety and limits (env or .env):**\n\n- **`BTC_NETWORK`** — `mainnet` or `testnet` (default `testnet`).\n- **`BTC_MAINNET_ENABLED`** — Set this to allow mainnet sends (safety flag).\n- **`BTC_DRY_RUN`** — When set (default), destructive ops (sends, sign-and-broadcast, deploy) do not broadcast; set it to `false` to allow real transactions.\n- **`BTC_MAX_SEND_BTC`** — Optional cap on send amount in BTC; the server rejects requests above this.\n- **`BTC_MAX_FEE_SATS`** — Optional cap on fee in satoshis per transaction.\n- **`STX_ACCOUNT_INDEX`** — Stacks derivation account index (default `0`).\n- Config otherwise drives the fee tier (fixed rate or mempool.space tier: hour, half-hour, fastest).\n\n## How it fits my agent stack\n\nI run agents on a three-layer architecture. The layers are cleanly separated so that memory, reasoning, and action stay in the right place.\n\n**Truth layer:** This is the memory substrate. It holds typed, structured data: holdings, flows, transactions, contacts, tasks, and the rest. In my setup the canonical store is [Neotoma](/posts/truth-layer-agent-memory). It uses event sourcing and reducers, with full provenance and entity resolution. Agents read from it. They never write truth directly. All updates flow through domain events produced by the execution layer.\n\n**Strategy layer:** This is where goals, constraints, and tactics live. Strategy documents, tactical playbooks, and operations manuals sit here. Agents use this layer to reason: they read world state, evaluate priorities and risk, and produce decisions and commands. Strategy is pure cognition. No side effects. State in, decisions out.\n\n**Execution layer:** This is where external actions happen. It takes commands from the strategy layer and performs side effects through adapters: email, calendar, DNS, and in this case the Bitcoin and Stacks wallet MCP. The wallet server is one execution adapter among many. It never mutates the truth layer. It does the thing (send, sign, swap) and the rest of the stack records what happened via domain events. Commands in, events out.\n\nI define and maintain the strategy. Agents read from the truth layer and call MCP tools to execute. I do not use point-and-click crypto UIs for routine operations. I only step in to approve actions that exceed my pre-set limits.\n\nShort-term my use cases are one-off: paying for services, rebalancing portfolios through manual prompting. Longer-term I want those flows automated. Agents would monitor, reason, and execute within policy. I would see explanations and approve when needed.\n\n## How I'm approaching the build\n\nI'm dogfooding the server in my own workflows first. I'm testing each surface (sends, PSBTs, Ordinals, Stacks transfers, swaps) gradually with small amounts and dry runs.\n\nI've wired it into the same stack where I already use [truth and strategy layers](/posts/agentic-search-and-the-truth-layer#where-ive-hit-limits). Agents can combine wallet actions with calendar, email, and data. External users aren't in scope yet.\n\nMy goal is to validate the shape of an agentic wallet surface and to make my own Bitcoin and Stacks operations agent-driven instead of manual.\n\nTo run it: clone [mcp-server-bitcoin](https://github.com/markmhendrickson/mcp-server-bitcoin) (or add as submodule at `mcp/btc_wallet/`), add the server to your MCP config (use the `run_btc_wallet_mcp.sh` script path), and use a test wallet with dry run on.\n",
    "heroImage": "agentic-wallets-mcp-bitcoin-hero.png",
    "heroImageStyle": "keep-proportions",
    "heroImageSquare": "agentic-wallets-mcp-bitcoin-hero-square.png",
    "createdDate": "2026-02-07",
    "updatedDate": "2026-02-09",
    "summary": "- I built an experimental MCP server with 93 tools for Bitcoin L1 and Stacks L2, one mnemonic for both.\n- Same two chains as human wallets, but agent-callable: less friction for people who want to hold their own keys without the full cognitive load.\n- L1 covers core Bitcoin, Ordinals and inscriptions, tx management, and Ledger; L2 covers STX, Clarity, swaps, sBTC bridge, stacking, BNS, market data, mempool monitoring, block explorer, contract events, token metadata, network stats, and enriched stacking cycle progress.\n- Destructive actions default to dry run; no keys or mnemonics are ever returned; preview and env limits keep execution policy-gated.\n- The wallet MCP is an execution adapter in my three-layer stack; I define strategy, agents execute, I approve when actions exceed my limits.",
    "shareTweet": "This weekend I pulled together an MCP server (i.e. tools for AI agents) for a Bitcoin wallet. It supports extensive L1 functionality: PSBTs, UTXOs, Ordinals, Ledger. It also supports L2 with Stacks: sBTC bridging, Clarity, Stacking, swaps.",
    "ogImage": "og/agentic-wallets-mcp-bitcoin-1200x630.jpg"
  },
  {
    "slug": "why-agent-memory-needs-more-than-rag",
    "title": "Why agent memory needs more than RAG",
    "excerpt": "Retrieval for agent memory should be driven by structure, not similarity. Learned hierarchies beat RAG but are brittle. Schema-first design gives the same advantage without putting the LLM in the critical path.",
    "published": true,
    "publishedDate": "2026-02-06",
    "category": "Technical",
    "readTime": 5,
    "tags": [
      "neotoma",
      "agent-memory",
      "architecture",
      "rag",
      "build-in-public"
    ],
    "body": "[RAG (retrieval-augmented generation)](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) augments an LLM by retrieving relevant passages from an external corpus, often via embeddings and similarity search, then feeding them as context so the model can answer from up-to-date or domain-specific data.\n\nIt works well for document search. For agent memory, it falls apart.\n\nA new paper, \"Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation\" (Hu et al., Feb 2026; [see paper](https://arxiv.org/abs/2602.02007)), from King's College London and the Alan Turing Institute, explains why and points to a better approach.\n\n## Why RAG falls short for agent memory\n\nStandard RAG assumes a large, mixed corpus: embed text, retrieve [top-k](https://en.wikipedia.org/wiki/Nearest_neighbor_search) by similarity, concatenate as context.\n\nAgent memory is the opposite: a bounded, coherent stream where the same fact appears in many phrasings. Applying RAG here creates three problems:\n\n1. **Redundant top-k.** You ask \"When did I last see the dentist?\" In a document corpus, top-k might return a few relevant paragraphs from different sources. In agent memory, many chunks say almost the same thing (\"Scheduled dentist March 15,\" \"Dentist appointment March 15,\" \"Booked dentist for March 15\"). Top-k fills with repetition. The paper calls this \"collapse into a single dense region.\" Similarity fails to separate what is *needed* from what is merely *similar*.\n2. **Pruning breaks evidence chains.** You ask \"Did we resolve the invoice dispute?\" The answer depends on a chain: \"Invoice #123 was disputed,\" then \"We agreed to a partial refund,\" then \"Paid the agreed amount.\" Post-hoc pruning might keep \"Paid invoice #123\" and drop the earlier turns. The model then answers \"Yes, resolved\" without knowing there was a dispute. Pruning fragments temporally linked evidence and produces wrong answers.\n3. **Similarity ignores structure.** You ask \"What's the status of the Barcelona trip?\" You need the project, the task (e.g. book flights), and the outcome. Similarity returns chunks that mention \"Barcelona\" or \"trip\": maybe a random mention, a past trip, a task from a different project. You needed a structural path (this project, these tasks, these outcomes). Similarity doesn't encode that. Structure does.\n\n## Structure over similarity\n\nA better approach is to use structure to drive what gets loaded, not similarity. Type entities (tasks, contacts, transactions, events) and retrieve by schema, entity IDs, relationships, and timelines. Keep observations and derived outputs as whole units; don't prune inside evidence blocks. Same input and same schema yield same output. No LLM in the critical path.\n\n## What the paper shows\n\nThe paper's system (xMemory) builds a four-level hierarchy (messages to episodes to semantics to themes) with embeddings and LLM summaries. It beats five other systems (Naive RAG[^1], A-Mem[^2], MemoryOS[^3], LightMem[^4], Nemori[^5]) on LoCoMo and PerLTQA, the benchmark datasets for long-conversation memory and personal long-term question answering. The paper doesn't require embeddings or LLMs; it requires structure. You can get there with a learned hierarchy (xMemory) or with deterministic, schema-first design. The paper also documents fragility in LLM-generated structure (A-Mem, MemoryOS): formatting deviations, failed updates. Deterministic, schema-first structure is a more reliable base.\n\n## xMemory vs Neotoma\n\nNeotoma is the [structured memory layer](/posts/truth-layer-agent-memory) I'm building: schema-first, deterministic, built for provenance and replay. Both systems move beyond RAG; they differ in how they build structure.\n\n**xMemory** builds a four-level hierarchy (messages to episodes to semantics to themes) with embeddings and LLM summaries. Episodes are contiguous blocks; semantics are reusable facts; themes group semantics for high-level access. A sparsity-semantics objective balances theme size. Too large causes redundant retrieval; too small fragments evidence. Retrieval is top-down: select a compact set of themes and semantics, then expand to intact episodes (and optionally messages) only when that reduces the reader model's uncertainty. No pruning inside units. On those benchmarks it beats the five baselines on quality and token use. The paper notes that LLM-generated structure (e.g. in A-Mem, MemoryOS) is brittle: formatting deviations, failed updates. Because xMemory builds its hierarchy with LLM summaries, it adopts the same brittleness.\n\n**Neotoma** builds structure without LLMs in the critical path. Entities are typed; relationships and timelines are explicit; retrieval uses schema, entity IDs, relationships, and time ranges. Same input and schema yield the same output. Schemas still evolve. Unknown fields land in a preservation layer. A deterministic pipeline can promote high-confidence fields to the schema. An LLM can suggest new fields or types as pending recommendations, applied only via tooling or human approval. Inference stays advisory: schema changes go through tooling or human approval; extraction and reduction stay deterministic; the schema remains source of truth. The paper's critique applies when the model *drives* structure, not when it suggests and humans or tooling apply. Ingest-to-retrieve stays deterministic.\n\n### Comparison\n\n| | xMemory | Neotoma |\n|--|--------|--------|\n| Structure source | Embeddings + LLM summaries (episodes, semantics, themes) | Schema-first, deterministic extraction and reducers |\n| Hierarchy | Four levels (messages, episodes, semantics, themes), guided by sparsity-semantics objective | Typed entities, relationships, timelines (no fixed \"levels\") |\n| Retrieval | Top-down: representative selection on graph, then uncertainty-gated expansion to intact episodes/messages | By schema, entity IDs, relationships, timelines |\n| Redundancy control | Representative selection + expand only when uncertainty drops | Structural queries return what you ask for; no similarity collapse |\n| Intact units | Yes (no pruning inside episodes/messages) | Yes (observations and entities kept whole) |\n| Determinism | No (LLM-generated structure varies) | Yes (same input, same schema, same output) |\n| Brittleness | Paper cites LLM formatting deviations, failed updates in similar systems | Schema and code are explicit; no LLM in the critical path |\n\n### Relative advantages\n\n**xMemory** excels when the input is a conversation stream and you want structure without defining schemas. Example: long-running chat with an assistant where you ask \"what did we decide about the trip?\" or \"when did I last mention the budget?\" xMemory builds episodes, semantics, and themes; retrieval is token-efficient. It also fits fast prototypes (support tickets, meeting notes) where you don't want to author schemas yet. You accept hierarchy drift and don't need auditability or first-class query by entity.\n\n**Neotoma** excels when you need traceability or your data is already structured. Example: auditable decisions (payments, agreements, task outcomes) where same inputs and schema must yield the same snapshot. Schema changes are versioned and applied deterministically; no LLM in the path. It is also the right fit for typed entities (tasks, contacts, transactions, events) with relationships and timelines. Query by entity type, ID, relationship, or time range. Neotoma treats those as native; xMemory would require serializing to text and loses first-class access.\n\n## Iterative structuring in conversation\n\nStructure often emerges in dialogue: \"add a task for that,\" \"record that we agreed to pay 500,\" and the agent acts. The two systems handle that differently.\n\n**xMemory:** The conversation is the primary object. What the agent does (e.g. \"I've created a task for the dentist\") stays in the message stream and flows into episodes, semantics, and themes. You get a better learned hierarchy but no separate, queryable entity graph. Structure lives inside the hierarchy.\n\n**Neotoma:** The conversation is one source of observations. When the agent creates or updates a task, contact, or transaction, those operations produce observations and entity snapshots. New fields from the dialogue can land in a preservation layer and be promoted to the schema when confidence is high. The dialogue and the structured graph stay in sync because both write into the same store.\n\n**Differing retrieval.** xMemory supports semantic retrieval over the hierarchy. Natural-language questions (\"what did we decide about the dentist?\") return themes, semantics, or intact episodes. It does not support structural retrieval (no entity types with IDs and relationships). That leads to expected failures in three kinds of cases:\n\n- **Evidence spread across turns.** \"Did we resolve the invoice dispute?\" The dispute, the negotiation, and the payment may live in different episodes or themes; retrieval can surface one or two and miss the rest, so the model answers incorrectly or incompletely.\n- **Set queries.** \"What tasks are due before Friday?\" or \"Show all payments to contact X.\" There are no task or transaction entities to filter; you get semantic matches (messages that mention \"task\" and \"Friday\" or \"contact X\"), not a definitive list, so results are partial or noisy.\n- **Relationship traversal.** \"Which tasks in project Y are still pending?\" Without a project-task graph, retrieval returns conversation snippets that may omit some tasks or projects; you cannot reliably enumerate by relationship.\n\nNeotoma supports both. You can ask semantic-style questions when the data lives in the store. You also get structural retrieval by entity type, ID, relationship, and time window, so set queries and relationship traversal return complete, first-class results. The tradeoff is that you need schemas and a store that accept those observations.\n\n## Structure over similarity, schema-first over brittleness\n\nFor agent memory, similarity over raw text fails. Retrieval has to be driven by structure: how you decompose and organise the stream, not how many chunks match a query. The paper shows that a learned hierarchy (xMemory) beats naive RAG and that LLM-generated structure is brittle.\n\nHowever, a deterministic, schema-first path gives you the same structural advantage without that brittleness. I'm building [Neotoma](https://github.com/markmhendrickson/neotoma) on the latter so ingest and retrieval stay reproducible and the schema stays source of truth.\n\n[^1]: **Naive RAG:** embed memories, retrieve fixed top-k by similarity, no hierarchy. No separate project; baseline defined in the [paper](https://arxiv.org/abs/2602.02007).\n[^2]: **A-Mem:** agentic memory for LLM agents; Zettelkasten-style links and agent-driven updates to a memory network. [Project](https://github.com/agiresearch/A-mem).\n[^3]: **MemoryOS:** hierarchical short/mid/long-term storage with update, retrieval, and generation modules for personalized agents. [Project](https://github.com/BAI-LAB/MemoryOS).\n[^4]: **LightMem:** lightweight memory inspired by Atkinson-Shiffrin stages; topic-aware consolidation and offline long-term updates. [Project](https://github.com/zjunlp/LightMem).\n[^5]: **Nemori:** self-organizing episodic memory with event segmentation and predict-calibrate for adaptive knowledge. [Project](https://github.com/nemori-ai/nemori).\n",
    "heroImage": "why-agent-memory-needs-more-than-rag-hero.png",
    "heroImageStyle": "keep-proportions",
    "heroImageSquare": "why-agent-memory-needs-more-than-rag-hero-square.png",
    "createdDate": "2026-02-05",
    "updatedDate": "2026-02-06",
    "ogImage": "og/why-agent-memory-needs-more-than-rag-1200x630.jpg",
    "summary": "- RAG fails for agent memory because of redundant top-k, pruning that breaks evidence chains, and similarity that ignores structure.\n- The paper validates structure over similarity: retrieval should follow the organisation induced by decomposition and aggregation, not a ranking over raw spans; xMemory does it with a learned hierarchy, Neotoma with schema-first design.\n- xMemory gives semantic retrieval over a four-level hierarchy but no structural retrieval; Neotoma gives both and stays deterministic, with optional advisory LLM suggestions for schema changes.\n- Semantic-only retrieval leads to expected failures when evidence is spread across turns, when you need set queries (e.g. tasks due before Friday), or when you need relationship traversal (e.g. tasks in project Y).\n- Deterministic, schema-first structure is more reliable than LLM-generated structure; inference can suggest schema changes without breaking determinism if it stays advisory and tooling or humans apply."
  },
  {
    "slug": "building-structural-barriers",
    "title": "Building structural barriers that incumbents can't copy",
    "excerpt": "You don't protect it through secrecy or patents. You build something they structurally can't pursue.",
    "published": true,
    "publishedDate": "2026-02-05",
    "category": "essay",
    "readTime": 5,
    "tags": [
      "strategy",
      "neotoma",
      "positioning",
      "defensibility"
    ],
    "body": "My dad asked me a good question this week: \"How do you protect your intellectual property when Big Guys could develop their own version?\"\n\nThe short answer: you don't protect it through secrecy or patents. You build something they structurally can't pursue.\n\n## The myth of the copyable startup\n\nThe standard startup fear goes like this: you build something, it works, and then Google/Microsoft/OpenAI ships the same thing in six months with better distribution.\n\nThis fear assumes incumbents can copy anything. They can't.\n\nNot because they lack engineers or capital. They have both. They can't copy certain architectures because their existing business models, technical stacks, and organizational incentives make it structurally expensive or impossible.\n\n## Five structural barriers that actually work\n\n### Incentive misalignment\n\nBuild something that would cannibalize a profitable revenue stream.\n\nExample: A privacy-first memory system that runs entirely on user-controlled infrastructure directly conflicts with ad-supported business models. The incumbent would have to choose between their existing margin pool and the new product. They typically choose the existing margin.\n\n### Architectural constraints\n\nBuild on foundations that require rewrites of core systems.\n\nExample: Event-sourced, deterministic workflows with full provenance require immutable data structures and hash-based entity IDs. Platforms built on mutable document stores or eventual consistency models would need to rebuild their entire data layer. That's a multi-year rewrite, not a feature sprint.\n\n### Business model conflicts\n\nBuild something that requires a different pricing model or customer relationship.\n\nExample: Mid-market annual contracts (€3k–€15k per year) with deep workflow integration don't fit self-serve, usage-based billing platforms. The sales motion, support model, and product expectations are fundamentally different.\n\n### Distribution mismatch\n\nBuild for a customer segment the incumbent can't reach effectively.\n\nExample: Sovereignty-conscious power users who actively avoid platform lock-in won't adopt features from the very platforms they're trying to escape. The incumbent's distribution advantage becomes a liability.\n\n### Timing and focus windows\n\nBuild fast enough that by the time they could copy, your compounding advantages make it irrelevant.\n\nExample: If you can ship deterministic workflows, cross-domain entity resolution, and extensible object schemas in 12 months while the incumbent is navigating internal prioritization, compliance reviews, and competing roadmap pressures, you'll be three generations ahead before they start.\n\n## Neotoma as a case study\n\nI'm building [Neotoma](/posts/truth-layer-agent-memory) ([GitHub](https://github.com/markmhendrickson/neotoma)) as a privacy-first, deterministic memory substrate for AI tools. It demonstrates all five barriers.\n\n**Privacy-first architecture (incentive misalignment).** Model providers make money from telemetry, training data, and platform lock-in. A system where users own their memory and can use any model directly conflicts with their revenue model. They could build it, but they'd be undermining their existing business.\n\n**Deterministic, event-sourced design (architectural constraints).** Neotoma uses immutable observations, hash-based entity IDs, and deterministic reducers. This guarantees that the same operation always produces the same final state. Platforms built on mutable stores or eventually-consistent systems would need to rebuild their data layer from scratch.\n\n**Cross-platform interoperability (business model conflicts).** Neotoma works with ChatGPT, Claude, and Cursor through MCP. Supporting competitor platforms directly conflicts with single-model lock-in strategies. Incumbents optimize for keeping users inside their ecosystem, not enabling them to leave.\n\n**Sovereignty positioning (distribution mismatch).** The target customers are people who explicitly want control over their data and memory. They won't adopt memory features from the platforms they're trying to avoid. Distribution advantage becomes distribution liability.\n\n**Architectural compounding (timing).** Every entity resolved, every schema extended, every workflow made reproducible increases the cost of replication. The value isn't in any single feature. It's in the network of typed relationships, the audit trail, and the compounding quality of deterministic memory.\n\n## General principles for defensible startups\n\nWhen evaluating startup ideas, ask these questions.\n\n**Would copying this hurt their existing revenue?** If yes, they won't do it fast. If no, you're competing on execution speed alone.\n\n**Does this require architectural changes they can't make incrementally?** If they can add it as a feature, they will. If it requires a rewrite, you have time.\n\n**Does this require a different customer relationship or sales motion?** If their existing go-to-market doesn't fit your ICP, they can't reach your customers effectively even if they build the same thing.\n\n**Is your distribution actually an advantage for this product?** Sometimes incumbents have anti-distribution for specific customer segments.\n\n**Can you compound advantages faster than they can mobilize?** Speed matters, but it's speed of compounding, not speed of shipping features.\n\n## What doesn't work\n\nThese don't create structural barriers.\n\n**Complexity alone.** Hard to build isn't the same as structurally hard to copy. If it's just engineering complexity, they have more engineers.\n\n**Better UX.** UI is easy to copy. Design systems can be replicated in weeks.\n\n**Network effects (early stage).** Network effects take time to compound. In the first year, you don't have them yet.\n\n**Patents.** Tech patents are expensive to enforce and easy to work around. Focus on structural barriers instead.\n\n**Secrecy.** Once you ship, the approach is visible. Secrecy buys you months at most.\n\n## The goal isn't to outrun them\n\nThe goal is to build something they can't pursue without making choices they won't make.\n\nYou're not trying to be faster forever. You're trying to construct a position where speed stops mattering because the structural constraints do the defending.\n\nFor Neotoma specifically: OpenAI could build a privacy-first, cross-platform memory system. But doing so would require them to give up telemetry, platform lock-in, and single-model revenue streams. They're structurally disincentivized from making that trade.\n\nThat's not fear-based positioning. That's understanding the board.\n\n## Takeaway\n\nWhen someone asks \"what if Big Guys copy you?\", the answer isn't \"we'll move faster\" or \"we have patents.\"\n\nThe answer is: \"We built something that would require them to make structural changes they're incentivized not to make.\"\n\nThat's the only moat that lasts.\n",
    "heroImage": "building-structural-barriers-hero.png",
    "heroImageStyle": "keep-proportions",
    "heroImageSquare": "building-structural-barriers-hero-square.png",
    "createdDate": "2026-02-05",
    "updatedDate": "2026-02-10",
    "summary": "- Incumbents face structural constraints that prevent them from copying certain architectures, even if they have more resources.\n- Effective barriers arise from incentive misalignment, architectural constraints, business model conflicts, distribution mismatch, and timing windows.\n- Neotoma demonstrates these principles: privacy-first architecture directly conflicts with incumbent revenue models, deterministic workflows require complete rewrites, and cross-platform support undermines lock-in strategies.\n- The goal is not to outrun replication through speed or secrecy, but to construct systems whose value compounds through principles incumbents are structurally disincentivized to pursue.\n- When evaluating defensibility, ask: would copying hurt their existing revenue, require non-incremental architectural changes, or demand a different customer relationship they can't serve?",
    "shareTweet": "My dad asked: \"How do you protect your IP when Big Guys could copy you?\"\n\nShort answer: you don't protect through secrecy or patents. You build something they structurally can't pursue.\n\nFive barriers that actually work: https://markmhendrickson.com/posts/building-structural-barriers",
    "ogImage": "og/building-structural-barriers-1200x630.jpg"
  },
  {
    "slug": "openclaw-and-the-truth-layer",
    "title": "OpenClaw is powerful at doing but could use some help remembering.",
    "excerpt": "OpenClaw runs on your machine, clears your inbox, and builds skills. The gap is memory you can verify, query, and fix. A truth layer under it gives you provenance, rollback, and the same result every time you ask.",
    "published": true,
    "publishedDate": "2026-02-04",
    "category": "essay",
    "readTime": 5,
    "tags": [],
    "body": "## What OpenClaw gets right\n\nOpenClaw (and the broader Claw/Clawdbot wave) is the first time a lot of people have felt like they have a real personal AI. It runs on your machine. It has persistent memory. It can read your texts, manage your calendar, browse the web, fill forms, and build skills that get better as you use it.\n\nBrandon Wang's [bull case](https://brandon.wang/2026/clawdbot) is a good read: promise extraction from texts into calendar events, price alerts with complex criteria (e.g. \"pullout bed OK if not in the same room as another bed\"), freezer inventory from photos into Notion, Resy booking that intersects your calendar with restaurant availability.\n\nThe agent *does* things. It also *remembers* things. Context accumulates. That's the \"sweet elixir of context\" he talks about.\n\nSo on the axis of \"can the agent act on my behalf and learn my preferences,\" the answer is yes. The gap I care about is the other axis: how that memory is stored and whether it's something you can trust, replay, and fix when it goes wrong.\n\n## Where \"more context\" hits the same ceiling\n\nI run a lot of my life through [one agent (Cursor plus MCP): email, tasks, finance, contacts, content](/posts/agentic-search-and-the-truth-layer). I've hit limits that aren't about retrieval or model size. They're about state.\n\n- **Overwrites with no undo.** The agent updates a contact or merges two tasks. The previous state is gone. There's no versioning, no rollback. Writes are in-place.\n- **No provenance.** When the agent gives a wrong number or a wrong total, I can't trace it back to a specific record or import. I don't know which observation led to which answer.\n- **No canonical identity.** \"Acme Corp\" in one session and \"ACME CORP\" in the next may or may not be treated as the same entity. The agent re-infers each time. There are no stable IDs or merge rules.\n- **Non-deterministic answers.** Same question (\"what's my total spend with vendor X?\"), different answer tomorrow. Missed files, truncated search, or different entity resolution. No way to reproduce or verify.\n- **Tool-bound memory.** What the agent \"knows\" lives inside that tool's memory or that provider's context. I can't use the same contacts and tasks from Claude.ai or ChatGPT. The memory isn't shared across the tools I use.\n\nThose limits don't go away when the agent gets *more* capable or *more* context. They get sharper. The more the agent does (calendar, contacts, tasks, transactions), the more you need a place where that state is first-class: identity, lineage, and the ability to query it deterministically and roll it back when something breaks.\n\n## What a truth layer adds under an agent like Claw\n\nA [truth layer](/posts/agentic-search-and-the-truth-layer) isn't a replacement for the agent. It's the layer *under* it. The agent keeps doing: reading texts, browsing, filling forms, making calendar events, building skills. The layer is where the resulting state lives and how it's queried.\n\n- **Persistent canonical identity.** Contacts, tasks, transactions, events get stable IDs. \"Acme Corp\" and \"ACME CORP\" resolve to one entity by rule, not by per-session inference.\n- **Provenance and audit.** Every record can be traced to a source (import, agent action, user edit) and a time. When a number is wrong, you can see where it came from.\n- **Deterministic queries.** \"Every transaction with vendor X in the last two years\" or \"all tasks for project Y\" hit a structured store. Same query, same result. No re-search, no truncation, no re-inference.\n- **Recoverability.** When the agent overwrites a contact or merges two tasks by mistake, you have versioning and an audit trail. You can see what changed and roll back. Mutations are explicit; they're not silent overwrites.\n- **Cross-tool truth.** The same contacts, tasks, and execution plans are available to Cursor, Claude, ChatGPT, or Claw, via something like MCP. One memory substrate, many agents.\n\nSo Claw (or any Claw-style agent) would still own the \"do\" part: interpret intent, browse, fill forms, create events, learn workflows. The truth layer would own the \"remember\" part: canonical entities, timelines, provenance, and idempotent, replayable queries. The agent writes into the layer and reads from it. You get the lift of an agent that does things *and* a memory that doesn't drift, overwrite without trace, or disagree across sessions or tools.\n\n## Concrete picture\n\nImagine Claw creating a follow-up task after you promise something in a text. Today that might live in the agent's memory or in a local list. With a truth layer, that task is a first-class entity: linked to the conversation that created it, to the contact if relevant, and to any project or execution plan. You can query \"all follow-ups from last week\" or \"tasks linked to this contact\" from any tool that talks to the layer. If the agent later merges two tasks by mistake, you have a history of changes and can revert.\n\nOr: Claw helps you track spending with a vendor. Without a structured store, it re-searches exports and emails each time and re-infers entity resolution. Totals can change. With a truth layer, transactions are normalized and tied to a canonical vendor ID. \"Total spend with vendor X\" is a query, not a one-off assembly. Same question, same answer. And if the agent \"corrects\" a transaction based on wrong inference, you have an audit trail and the option to roll back.\n\nBrandon mentions writing workflows to Notion so he can see what Claw has learned. That's visibility into behavior. A truth layer adds visibility into *state*: what entities exist, how they're linked, where they came from, and how they changed. That's the complement to \"the agent did something.\" \"The agent did something, and here's the state it wrote, with lineage and the ability to fix it.\"\n\n## Why I'm building Neotoma this way\n\nI'm building [Neotoma](https://github.com/markmhendrickson/neotoma) as a structured memory layer with those primitives: entity resolution, timelines, provenance, determinism, and cross-platform access via MCP. I'm dogfooding it in my own agentic stack to see where they matter. The lesson from that work is that [retrieval (embedding-based or agentic)](/posts/agentic-search-and-the-truth-layer) and \"more context\" don't by themselves give you stable identity, verifiable state, or recoverability. Something that does has to sit underneath. OpenClaw and its ecosystem are proving that agents can do a lot. I think the next step is making sure what they do is grounded in a memory layer that you can trust, query, and fix. That's the layer I'm building.\n",
    "heroImage": "openclaw-and-the-truth-layer-hero.png",
    "heroImageStyle": "keep-proportions",
    "createdDate": "2026-02-04",
    "updatedDate": "2026-02-09",
    "ogImage": "og/openclaw-and-the-truth-layer-1200x630.jpg",
    "summary": "- OpenClaw and Claw-style agents excel at doing (calendar, texts, browsing, forms, skills). The gap is remembering in a way that is verifiable, consistent, and recoverable.\n- More context does not fix overwrites, missing provenance, unstable canonical identity, non-deterministic answers, or tool-bound memory. Those limits are about state, not retrieval.\n- A truth layer sits under the agent: the agent keeps doing; the layer keeps state with canonical identity, provenance, deterministic queries, recoverability, and cross-tool access.\n- With a truth layer, tasks and transactions become first-class entities with lineage. Same query yields same result; bad writes can be rolled back; the same data is available to Cursor, Claude, ChatGPT, or Claw via MCP.\n- Visibility into what the agent learned (e.g. workflows in Notion) is behavior. A truth layer adds visibility into state: what entities exist, how they are linked, where they came from, and how they changed."
  },
  {
    "slug": "barcelona-guest-floor",
    "title": "Barcelona guest floor",
    "excerpt": "Your own floor in the heart of Barcelona. Quiet street in Gràcia, kitchen, king bed, rooftop terrace. Welcoming to all.",
    "published": true,
    "publishedDate": "2026-02-04",
    "category": "article",
    "readTime": 4,
    "tags": [
      "barcelona",
      "rental",
      "gràcia",
      "guest floor"
    ],
    "body": "Thanks for your interest in staying with us during your visit to Barcelona, Spain!\n\nWe live in a townhouse on a quiet side street near the heart of the city, with a top-floor designed to host friends and family in a separate unit with its own private kitchen, living room, bedroom and bath.\n\nThis guest floor is directly accessible upon entry into the building via an elevator that goes to all floors, including a rooftop terrace that you're invited to enjoy during your stay.\n\n## Our home is\n\n- LGBTIQA+ friendly\n- Kid friendly\n\n## Contact\n\nIf interested, contact us via email or WhatsApp:\n\n**Ana:** [+34 680 658 030](tel:+34680658030) or [ana.serrano.solsona@gmail.com](mailto:ana.serrano.solsona@gmail.com)\n\n**Mark:** [+34 722 513 042](tel:+34722513042) or [markmhendrickson@gmail.com](mailto:markmhendrickson@gmail.com)\n\n## What this place offers\n\n- Wifi ([speed test result](https://www.speedtest.net/es/result/18737866754))\n- Air conditioning and heating\n- Shared patio or balcony\n- Crib\n- Dedicated workspace\n- Window AC unit\n- Luggage dropoff allowed\n- Hair dryer\n- Free laundry (washer and dryer)\n- Living room with sleep-able sofa\n- Kitchen with full appliances\n- King-sized Tempur-Pedic bed\n- Spacious bathroom with walk-in shower\n- Elevator access\n- Rooftop terrace with barbecue\n- Bike-share program credentials\n- Ninebot KickScooter MAX G30E II Powered by Segway\n- Apple Home Pod speaker\n- Nespresso coffee machine\n- Peaceful side street location\n- Space for exercise (yoga)\n\n## Availability and prices\n\nPlease contact us for direct bookings and availability with the following pricing in mind, for payment via cash or crypto.\n\n### Standard rates (low season: November–March, excluding holidays)\n\n| Stay | Price |\n| --- | --- |\n| Per night (up to 6 nights) | €200 |\n| 1 week (7 nights) | €1,100 (then €125/night up to 2 weeks) |\n| 2 weeks (14 nights) | €1,400 (then €100/night up to 3 weeks) |\n| 3 weeks | €1,800 (then €75/night up to 4 weeks) |\n| 1 month / 4 weeks | €2,200 |\n\n### Shoulder season (April–May, October)\n\n| Stay | Price |\n| --- | --- |\n| Per night (up to 6 nights) | €240 |\n| 1 week (7 nights) | €1,300 |\n| 2 weeks (14 nights) | €1,700 |\n| 3 weeks | €2,200 |\n| 1 month / 4 weeks | €2,990 |\n\n### Peak season (June–September)\n\n| Stay | Price |\n| --- | --- |\n| Per night (up to 6 nights) | €280 |\n| 1 week (7 nights) | €1,600 |\n| 2 weeks (14 nights) | €2,000 |\n| 3 weeks | €2,600 |\n| 1 month / 4 weeks | €3,990 |\n\n*Special events:* Additional premiums apply during [Mobile World Congress](https://www.mwcbarcelona.com/) (March 3–7, +40%), [Primavera Sound](https://www.primaverasound.com/) (June 1–5, +30%), [Festa Major de Gràcia](https://ajuntament.barcelona.cat/gracia/ca/el-districte-i-els-seus-barris/la-vila-de-gracia/festa-major) (August 15–21, +25%), [Festa Major de Sants](https://barcelonasecreta.com/en/sants-festival-2025-barcelona/) (August 23–31, +25%), and [La Mercè](https://www.barcelona.cat/lamerce/en) (September 23–28, +25%). Stays overlapping events are priced night-by-night for the overlapping dates. Monthly stays: March €2,600, June €4,410, August €5,110, September €4,410 (event days prorated).\n\n**Additional information:**\n\n- Electricity and water utilities are included at no extra cost.\n- Housecleaner will prepare the guest floor ahead of your arrival though not during your stay unless requested for an extra cost of €50 per cleaning.\n- You also have the option to book via our [Airbnb listing](https://www.airbnb.com/) if you'd like to stay for over one month.\n\n## Photos\n\n### Bedroom\n\n![Bedroom exterior view](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F586a4d66-fd0a-4ae7-bc4f-844b45b2892c%2Fdormitorio_exterior.jpg?table=block&id=c8f26fe7-0d05-4081-9ffe-5ade93310a04&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Bedroom interior](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2Fa2ad780f-dfc1-4ad2-aa3c-206b7c725dbf%2Fdormitorio_interior.jpg?table=block&id=17bd7078-3396-4034-878d-5997d6bbfae9&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Bedroom](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2Fee144e0c-1f55-4c92-bde0-8bee0ba6d7ae%2Fdormitorio.jpg?table=block&id=11b545b0-a18f-4cd2-9a78-c6257ee3e2dc&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n### Bathroom\n\n![Bathroom vanity](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F4057ee1e-df6e-4cd1-94b0-31cb126545eb%2Ftocador.jpg?table=block&id=2c989f1c-e6d1-43e5-99c9-bc29026b610e&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Bathroom sink and shower](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F5777853a-4f29-49e8-9df7-05c9561cca80%2Flavabo_ducha.jpg?table=block&id=aa98b51e-4408-4f29-9320-86b3c1ade0ea&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Bathroom sink](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F3211cfb0-b081-42f1-9430-cd37c783ce13%2Flavabo.jpg?table=block&id=7c25ee12-7be0-4a79-9bb7-ec23185c3992&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n### Kitchen\n\n![Kitchen cabinets](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F605fe0b7-d6bd-41f2-b38a-0cae1ca101ab%2Fcocina_armarios.jpg?table=block&id=9f87e304-f045-4b45-8d97-09ca67f7feff&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Kitchen drawers](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F41da9c71-e632-4b2f-9fba-c3e88ee1d3a9%2Fcocina_cajones.jpg?table=block&id=0fb1c1f5-925d-446e-9ff1-3bbd5e94b5fe&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Kitchen refrigerator](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F66b03206-ce45-47a0-964d-50997fccba8e%2Fcocina_nevera.jpg?table=block&id=e3613bf0-01ec-4716-a4c9-c44b8db66632&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Kitchen](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F31f68117-8703-400d-bf93-1648337c2c90%2Fcocina.jpg?table=block&id=962c0f53-441c-48ef-86aa-322c58a96116&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n### Living room\n\n![Living room](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F26b1e2c2-55be-4c06-8d3b-e3be9c1dd091%2Fliving.jpg?table=block&id=3fa0a7f3-8535-49a0-a4bb-6f060913a5f7&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Living room detail](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F693ca849-a67c-40c5-8f2d-b768616d2980%2Fliving_detalle.jpg?table=block&id=e6dc791a-2fd8-41ea-bae5-70662fa26d90&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Dining area detail](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F68598226-f5b6-4983-ae68-1799d5adba8f%2Fcomedor_detalle.jpg?table=block&id=96dbdebd-ddf2-4aa6-a4bb-1e30d25d1c1a&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Dining area](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2Faea7b845-47ff-4350-bd1e-b160dc37f468%2Fcomedor.jpg?table=block&id=a81812b4-9531-4fc3-936a-2850753a43ca&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n### Rooftop terrace\n\n![Rooftop terrace detail](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F388951aa-f6fd-40d1-9dc0-5b8b68637fe4%2Fexterior_detalle.jpg?table=block&id=a02b8a58-6724-43e9-b6db-66bf27b9e60d&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Rooftop terrace living area](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F7b188fca-1266-4a4b-81ff-681d7ea059f0%2Fexterior-living.jpg?table=block&id=b7ff84fd-12ca-40b4-b735-c65d188f223d&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n### Elevator, stairs and entrance\n\n![Elevator](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F5841d439-98b6-40ac-ae57-683ebb2551e5%2Fascensor.jpg?table=block&id=c5f9799a-8a2b-4ae8-b892-702ce85118e0&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Stairs](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F21448f11-50c5-4e40-b39b-25687db9659f%2Fescalera.jpg?table=block&id=b3ca3a66-f67d-475a-a23b-75c62a5c29fd&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Entrance hall](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2Ffe5d99ad-e66b-44c6-ac73-cda2ca00ef71%2Frecibidor.jpg?table=block&id=fcd1961f-02c1-4780-a5bb-c61ed80e4591&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Building entrance](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2Fd87d9ecb-23fe-47d7-9864-9b5c00ef532c%2Fportal.jpg?table=block&id=ac530e4f-cd54-41f6-b830-20acced261c1&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n## How to get here\n\nOur home is located in the residential neighborhood of Gràcia on a quiet side street near the center of Barcelona, just minutes walking from the famous [Sagrada Familia](https://sagradafamilia.org/).\n\nIf you arrive by plane, you can take a 20–30 minute taxi from the airport for an estimated €35–40.\n\nIf you arrive by car, there are various parking garages in the vicinity.\n\n## What's nearby\n\nThere are many sites to see within walking distance, with others conveniently accessible by bike, taxi or subway:\n\n- [Sagrada Familia](https://sagradafamilia.org/) (5 minute walk)\n- [Parc Güell](https://parkguell.barcelona/) (10 minute taxi or 27 minute walk)\n- Central Gràcia (12 minute walk)\n- Local farmers market (1 minute walk, open daily apart from Sunday)\n- Supermarkets Veritas (all organic) and others, 2 minute walk\n- Convenience store (1 minute walk)\n- Wide boulevard of Sant Joan with kids playgrounds (2 minutes walk)\n- Café with specialty coffee Bar La Camila (6 minute walk)\n- Bakeries Origo and Turris (6 minute walk)\n- Beach (15 minute taxi, 14 minute bike or 47 minute walk)\n- Central Park / Ciutadella (7 minute bike or 30 minute walk)\n- Gothic quarter / El Born / old city (8 minute bike or 27 minute walk)\n- Las Ramblas (10 minute taxi, 10 minute bike or 33 minute walk)\n- Passeig de Gràcia / luxury shopping district (6 minute bike or 15 minute walk)\n- Montjuïc / magic fountain (15 minute taxi or 27 minute subway)\n\nSee [Spain visitor information](https://www.spain.info/en/) for restaurants and other recommendations.\n",
    "heroImage": "barcelona-guest-floor-hero.jpg",
    "heroImageStyle": "keep-proportions",
    "createdDate": "2026-02-04",
    "updatedDate": "2026-02-04",
    "excludeFromListing": true
  },
  {
    "slug": "agentic-search-and-the-truth-layer",
    "title": "Agentic retrieval infers. It doesn't guarantee.",
    "excerpt": "Your AI finds things. It also misses things, overwrites them, and often can't say where an answer came from. Why that happens and what would fix it.",
    "published": true,
    "publishedDate": "2026-02-04",
    "category": "essay",
    "readTime": 5,
    "tags": [
      "neotoma",
      "agentic-search",
      "truth-layer",
      "rag"
    ],
    "body": "[Boris Cherny (creator of Claude Code at Anthropic) tweeted](https://x.com/bcherny/status/2017824286489383315) that Claude Code moved from RAG plus local vector DB to agentic search. It works better, he said, and is simpler, with fewer issues around security and privacy. Other tools take a different path. Cursor, for example, uses cloud-based embeddings to index the codebase and search by semantic similarity.\n\nSo we have at least two retrieval paradigms: embedding-based search (pre-indexed, vector similarity) and agentic search (on-demand tool use). They are not the same. Each has different tradeoffs. Both are retrieval strategies. A truth layer is something else. It persists canonical entities, maintains provenance, and supports deterministic queries. It's about state, not retrieval. This post compares a truth layer to both retrieval models. It also ties in the limits I've hit when relying on retrieval alone.\n\n## Where I've hit limits\n\nI use Cursor as my central interface for all of my digital workflows, not just coding. Email triage, task management, finance queries, content planning, transactions, contacts. They all run through the same agent with access to the same repo. Agentic search across files often works well. The agent finds context, infers connections, and gets things done.\n\nBut I've hit limits. The agent infers; it doesn't guarantee. Here's what that looks like:\n\n- **Large datasets, incomplete recall.** On-demand search misses things or truncates across thousands of transactions or hundreds of contacts. Retrieval re-derives each time. There's no structured store to query for complete results.\n- **Irrecoverable overwrites.** An agent overwrites a contact or task and the previous state is gone. No rollback. Writes are in-place. There's no versioning or append-only trail to trace and roll back.\n- **No cross-tool access.** I can't use the same records from Claude.ai or ChatGPT. Retrieval is provider-bound.\n- **Non-reproducible answers.** Same question, different answer. I can't reproduce a result for verification or debugging. Retrieval is non-deterministic.\n- **No traceability.** When the agent gives a wrong number or claim, I can't trace it back to source files or records. Retrieval has no provenance.\n- **Unstable canonical identity.** The agent may treat \"Acme Corp\" and \"ACME CORP\" as the same in one session and different in the next. Retrieval re-infers each time. There are no persistent canonical IDs or merge rules.\n\n## Two retrieval paradigms, one state paradigm\n\nEmbedding-based search and agentic search both get information to an agent. They are not the same. Embedding-based search (e.g. Cursor) pre-indexes a corpus and answers via vector similarity. The index can be cloud-hosted and updated. Agentic search (e.g. Claude Code) skips a persistent index and uses tools to search and read on demand. Different implementations, different tradeoffs: privacy, staleness, simplicity.\n\nWhat they share is retrieval. The agent finds things at query time. A truth layer is not retrieval. It is persistent, structured state: canonical entities, provenance, deterministic queries.\n\nWe're comparing one state paradigm (truth layer) to two retrieval paradigms (embedding-based and agentic). The table below lines up all three. Where both retrieval columns share a limit (e.g. no provenance), that's a similarity between them relative to a truth layer. It's not an equation of the two.\n\n| Domain | Embedding-based search | Agentic search | Truth layer |\n|--------|------------------------|----------------|-------------|\n| Document retrieval | Pre-indexed similarity, semantic match | On-demand search, inference | Entity resolution, dedup, provenance |\n| Multi-source aggregation | Index scope and freshness depend on build | Live search across sources | Unified graph, deterministic merge |\n| Entity lookup | Similarity over embeddings; no canonical ID | Per-session inference | Canonical IDs, rule-based merge |\n| Timeline queries | Only if indexed; no native time model | On-demand assembly | Pre-computed, schema-driven |\n| Provenance and audit | None | None | Immutable audit trail |\n| Cross-platform | Tied to provider/index | Provider-specific tools | Same data across tools |\n\nBoth retrieval approaches optimize for convenience and flexibility. A truth layer optimizes for consistency and verifiability.\n\n## What a truth layer provides\n\nA structured memory layer is built around different primitives:\n\n1. **Persistent canonical identity.** Stable entity IDs across sessions and tools.\n2. **Deterministic merge logic.** Rule-based combination of observations, not per-session LLM inference.\n3. **Provenance and audit.** Traceable lineage from source to answer.\n4. **Idempotence.** Same inputs yield same outputs.\n5. **Cross-platform truth.** Same memory across ChatGPT, Claude, Cursor.\n6. **Clear privacy model.** User control, no provider training use, clear data boundaries.\n\nThese are not incremental improvements over agentic search. They are a different design. Best-effort retrieval and orchestration versus verifiable, replayable state. The choice depends on what you need.\n\n## What retrieval can approximate (agentic or embedding-based)\n\nThree examples show retrieval (agentic or embedding-based) approximating the capabilities above. In each example, the agent gets something that looks right for the moment. In each, the same limits show up: no persistent canonical identity, no provenance, no guarantee that \"same query\" yields \"same result\" across sessions or index rebuilds. The examples below use agentic terms (tools, on-demand search). Embedding-based retrieval can approximate the same behaviors via semantic search over an index and hits the same limits.\n\n**Example 1: Session-scoped entity resolution.** The agent has tools to search files, email, and cloud. It has instructions to treat mentions of the same entity as one. You ask: \"What's my total spend with Acme Corp?\" The agent searches bank exports, receipts, invoices. It finds \"Acme Corp\", \"ACME CORP\", \"Acme Corporation\", infers same entity, sums amounts. That looks like entity resolution for this query and session. What goes wrong: ask again tomorrow and the number may differ. The agent may miss a file (truncated search, wrong path) and undercount. Or it may treat \"Acme Corp\" and \"Acme Industries\" as the same and overcount. No way to verify. No audit trail, no stable IDs. Different sessions may disagree.\n\n**Example 2: On-demand timeline assembly.** The agent has broad file and date access. You ask: \"What were my major expenses in Q3 2024?\" The agent searches, parses dates, assembles a chronological list, filters by \"major.\" You get a timeline-like answer without a dedicated timeline system. What goes wrong: \"Major\" is inferred each time. One session excludes a €500 item. The next includes it. Documents with non-standard date formats get dropped or misordered. The agent may truncate (\"here are the top 10\") when there were 15. Same query, different results, every time.\n\n**Example 3: Hybrid memory layer.** A provider ships agentic search plus lightweight memory. The agent extracts structured snippets, stores them, and retrieves them later. It processes a receipt, stores `{vendor: \"Acme Corp\", amount: 150, date: \"2024-07-15\"}`. A later session retrieves this and merges with live search results. That looks like structured memory. What goes wrong: a later extraction overwrites the snippet. No versioning, no rollback. The same vendor appears as \"Acme Corp\" in stored memory and \"ACME CORP\" in a fresh search. Duplicates accumulate. The provider changes the feature or schema and your stored snippets vanish. No way to trace a wrong number back to its source.\n\nIn each example, the behavior approximates what a truth layer provides. The limits are inherent to retrieval. Whether the agent uses embedding search or agentic search, you still get session scope and inference-based merge. You still get no provenance and no cross-platform guarantee. A truth layer addresses those by persisting state instead of re-retrieving it.\n\n## When retrieval excels (agentic or embedding-based)\n\n**Exploratory discovery.** \"Find anything in my downloads or notes about the Barcelona apartment.\" You don't know where it lives or what it's called. Agentic search across files, folders, and formats surfaces relevant snippets. No schema required. The agent infers and assembles.\n\n**Rapid cross-source summarization.** \"What did we decide in the last three emails with the contractor?\" Search inbox, extract thread, summarize. One session, one answer. You don't need that summary to persist or match exactly next time.\n\n**Ad hoc code and docs traversal.** \"Where do we handle Stripe webhooks?\" Search codebase, README, internal docs. Layout varies by repo. Agentic search adapts. No unified graph needed.\n\n**Single-document or single-thread triage.** \"Summarize this PDF\" or \"What's the ask in this email?\" Context is bounded. Inference is sufficient. No entity resolution or cross-session state.\n\n## When a truth layer excels\n\n**Complete recall over large datasets.** \"List every transaction with vendor X in the last two years.\" With thousands of rows, agentic search may miss records, truncate, or hallucinate aggregates. A truth layer queries a structured store. You get all matching records or a precise count.\n\n**Cross-session consistency.** The agent creates a follow-up task in session one. You open a new session tomorrow. The task must be there, linked to the right contact and email. Agentic search has no persistent graph. A truth layer does.\n\n**Audit and provenance.** \"Where did this number come from?\" Trace it to source records, import dates, and derivation rules. Agentic search returns inferred answers. A truth layer returns answers with lineage.\n\n**Entity resolution at scale.** Hundreds of contacts, some duplicates (name variations, merged companies). Thousands of transactions referencing the same vendor under different spellings. A truth layer maintains canonical IDs and merge rules. Agentic search re-infers each session and may disagree.\n\n**Deterministic replay.** Same query, same result, every time. Critical for reporting, compliance, or debugging. Agentic search is non-deterministic. A truth layer is idempotent.\n\n**Recoverability from bad writes.** An agent overwrites a contact, merges two tasks into one, or \"corrects\" a transaction based on wrong inference. With agentic search and direct file writes, the previous state is gone. No undo. A truth layer uses append-only or versioned writes. You can trace what changed and roll back. Mutations are explicit operations, not silent overwrites.\n\n## Why the distinction matters\n\nRetrieval (embedding-based or agentic) is session-bound. It doesn't by itself give you persistent identity, provenance, or cross-session consistency. Its value is flexible, on-demand access. A truth layer's value is persistent, cross-session truth. Deterministic, auditable entity resolution is hard. Neither embedding similarity nor ad hoc agentic search is equivalent. Provider-hosted agents face incentives that conflict with user-controlled, privacy-first memory. Their memory and tools tend to be product-specific.\n\nThe Cherny tweet reflects a real shift. RAG plus vector DB was complex and had privacy implications. Agentic search simplified retrieval for Claude Code. Cursor and others take a different retrieval path (cloud embeddings). Both retrieval paradigms solve \"how does the agent find things?\" Neither solves \"how do we get stable identity, provenance, and verification?\" A truth layer targets the latter. Retrieval and state layers will coexist. They solve different problems.\n\n## What I'm building\n\nI'm building [Neotoma](https://github.com/markmhendrickson/neotoma), a structured memory layer that takes the truth layer approach: entity resolution, timelines, provenance, determinism, cross-platform via MCP. I'm dogfooding it in my own agentic stack to see where these primitives matter in practice. Embedding-based search and agentic search are two retrieval strategies. Neither gives you persistent identity or verifiable state. A truth layer does. I'm building the latter.\n",
    "heroImage": "agentic-search-and-the-truth-layer-hero.png",
    "heroImageStyle": "keep-proportions",
    "heroImageSquare": "agentic-search-and-the-truth-layer-hero-square.png",
    "createdDate": "2026-02-04",
    "updatedDate": "2026-02-09",
    "summary": "- Retrieval (embedding-based or agentic) infers; it doesn't guarantee. A truth layer is persistent, structured state: canonical entities, provenance, deterministic queries. It's about state, not retrieval.\n- Two retrieval paradigms (pre-indexed similarity vs on-demand tool use) share the same limits relative to a truth layer: no persistent canonical identity, no provenance, no cross-session consistency, no cross-platform access.\n- In practice, retrieval leads to incomplete recall at scale, irrecoverable overwrites, non-reproducible answers, no traceability, and unstable canonical identity (e.g. \"Acme Corp\" vs \"ACME CORP\" across sessions).\n- A truth layer provides persistent canonical identity, deterministic merge logic, provenance and audit, idempotence, cross-platform truth, and a clear privacy model. Different design goals, not a spectrum.\n- Retrieval can approximate entity resolution, timelines, and structured memory within a session but re-derives each time; a truth layer persists state instead of re-retrieving it.\n- The choice depends on whether you need best-effort retrieval and orchestration or verifiable, replayable state. Retrieval excels at exploratory discovery and ad hoc summarization; a truth layer excels at complete recall, cross-session consistency, audit, and recoverability from bad writes.",
    "ogImage": "og/agentic-search-and-the-truth-layer-1200x630.jpg"
  },
  {
    "slug": "truth-layer-agent-memory",
    "title": "Building a truth layer for persistent agent memory",
    "excerpt": "Agent memory is forgetful. Why I'm building inspectable, structured memory for trustworthy agentic systems.",
    "published": true,
    "publishedDate": "2026-02-02",
    "category": "essay",
    "readTime": 6,
    "tags": [
      "neotoma",
      "agent-memory",
      "truth-layer",
      "determinism"
    ],
    "body": "I've been working on something called **[Neotoma](https://github.com/markmhendrickson/neotoma)**.[^1]\n\nThere's nothing to try yet. This isn't a launch post, and I'm not announcing a product or asking for signups. The problem has been bothering me for a while, and more importantly, it's been actively getting in the way of work I've been trying to do.\n\nOver the past year, I've spent a lot of time experimenting with agentic systems: automating workflows, delegating tasks to agents, letting systems operate across sessions instead of starting from scratch each time. Again and again, I've run into the same wall. The systems were capable, often impressively so, but I couldn't trust them with real, ongoing state.\n\nThat limitation hasn't just been theoretical. It's been a practical blocker to automation.\n\n## AI systems are quietly changing roles\n\nThey used to be something you just consulted: you asked a question, got an answer, and moved on. Increasingly, they act. They write files and documents, call tools and APIs, refer to past conversations across sessions, and chain decisions over time without being explicitly prompted for each step.\n\nAt that point, personal data stops being reference material and starts becoming *state*.\n\nAnd state has different requirements.\n\n## The thing that keeps breaking is not intelligence, but trust\n\nCurrent AI memory systems are built around convenience. They optimize for recall, speed, and fluency, and for whether the system *feels* like it remembers you. None are built around provenance, inspectability, replay, or clear causality.\n\nIn practice, that means I can get an agent to do something once, but I hesitate to let it do something *again*. Memory changes implicitly. Context drifts. Assumptions accrete. And when something goes wrong, I can't answer what changed, why it changed, or whether the system would make the same decision if I reran it from scratch.\n\nThis is tolerable when AI is advisory but not when it's operational.\n\n## Part of the problem is a category mismatch\n\nWe still treat personal data like notes, text blobs, or loose context. Agents, meanwhile, treat that same data like inputs, constraints, triggers, and long-lived state. You cannot safely automate against data you can't inspect, diff, audit, or replay.\n\nThis isn't a UX problem. It's a systems problem.\n\n## What feels missing is a basic primitive\n\nExplicit, inspectable, replayable personal state.\n\nOther domains solved this long ago. Databases made application state reliable. Event logs made distributed systems understandable. Ledgers made financial history auditable. Personal data never needed that level of rigor before, because humans could carry context in their heads or reconstruct it by reviewing records manually.\n\nAgents change that assumption.\n\n## The uncomfortable implication is that doing this correctly adds friction\n\nState changes can't be implicit.\n\nMemory updates have to be named operations rather than side effects. Inputs have to be visible rather than inferred. History has to be reconstructable rather than hand-waved.\n\nYou give up some magic and accept more ceremony. Otherwise you and your agents will end up living together unreliably through divergent lenses of reality.\n\nThere isn't a shortcut around this tradeoff. Convenience-first systems and agent-safe systems pull in opposite directions.\n\n## I'm treating personal data the way production systems treat state\n\nThat leads to some unavoidable consequences. Behavior has to be contract-first: state changes are explicit, typed operations, not ad hoc updates. Mutations have to be explicit. Nothing \"just updates memory.\"\n\nIf agents are going to act, they need constrained, auditable interfaces rather than opaque prompts or embeddings. Replay matters as much as the current answer: being able to explain how you got here is part of the truth.\n\nSame input always produces the same output since the memory layer is deterministic and agents have a reliable substrate. Changes are immutable and queryable so you can see entity state at any point in time.\n\nMemory comes from both documents you upload and data agents write during conversations, one structured graph unifying entities and events so agents can reason across all of it.\n\nThese aren't aesthetic preferences. They fall directly out of trying, and repeatedly failing, to automate real workflows without losing trust in the system doing the work.\n\n## Why I'm designing it this way\n\nI'm keeping it MCP and CLI-first. There's no web UI and no hidden memory. It's local-first by default, with explicit interfaces for agents. I'm ingesting only what I explicitly provide with no automatic scanning or background ingestion. Those aren't omissions, they're guardrails. They make it harder, accidentally or otherwise, to lie about what the system knows and how it got there.\n\nI'm also making it cross-platform and privacy-first by design. It works with ChatGPT, Claude, and Cursor via MCP, not locked to a single provider. Your data remains yours, user-controlled, never used for training. Those aren't conveniences; they're prerequisites for trust.\n\n## What it's not\n\nIt's not a note-taking app or a \"second brain\"; it's a structured memory substrate for agents.\n\nIt's not provider-controlled ChatGPT Memory or Claude Projects; it's your own substrate, exposed via MCP so any agent can use it.\n\nIt's not a vector store or RAG layer; it's schema-first, structured memory with provenance.\n\nIt's not an autonomous agent or workflow engine or AI assistant with invisible memory; it's the memory layer agents read and write, and you control.\n\n\nAnd it's not something I'd call reliable yet. I'm trying to build the foundation layer before pretending guarantees exist.\n\n## Why now\n\nWe're normalizing systems that take actions on our behalf, persist beliefs, and accumulate decisions over time. When those systems fail, and they will, the first question will be, \"How did this happen?\"\n\nRight now, most tools won't be able to answer that. And over the past year, that inability has been the main thing preventing me from trusting agents with anything that matters. That problem is about to scale.\n\nThe agentic web is emerging. We need one where users remain in control of memory, not one where we hand it to centralized platforms and agents act on our behalf using opaque, unreliable methods. I'm building Neotoma to provide that: a substrate that is inspectable, replayable, and user-controlled as the agentic web grows.\n\n## Upcoming developer preview\n\nI'm working on releasing a developer preview for my own usage and public testing. It will be rough and explicitly unreliable (e.g. APIs may change). Its purpose will be to pressure-test these ideas in real use, not to sell anything.\n\nHow I'm approaching the build: I'm dogfooding it first in my own agentic stack so I can see where determinism and provenance actually help and where they get in the way. Use cases include:\n\n- **Tasks and execution** — Tasks, plans, projects, and outcomes with due dates and follow-up reminders\n- **Contacts and relationships** — Contact records and relationship graph linked to communications, tasks, and events\n- **Communications** — Email triage, workflow-triggered processing, and conversation tracking\n- **Finance** — Transactions, flows, income, holdings, transfer and cost recording\n- **Record keeping** — Purchases, accounts, property and one-off analysis reports\n- **Content** — Posts, personal history, favorite media and consumption sources\n- **Health** — Habits, exercises, and ongoing tracking\n\nI'm prioritizing MCP stability and a minimal CLI before adding more surface area, stress-testing entity and relationship resolution and timeline queries as usage scales.\n\nIf this framing resonates, the work is happening in the open here:\n[https://github.com/markmhendrickson/neotoma](https://github.com/markmhendrickson/neotoma)\n\nStarring the repo is the simplest way to keep track of it as it evolves. Input from people thinking about agentic systems and scalable state is always welcome.\n\n[^1]: Named after the genus *Neotoma* (packrats), known for collecting and preserving material.\n",
    "heroImage": "truth-layer-agent-memory-hero.png",
    "heroImageStyle": "keep-proportions",
    "heroImageSquare": "truth-layer-agent-memory-hero-square.png",
    "createdDate": "2026-02-02",
    "updatedDate": "2026-02-04",
    "summary": "- What keeps breaking AI-based automation is trust, not intelligence. Memory changes implicitly, context drifts, and you can't see what changed or replay it.\n- When agents act, personal data becomes state. The missing primitive is explicit, inspectable, replayable personal state. Other domains have it (DBs, event logs, ledgers). Personal data didn't need it until agents.\n- Doing it right adds friction: named operations, visible inputs, reconstructable history. Convenience-first and agent-safe pull in opposite directions.\n- Neotoma is the \"truth\" layer I'm building. Contract-first, deterministic (same input, same output), immutable queryable state, one graph for documents and agent-written data.\n- The agentic web is emerging. We need user-controlled, inspectable memory, not opaque platforms. I'm building Neotoma as that substrate.\n- MCP and CLI-first, local-first, explicit ingestion only. Cross-platform and privacy-first. Not a note-taking app, provider memory, or vector store. Not reliable yet.\n- Developer preview next. I'm dogfooding it in my own agentic operating system.",
    "ogImage": "og/truth-layer-agent-memory-1200x630.jpg"
  },
  {
    "slug": "professional-mission",
    "title": "Mark Hendrickson",
    "excerpt": "Building sovereign systems at the intersection of crypto and AI",
    "published": true,
    "publishedDate": "2025-01-01",
    "category": "essay",
    "readTime": 2,
    "tags": [],
    "body": "I build systems that restore sovereignty, clarity, and long-range capability to individuals in a world defined by complexity, centralized control, and cognitive overload.\n\nMy technology transforms chaos into structure, volatility into signal, and information abundance into actionable leverage, empowering people to operate with more agency, creativity, and strategic independence.\n\nMy work centers on designing personal infrastructure that is open, privacy-preserving, and fundamentally user-owned. Through data ingestion, contextual modeling, and automation, I eliminate friction and restore the time, mental bandwidth, and autonomy lost to modern digital and institutional systems. This gives people the structural foundation to think more deeply, act more decisively, and build more freely.\n\nI take an antifragile approach: systems grow stronger through disruption, not weaker. The tools I build help people thrive under uncertainty, adapt intelligently to changing conditions, and make decisions from clarity rather than reactivity.\n",
    "heroImage": "profile.jpg",
    "heroImageStyle": "float-right",
    "excludeFromListing": true,
    "showMetadata": 0,
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "the-flip-side",
    "title": "The Flip Side",
    "excerpt": "I've been reading The Flip Side since October and find it's an incredibly effective and efficient way to encounter and digest thoughtful opinions...",
    "published": true,
    "publishedDate": "2019-03-22",
    "category": "essay",
    "readTime": 1,
    "tags": [],
    "body": "I've been reading The Flip Side since October and find it's an incredibly effective and efficient way to encounter and digest thoughtful opinions daily from across the left-right divide in the US. I highly recommend it for bursting your political bubble: [theflipside.io](https://www.theflipside.io)",
    "excludeFromListing": 1,
    "showMetadata": 0,
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "yc-w19-web-developer-contract",
    "title": "Yc W19 Web Developer Contract",
    "excerpt": "Web developers: The founder of a YC W19 company I know is looking to contract immediately for a project involving React, Node, and MongoDB with...",
    "published": true,
    "publishedDate": "2018-12-05",
    "category": "essay",
    "readTime": 1,
    "tags": [],
    "body": "Web developers: The founder of a YC W19 company I know is looking to contract immediately for a project involving React, Node, and MongoDB with hosting on Digital Ocean. If you're interested or know someone who might be, please contact him directly at [jusduan@gmail.com](mailto:jusduan@gmail.com).",
    "showMetadata": 1,
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "facebook-police-state",
    "title": "The Facebook police state",
    "excerpt": "Should Facebook really be responsible for policing the misuse of user datahttps://newsroom.fb.com/news/2018/03/forensic-audits-cambridge-analytica/...",
    "published": true,
    "publishedDate": "2018-03-20",
    "category": "technical",
    "readTime": 2,
    "tags": [],
    "body": "Should Facebook really be responsible for [policing the misuse of user data](https://newsroom.fb.com/news/2018/03/forensic-audits-cambridge-analytica/) by companies that acquire it through legitimate means [then subsequently resell it to other entities](https://www.nytimes.com/2018/03/17/us/politics/cambridge-analytica-trump-campaign.html), for whatever purpose?\n\nFacebook is a broker that can directly control only how users and companies exchange data using its tools. Expecting the company to know – or even try to know – and respond to what happens to that data once data has been exchanged with its tools and taken off-platform is an impossible expectation for it to fulfill. We’re setting ourselves up as a society for continual disappointment and disillusionment having expected too much from a single organization.\n\nWouldn’t it be better to educate the general public about the risk of data reuse and resale should they opt to hand it over to whatever game or app developer through Facebook? Individuals could then decide if they want to assume that risk.\n\nFacebook already gives individuals clear and granular controls on what to share. And journalism is currently raising awareness of what could possibly go wrong should people give away their data thoughtlessly. The mature response to these learnings isn’t to blame Facebook and delete your account; it’s to realize that Facebook has given you great power over your personal data and while many may have shared it unwisely, that doesn’t mean you have to as well.\n\nFacebook and other monolithic networks aren’t social problems because they can’t control what goes on in or around them but rather because we expect and demand that they do.\n\nWe risk characterizing them as omnipotent parental figures that we should benevolently guide the multiplicity and complexity of our interactions while somehow resolving disputes with a fair hand.\n\nBut these aren’t governmental bodies and we’d be unwise to push them into that role by demanding they police data and behavior on their own or as agents of actual governments. They are international, capital-seeking enterprises, not representative bodies established, managed and adapted by referenda and all the protections of republican democracy. It’s hard enough to run a government responsive to the social needs of a specific population in an ever-globalized world. It’s madness to expect a company with nine board members and a user base of billions that spans the entire Earth to draw lines of social acceptability and benevolence let alone try to enforce them.\n\nIf we want paternalistically to prevent companies from exchanging personal data by applying categorical regulation, we already have real governments, locally, nationally and internationally available to do that. They can pass laws and enforce them, no matter how Facebook decides to expand or restrict its APIs and no matter how informed or ignorant its users are.\n\nWant to prevent companies from reselling data without originator consent? Make it punishingly expensive to do so by weiding courts and class action lawsuits while taking care not to circumscribe the very concept of consent by infantilizing individuals and therefore assume consent itself is not pragmatically possible.\n\nDid a company resell your data after promising it wouldn’t? Sue them. Did they never make that promise? Sorry, you’re out of luck. Just as you wouldn’t tell a secret to someone you don’t trust, you shouldn’t give any app access to your sensitive data if you don’t get legally enforceable guarantees to privacy.\n\nThe biggest shame that could come from our crisis of faith in Facebook and other platform providers would be to pressure the company into a defensive posture about the free flow of data and communication in general, effectively becoming totalitarian about its policies for fear of market and government retribution that stems from our impatient imposition of moral and legal responsibility.\n\nUsers will suffer from being treated as increasingly untrusted to share their data freely and whatever the form, whether demographic information, photos, status updates or medical history. Platforms will limit functionality to produce and exchange that data both on-platform and off, and the utility of their software will drop just as we expect it to rise in correspondence with our inflated sense of the pace of technology.\n\nThis will result in a two-pronged, contorted revolt against both the company’s tyranny and its impotence, leading eventually to mass emigration despite network lock-in effects. If this results in the use of decentralized platforms, people will have no choice but to seek social remedy from actual governments – that’d be the best-case scenario of creation arising from destruction.\n\nIf the migration leads to subsequent adoption of other centralized platforms, we risk entering into a cycle of losing progressively more liberty over our digital lives in the name of social safety and due to our lack of faith in higher authorities. The very platforms that are so uniquely situated to give us that positive liberty will be quixotically tasked with limiting it and that faith will only further degrade.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "finding-flow-beyond-distraction",
    "title": "Finding flow beyond distraction",
    "excerpt": "One my highest priorities over the past several years has been to establish a more frequent state of flowhttps://en.wikipedia.org/wiki/Flowpsychology...",
    "published": true,
    "publishedDate": "2018-02-28",
    "category": "technical",
    "readTime": 8,
    "tags": [],
    "body": "One my highest priorities over the past several years has been to establish a more frequent state of [flow](https://en.wikipedia.org/wiki/Flow_(psychology)) in my life.\n\nFlow has become increasingly important to me as I've internalized the belief that the most durable peace and satisfaction derives from an active concentration in the present moment, whatever it may contain. The emphasis on this experiential value is in contrast with my past preoccupation with the pursuit of future achievements, which after thirty-odd years, have proven to be emotionally all too fleeting.\n\nWhile in principle this switch sounds easy enough, its practice requires a constant tending to several conditions, not the least of which is a thorough reduction of distraction. And given that distraction is near constant factor for many of us, the question is whether to reduce it and how.\n\nThere are two main ways I've found that I get distracted – **by external interruptions and by internal ones**.\n\nExternal interruptions are the most obvious in that they're usually seen or heard. However, internal ones are just as pernicious, even if they're often discarded as thoughts that can't or shouldn't be helped.\n\nThe ultimate objective to minimizing both types of distractions is finding myself in a state of mind where I can focus on only one thing at a time and with pleasure instead of struggle. That thing could be a conversation with a good friend, the process of designing a new application, or the writing of a blog post. It could even be just walking down the street of a busy city and enjoying one observation at a time, not passively but through an active engagement with my thoughts, senses and feelings.\n\nThe ways I seek to decrease *external distractions* mainly involve practices to break smartphone addiction and maintain a clean work environment:\n\n- **I turn off all phone notifications, entirely**. In 2018, almost all of us have adopted a crazy habit of allowing any action with any level of importance related to our digital lives interrupt us with a sudden vibration or ding.\n\n  This is simply madness in the name of connectivity. I have an iPhone but have disabled all notifications so that at no point in time will my phone make a sound or vibration and interrupt me. If I want to check what I've missed, I can always open it up and pull down the notification center, which serves more rather like an imposing mailbox.\n\n- **I stop using my phone completely upon arriving home at night and until finishing my morning routine the next day**. Home is a place to recuperate, and if I check my messages or the news there (especially if I'm tired from the day or groggy from a night's sleep), I'm basically inviting the external world to interfere with that recuperation.\n\n  When I arrive at home, I plug the phone into the charger in my laundry room and resist the urge to take it out until I'm heading out the door again after breakfast the following day. If I'm heading to exercise first-thing, I resist checking it even until after I'm done and truly in a good position to react to anything I might see pop up in my digital life.\n\n- **I set up my workstation as neutrally as possible**. I love being around people while I work as a certain level of ambient noise actually helps me concentrate and feel emotionally connected. But it's just as important that I can focus for long stretches of time without distraction, either from my coworking peers, my friends digitally, or the entropy that results from moving between tasks.\n \n  Physically that means situating myself somewhere where people won't interrupt my work sessions often. \n\n  Digitally that means closing all windows and tabs that could possibly provide an avenue to interruption, such as email or Facebook. It also means maintaining inbox zero across all messaging and email interfaces (Gmail, WhatsApp, Facebook, etc), cleaning all files off my desktop and even setting the desktop color and system interface to a neutral dark grey.\n\n  A simple app called [Divvy](http://mizage.com/divvy/) helps me maintain perfectly divided windows, reducing cognitive friction even further by keeping everything in sight with the right proportions.\n\nI've found the key to minimizing *internal distractions* lies in creating well-organized places to tuck away concerns for the moment, as well as structuring time to ignore competing ones without constant ambivalence:\n\n- **I use [Asana](https://asana.com) religiously to track anything I feel I \"ought\" to do**. Instead of carrying various points of obligation around in my head and struggling to remember them at the right time, I organize any personal or professional tasks in Asana and seek to assign most of them due dates, which correspond to when I'll actually address them. This lets me temporarily forget that they even exist, since in a way, they really don't exist until they're actionable.\n\n- **I apply a modified [Pomodoro technique](https://en.wikipedia.org/wiki/Pomodoro_Technique) with [Focuslist](http://focuslist.co/)**. It's often hard to give a specific task my full attention because I'm actively doubting whether I should actually be focused on another priority.\n\n  But I've learned that I can temporarily supress that doubt by setting up 55-minute work intervals wherein I decide upfront the single thing I want to accomplish and commit to focusing on just that until the timer goes off.\n\n  During a subsequent 10-minute break, I not only give myself permission to indulge in any form of distraction but even force myself to do so, creating a sort of rest and reward cycle for myself.\n\n- **I'm an organizational freak about my finances**. Money can be one of the main drivers of stress and distraction, both explicitly through worrying about how to make ends meet and implicitly through the fretting of office politics that arises from feeling beholden to any given employment option.\n\n  For me, having a comprehensive picture of – and plan for – my money reduces that stress even when savings are low. That means obsessing over the details of just how much money I have and how I expect it to change in the foreseeable future in the context of my upcoming needs.\n\n  I use an app called [Foreceipt](http://www.foreceipt.com/) to track every purchase I make manually and map them to expense categories that I want to budget, such as dining and discretionary purchases. At the end of the month, category totals enable me to review precisely just whether I've met or exceeded those budgets and adjust accordingly.\n\n  On a monthly basis, I update a many-tabbed spreadsheet to record the current state of my assets and track recurring changes to them due to income, expenses and savings. Specifically, I allot a percentage of all income to several types of savings accounts (e.g. 10% for \"travel savings\") to automate my financial cushion.\n\n  This gives me an immense amount of peace of mind about how to sustain myself financially and prevents financial worries from arising when I'm not explicitly sitting down to manage them.\n\n- **Everything else gets purged onto paper**. Sometimes none of the above helps me get worries out of my head since they're too abstract or confusing to address proactively, at least yet.\n\n  In such cases, I simply take out a pen and piece of paper to jot down a rough outline of what keeps robbing my attention. The notes can take any form and they're not primarily about deciding what to actually *do* about the thoughts. This simple therapy hinges mainly on the act of getting it all out my head in the first place.\n\n  But after I complete the brain dump, I go through my outline and decide which are thoughts that I want to address and which I simply want to let run their course without any action at all, which is a surprisingly effective way to minimize them when done decisively.\n\n  I meditate on those I do want to address with action until coming up with at least one task that'd concretely help if not resolve the worry completely. That task then, of course, goes into Asana above.\n\n  I find that this paper-based exercise almost always reduces and many times resolves the distraction caused by scattered thoughts by moving them either firmly into my locus of control or out entirely.\n\nApplying all of the above won't, of course, necessarily result in a state of flow. I find it's also contigent on a base level of rest and physical health and often aided by a comfortable yet stimulating relationship with the task or experience at hand.\n\nHowever, in a connected world where many considerations vie for my attention at any given moment, these practices have been invaluable in helping me find that flow and enjoy the autotelic experiences that result with greater regularity.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "alps-snowboarding-recommendations",
    "title": "Alps Snowboarding Recommendations",
    "excerpt": "Any recommendations on the best snowboarding destinations in the Alps with easy access from Barcelona and London? 🏂",
    "published": true,
    "publishedDate": "2018-02-09",
    "category": "essay",
    "readTime": 1,
    "tags": [],
    "body": "Any recommendations on the best snowboarding destinations in the Alps with easy access from Barcelona and London? 🏂",
    "showMetadata": 1,
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "tutor-catala",
    "title": "Tutor Catala",
    "excerpt": "Algú coneix a un tutor de català bo? Estic buscant un per a fer classes d'una hora i mitja a La Dreta per els matis de dimarts i dijous abans de la...",
    "published": true,
    "publishedDate": "2017-12-10",
    "category": "essay",
    "readTime": 1,
    "tags": [],
    "body": "Algú coneix a un tutor de català bo? Estic buscant un per a fer classes d'una hora i mitja a La Dreta per els matis de dimarts i dijous abans de la feina cada setmana. Tinc un nivell básic però vull parlar amb fluïdesa 🤓",
    "showMetadata": 1,
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "the-outline",
    "title": "The Outline",
    "excerpt": "This is the most impressive media website I've seen in a while, from a creative design and construction quality point of view:...",
    "published": true,
    "publishedDate": "2017-12-10",
    "category": "technical",
    "readTime": 1,
    "tags": [],
    "body": "This is the most impressive media website I've seen in a while, from a creative design and construction quality point of view: [https://theoutline.com](https://theoutline.com)",
    "showMetadata": 1,
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "foursquare-swarm-checkins-sync",
    "title": "Syncing Foursquare / Swarm checkins to my website",
    "excerpt": "Neotoma now backs up all of my Foursquare / Swarm check-ins to my Dropbox account whereupon they're republished instantly to my website on a new check-ins page.",
    "published": true,
    "publishedDate": "2017-11-13",
    "category": "technical",
    "readTime": 2,
    "tags": [],
    "body": "[Neotoma](https://neotoma.io) ([on GitHub](https://github.com/neotoma)) now backs up all of my [Foursquare](http://foursquare.com) / [Swarm](https://www.swarmapp.com) check-ins to my Dropbox account whereupon they’re republished instantly to my website on {{#link-to 'checkins'}}a new check-ins page{{/link-to}}.\n\nThis setup relies on {{#link-to 'post' $post-dropbox-website-publishing.id}}the same publishing technique{{/link-to}} as my other website content as well as [recent changes to the Neotoma sync software](https://github.com/neotoma/sync-server/commit/fc5a2a2412ad405f5e1c670f1f6963c4300fe527#diff-6365ffb16fdc3a539e4cda9e40ab2a1cR825).\n\nThat software now transforms Foursquare check-ins copied initially in a proprietary JSON format from its API (e.g. [one from this past weekend](https://gist.github.com/markmhx/52d49a3ed4328c6141271b2640a25eea)) into a cleaner [JSON API](http://jsonapi.org/) format before saving them to Dropbox so [my website software](https://github.com/neotoma/personal-server) can easily understand and serve them as content:\n\n```\n{\n  data: {\n    id: \"foursquare-5a0719c1d4cc9849790606eb\",\n    type: \"check-ins\",\n    attributes: {\n      place-state: \"Catalonia\",\n      place-postal: \"43840\",\n      place-name: \"Corcega\",\n      place-longitude: 1.1382177519242145,\n      place-latitude: 41.076488193500616,\n      place-country-code: \"ES\",\n      place-country: \"Spain\",\n      place-city: \"Salou\",\n      place-category: \"Spanish Restaurant\",\n      place-address: \"C. Major, 31\",\n      photo-url: \"https://igx.4sqi.net/img/general/original/11437_gvUS2Nmh9bAaE7O2PP98sz5TZTzTbzr-wQlEShLGkmU.jpg\",\n      likes-count: 1,\n      foursquare-venue-id: \"4d0bce3d46bab60c9cc82990\",\n      description: \"Primera calçotada de l’any!\",\n      created-at: \"2017-11-11T15:39:45.000Z\"\n    }\n  }\n}\n```\n\nThe exact format I use here is definitely going to evolve as I build out functionality around this data on my website. For example, I plan to break out the places embedded in check-ins into their own files so I can rank the places I visit by frequency. But for now this format provides a quick and simple way to get the check-ins displayed reverse chronologically.\n\nAlso, Neotoma currently conducts a full historic backup of my check-ins when I connect my Dropbox and Foursquare accounts to it, but it doesn't keep watching for new check-ins.\n\nI'll be improving the system shortly to sync all new / future check-ins automatically so they appear on my website as well, both on the above check-ins page and on the homepage where I show my latest check-in up top.",
    "showMetadata": 1,
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "sync-server-dropbox-api-version-2-fix",
    "title": "Dropbox API v2 fix for Neotoma sync server",
    "excerpt": "As part of my working sprint at IndieWebCamp Berlin today, I managed to fix a show-stopping bug that's been in production for sync-server on Neotoma and run a backup job for my latest Foursquare / Swarm check-ins.",
    "published": true,
    "publishedDate": "2017-11-05",
    "category": "technical",
    "readTime": 2,
    "tags": [],
    "body": "As part of my working sprint at [IndieWebCamp Berlin](https://indieweb.org/2017/Berlin) today, I managed to fix a show-stopping bug that’s been in production for [sync-server](https://github.com/neotoma/sync-server) on [Neotoma.io](https://neotoma.io) apparently since September 28, 2017 when [Dropbox fully retired its API v1](https://blogs.dropbox.com/developers/2017/09/api-v1-shutdown-details/) in favor of API v2.\n\nI wasn’t aware of this bug until this week since error handling in production hasn’t been set to notify me (via email or otherwise), but setting up that notification is now [a prioritized task](https://github.com/neotoma/sync-server/issues/87) to avoid silent problems like this one in the future.\n\nAfter digging through the code, it turned out that the [Passport](http://www.passportjs.org/) implementation for Dropbox specifically was not passing an `apiVersion` parameter upon initialization of [its strategy](https://github.com/florianheinemann/passport-dropbox-oauth2), and as such, it was defaulting to Dropbox’s API v1 without my realization.\n\nI’ve added `apiVersion` [as a parameter here](https://github.com/neotoma/sync-server/commit/d9b1f15400201ef962a8dea79a121ad9d996c686#diff-25ac49459f3ccaa62fa691b8b449625cR69) and also as an attribute on the [storage model](https://github.com/neotoma/sync-server/commit/d9b1f15400201ef962a8dea79a121ad9d996c686#diff-430f49ef85b837131a35d1dd553659aeR23), specifically setting it to “2” for Dropbox’s storage document.\n\n*Note: This attribute apparently needs to be a string, not an integer, the latter of which failed to work for me when attempted.*\n\n```\nreq.strategy = new passportStrategy.Strategy({\n  apiVersion: document.apiVersion,\n  clientID: document.clientId,\n  clientSecret: document.clientSecret,\n  consumerKey: document.clientId,\n  consumerSecret: document.clientSecret,\n  callbackURL: `${req.protocol}://${req.get('host')}${path.resolve('/', Model.modelType(), document.slug, 'auth-callback')}`,\n  passReqToCallback: true,\n  profileFields: ['id', 'displayName', 'emails']\n}...\n```\n\nAs a result, Dropbox authentication now works again and I’ve been able to run a backup job for my Foursquare / Swarm check-ins, syncing the most recent ones to my Dropbox since last running backup earlier in the summer.",
    "showMetadata": 1,
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "dropbox-website-publishing",
    "title": "Publishing to my website instantly with Dropbox",
    "excerpt": "My website's content is now populated automatically via Dropbox using Neotoma personal server and publishing software, reducing the friction to publishing, keeping data in sync, and paving the way for content aggregation.",
    "published": true,
    "publishedDate": "2017-11-03",
    "category": "technical",
    "readTime": 6,
    "tags": [],
    "body": "I like files.\n\nWhen I was a kid just learning how to use the computer, I would organize all my files into folders by type: my [Kid Pix](https://en.wikipedia.org/wiki/Kid_Pix) drawings into one, my [SimCity](https://en.wikipedia.org/wiki/SimCity_(1989_video_game)) cities into another, etc.\n\nI still have distinct memories of thinking through the best ways to organize my files so that everything important to me was tucked away nicely and easy to find whenever I wanted it. This may have been an early sign of being a clean freak (or put more nicely, a minimalist) but file organization helped me maintain a peace of mind in an otherwise disorienting and scattered world of computing. It provided me a sanctuary of sorts within a broader realm of adventure.\n\nI remember [when Dropbox came out in 2009](https://techcrunch.com/2008/03/11/dropbox-the-online-storage-solution-weve-been-waiting-for/) that I realized immediately how this digital sanctuary could be extended to the cloud by turning one's strictly “local” collection of files into one that synced to \"remote\" storage on the internet, accessible wherever one went and with whatever device one may have on hand. The power of Dropbox was that I could simply drag all of my organized files into it and they would be instantly transformed from an isolated digital homebase to an omnipresent one, linked and yet independent from the physical reality of any one personal device.\n\nMeanwhile, however, I've built a career on developing websites and applications that are backed by all sorts of *databases*, not the so-called *flat files* I have come to love in my personal life – those that any normal human could open and read without special software. Databases have all sorts of advantages in making it easy to query and relate data, but fundamentally, they fall short in terms of transparency, transportation and transformation when compared to files. It's simply easier to see content, move it somewhere else, or change it when that content is stored regular files as opposed to databases.\n\nThe content we each produce as individuals in increasingly important to the world in which we live, and the nature of the Internet in particular. Each of us is a small publishing house and content producer, and every year we see the growing power of that role we play in the public realm. But we're terrible content *managers* and *crafters*, with either our public or private data. \n\nWe tend to throw our content out there to whatever distribution point makes it easiest to get it in front of other people, but that content ends up as rows in databases we hardly control and displayed on websites we hardly design. Even if you host your own blog on your own domain, chances are the content is stored in a database and on server for which you can't quite remember the password. Sure, it's out there *somewhere*, but it'd be a lot more powerful to put it all at the immediate control of your finger tips, just as you have control over the files on your laptop computer using [a wide range of learned techniques](https://www.youtube.com/watch?v=YtdWHFwmd2o).\n\nAbout a year ago I started to attempt a convergence of my two digital worlds – my sanctuary of private files and my slew of poorly managed public (or semi-public) content online – by releasing a new version of [my website](http://markmhendrickson.com), powered by some custom open-source software I created under the broader tent of [Neotoma](http://github.com/neotoma). The website is powered entirely by flat files that I edit directly on my MacBook, iMac or iPhone. These files are loaded by the software's [server](http://github.com/neotoma/personal-server) piece so that the [website](http://github.com/neotoma/personal-web) piece can load the data it needs to show without involving any sort of database. For example, if you ask the server directly for the content that makes up [this post](http://api.markmhendrickson.com/posts/), you'll see it provides exactly the same content stored as a file on my computer:\n\n![]()\n\nUntil this week, however, any of these files saved to my Dropbox that I wanted to publish to my website had to be copied manually to my website's server using [a script](https://github.com/neotoma/personal-server/blob/fefbdd6eb565958cafb79f94a973a3f6e9438d13/Gruntfile.js#L46), making the convergence I was looking for hardly seamless. My goal has been to hit the save button on my computer's text editor and immediately have any changes go live on my website, without any other extra step required. That way, there's never a question of my content being out of sync between my private and published worlds. I simply have to make the choice as to what content I want private versus public, and that decision is made simply by the organization of files into different folders on my computer. The rest is magic.\n\nI met that goal this week after noticing that Dropbox has [a handy Linux version](https://www.dropbox.com/install-linux) of their sync application that I could install on my website's server and configure to [selectively sync](https://www.dropbox.com/help/desktop-web/linux-commands) just the folders I want to make public. After setting the app up, I now just need to save any file into Dropbox that I want published and wait momentarily for my computer's Dropbox sync application to upload the changes to Dropbox then download them to my server where they'll be published instantly.\n\nI'm hardly the first person to think of this as a technique for personal website publishing. However, I hope to push the principle a lot further by automatically copying my content from a variety of database-driven, corporate services (such as Facebook, Twitter, and Foursquare) to my Dropbox in addition to my independently created content.\n\nThat copied content will then get automatically synced not only to my “local” devices (e.g. laptop) for backup as files but also to my website for republishing any way I choose. Aggregated republishing onto one's own website is one of the main use cases for the Neotoma sync service, and I've decided recently that I want to focus on developing the sync service primarily for this use case, dogfooding it on my own website and setting the system up for friends who want to self-publish in the same way as well.\n\nCopying data from a wide range of services into this system will take time and lots of work, so I'm looking to release support for them iteratively, starting with Foursquare since it's an app I use intensively. I'm currently focused on setting up Neotoma to sync my check-ins from Foursquare to my Dropbox and subsequently to my website where I can list them in reverse chronological order, creating a timeline of my life (with photos and other content related to my daily travels).\n\nThen I plan to sync my tips content from Foursquare as well to create custom-designed city guides on my website. These guides will automatically incorporate both the tips and check-ins data I generate on an ongoing basis to display the most up-to-date information about my favorite places in a way that's beautifully packaged, easy to use, and won't go stale.\n\nIf you're interested in setting up your personal website in a similar way, I'd be happy to chat about your needs and how my efforts here may help you as well. And if you're a developer interested in helping with any of the software involved, let's talk about Neotoma and where you could make an impact.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "park-ranger-grunt-hoist-proxy",
    "title": "New Node.js repos for environment management, deployment and proxying",
    "excerpt": "While working more or less full-time on Neotoma since last summer, I've created a few repositories that are hopefully useful as modules to other Node.js apps in general.",
    "published": true,
    "publishedDate": "2017-04-04",
    "category": "technical",
    "readTime": 3,
    "tags": [],
    "body": "As I’ve been working more or less full-time on [Neotoma](https://neotoma.io) (formerly Asheville) since last summer, I’ve had the opportunity to cut my teeth truly on open-source software for the first time.\n\nI’m happy to say that in addition to working on publicly available repositories specific to Neotoma, I’ve created a few repositories that are hopefully useful as modules to other Node.js apps in general:\n\n- [Park Ranger](https://github.com/markmhx/park-ranger): A manager for environment-specific dependencies such as environment variables, configuration files and SSL certificate files.\n\n  It’s called “park ranger” because a computer program always runs in a given environment, often determined by its device or a specific environment chosen within that device alongside other possible environments. And whom do you seek out when you’re enjoying the natural environment and have questions about it…? That’s right, a park ranger.\n\n  I basically kept rewriting the same utility code across my repositories to handle environment-based differences, mainly between my local development machine and deployment host. So, I refactored it all into this module to speed up code improvements and maintenance going forward. My starting point was [dotenv](https://github.com/motdotla/dotenv) but I quickly realized it was too simple for my needs.\n\n- [Hoist](https://github.com/markmhx/grunt-hoist): A suite of Grunt tasks to deploy Node.js apps to hosts and execute related remote procedures.\n\n  Similar to my experience with Park Ranger, I found myself rewriting slight variations of the same deployment routines across repositories (such as rsync'ing files, running \"npm install\" and restarting the remote server). So I created this set of tasks (which automatically make themselves available to parent projects as npm scripts) to standardize the way in which I approach this. They also greatly simplify my approach to continual development, even as I spin up new micro-services or make quick changes to local dependencies along the way.\n\n- [Proxy](https://github.com/neotoma/proxy): A proxy server for HTTP and HTTPS requests.\n\n  As I began hosting early versions of Neotoma for closed testing, I needed a simple way to support different servers running on the same host across protocols (HTTP vs. HTTPS), ports and subdomains. For example, the same host that uses HTTP to serve the landing page for Neotoma also serves HTTPS requests to its underlying API and both HTTP and HTTPS requests to the actual Neotoma web app running on a subdomain for testing.\n\n  While this repository lives under the Neotoma organization, it can be used by anyone who wants to do the same for their hosts that run multiple servers.\n\nThere are a number of other public repositories under development more directly related to Neotoma that I won’t list here but can be found on the [Neotoma GitHub organization](https://github.com/neotoma). While I haven’t actively sought contributions yet, all of the repositories listed above and in that organization are open to pull requests should you want to make any changes!",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "touch-bar-autosuggest",
    "title": "Touch Bar Autosuggest",
    "excerpt": "\"Hmmm what I've been doing is that I wanted to be a good friend to my life and my life is so far away\" - My Touch Bar autosuggest",
    "published": true,
    "publishedDate": "2017-03-02",
    "category": "essay",
    "readTime": 1,
    "tags": [],
    "body": "“Hmmm what I’ve been doing is that I wanted to be a good friend to my life and my life is so far away” - My Touch Bar autosuggest",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "spanish-non-lucrative-residence-visa",
    "title": "Applying for the Spanish non-lucrative residence visa",
    "excerpt": "Just over a year ago, I moved to Barcelona in Spain to live abroad for the first time. I booked a flight from San Francisco with little else in terms of preparation, an intentionally rash decision to experience what it would be like to land in a new country and improvise the establishment of a life there.",
    "published": true,
    "publishedDate": "2014-10-26",
    "category": "article",
    "readTime": 29,
    "tags": [],
    "body": "Just over a year ago, I moved to Barcelona in Spain to live abroad for the first time.\n\nI booked a flight from San Francisco with little else in terms of preparation, an intentionally rash decision to experience what it would be like to land in a new country and improvise the establishment of a life there. I brought just enough possessions to stuff a carry-on bag and backpack, with my American passport serving as my only legal documentation.\n\nThis was possible since American citizens are permitted three months out of every six to stay in the vast majority of Europe (anywhere covered by the [Schengen Area](http://en.wikipedia.org/wiki/Schengen_Area)) without needing a formal visa. They just stamp your passport when you arrive and (maybe) check it when you leave to see whether you broke that rule.\n\nJust a couple months into living in Barcelona, however, I realized that I wanted to live there for much longer and without frequent interruption. Discussing my options with other American expats, I learned that many opt to simply overstay the implicit three-month tourist visa and hope they don’t get caught, specifically when leaving Europe through customs.\n\nI ended up testing that method by overstaying a couple of months, pushing my initial stay to five. But when I flew out of Barcelona that first time, the customs officer stared at my passport for an anxiety-inducing long amount of time then lightly grilled me for having broken the rule. He let me off the hook without a penalty, but the specter of running into real trouble — perhaps a fine or ban, who knows — during future trips made me think more seriously about obtaining a proper longer-term visa.\n\n# The Options\n\nFast-forward to June during my second stay in Spain on the tourist visa when I finally got serious about doing some research. I started off by visiting an immigration lawyer who had been referred by a group of friends, several of whom consulted with him to manage their own immigration issues. I visited him for an introductory consulting session wherein he laid out a few visa options given my situation:\n\n- **Student visa:** If I were willing and interested in taking Spanish classes at a formal school (as opposed to self-teaching and working with a personal tutor, as I’d been doing), then I could apply for a student visa. The downside would be that I had to pay for and attend classes, somehow scheduling them alongside my other activities. Or, I could simply sign up and pay but not go, but that would mean burn money and risk getting caught, pretty much defeating the purpose.\n\n- **Work visa:** I could find employment with a Spanish company and they could sponsor me to live in Spain. However, I had little interest in working for a local company let alone having me ability to reside in Spain rely on one.\n\n- **Entrepreneur visa:** Apparently, Spanish law had been recently changed to permit entrepreneurs to obtain visas if they could whip up business plans and pitch decks that persuaded the government they would actually be profitable and employing Spaniards in short order. It seemed like a lot of work and, considering my predilection for speculative internet ventures, an exercise in bullshitting my way to a visa.\n\n- **Non-lucrative visa:** I had heard rumors of this type of visa, one in which rich or retired people could basically buy their way into Spain by showing they were financially very well off. But before visiting with the lawyer, my impression was that the minimally required amount would be prohibitively high given my finances. Luckily, he informed me that the bar was actually set at a level the government had determined would simply support a person for a year, to provide assurance that the visa holder would not end up taking a local job or weighing down the welfare system.\n\nThis last option jumped out at me as my path forward. Luckily, due to some decent saving over the previous few years, I had enough savings in the bank to cover the requirement of €25,560 (or about $32,660 at time of writing). And that was really the main requirement. As long as I wasn’t carrying any serious diseases or criminal history with me as baggage, I was looking at fairly smooth sailing.\n\nUnfortunately, I was about to get schooled by the world of government bureaucracy, learning just how frustrating it can be to piece together the procedural details of a visa application with minimal guidance from the Spanish government or anyone else for that matter (my lawyer turned out to be not-so-good at helping me get anything done so I struck out on my own). As such, I’ve learned through personal experience what it’s like to apply for this kind of visa, and I’d like to spare others some of the pain and mystery by relating that experience in detail.\n\n**A big caveat emptor**: I’m not a lawyer so none of this should be taken as authoritative legal advice. I also applied for the visa through the Spanish consulate in San Francisco during the fall of 2014. What follows comes from my single experience, so while it’ll hopefully guide you towards a similar outcome, it’s highly advised that you cross-check any information here with the most official sources you can find. As far as you or I know, the lessons I’ve drawn could be unique to my application, or they may have been dependent on changing protocol, now outdated by the time you read this.\n\n# The Application\n\nThe non-lucrative residence visa application is a [concise document](http://www.exteriores.gob.es/Consulados/SANFRANCISCO/en/ConsularServices/Documents/visas/NonLucrative.pdf), spanning a mere two pages. But therein lie a great number of details and potential pitfalls. Let’s start from the top and work our way down.\n\n> \"This visa allows you to reside in Spain without engaging in any type of lucrative activities.\"\n\nThis basically means you can live in Spain and do whatever you want as long as you don’t work for a Spanish company. \n\n> \"The process may take between 2 to 4 months from the day all documents are presented.\"\n\nThis was the part that gave me the most anxiety. The lawyer in Barcelona assured me it would take just a matter of days to apply and get approved. Others warned it may take a few weeks, still giving me the chance to swing through San Francisco for less than a month and return to Barcelona without missing a beat. But when I read _4 months_, my heart sunk and I spent weeks readjusting my expectations.\n\nThe time between submitting the application and receiving an approval notice ended up lasting only 19 days (just under three weeks). So, while the formal estimate is 2-4 months, that clearly wasn’t accurate guidance in my case and only served to stress me out. I should note, however, that the consulate official told me in person that it would probably take two months, with the quickest in recent memory having gotten processed in three weeks. Perhaps I got lucky or managed to prepare better than the average applicant. Either way, I’m glad it was a pleasant surprise.\n\nThe official also told me that the clock wouldn’t start ticking on my application process until I had returned to the consulate to deliver a final, missing document (the notarized criminal record clearance, which hadn’t been mailed back to me yet). This echoed the official guidance here in the application’s instructions. However, she ended up starting the process without that document, saving me about two weeks of waiting, since I think she was happy with how thoroughly everything else had been prepared. So, it pays to put your best foot forward even if can’t manage to bring everything ready in time for your application appointment.\n\n> \"Once your visa is authorized, we will contact you by email or mail, and you (and all your family members applying for a visa) will have to come in person to this Consulate General within a month with your passport and an itinerary of flight to Spain to obtain the visa.\"\n\nThis is pretty straightforward. I ended up receiving notice of my application approval by email. It stated that I had a month to come back to the consulate in person to pick it up, and that I needed to both book a flight to Spain before pickup. Additionally, because I had indicated I wanted to leave for Spain ASAP, I was asked to notify the official by email a few days before my visa pickup with the flight’s date. Presumably that information would be used to set an official start date for the visa’s one-year applicability.\n\n> \"For information about how to obtain the Apostille of the Hague Convention in the US, please visit: [http://travel.state.gov/law/judicial/judicial_2545.html](http://travel.state.gov/law/judicial/judicial_2545.html)\"\n\nOk, so this was a bit of a head-scratcher for me. \"What the hell is an ‘Apostille of the Hague Convention’ and is it going to be a pain to get it?\" were my first thoughts. The answers turned out to be both simpler and more complicated than expected.\n\n\"Apostille\" is French for…notarization or something? I don’t even know how to pronounce it, but basically it’s an internationally recognized form of notarization. You have to get it from the government, and you have to do so either by mailing it to them or dropping it off in person, then waiting weeks for return processing. In California, that means mailing it to the California Secretary of State or driving to Sacramento, the latter of which may save you a few days but ultimately, it’s a waiting game regardless.\n\nHere’s the catch: an Apostille (which comes in the form of a cover letter with a stamp that gets imprinted half on the original document and half on the letter) is required only for the criminal record clearance. And this clearance, as I detail below, must be obtained from the government as well. However (and this is important!), even though they’re both obtained from the government, you have to do so by processing paperwork with two separate government departments via mail. \n\nOnce you get the criminal record clearance back from one department, you have to mail it out to a different department, leading to more waiting. The department that produces the clearance can’t provide an Apostille for it as well, nor can that department mail the clearance directly to the department that _can_ provide an Apostille for it. Bureaucracy at its finest.\n\nI found [these exact instructions](http://www.sos.ca.gov/business/notary/authentication.htm) for obtaining an Apostille in California. They entail mailing the original clearance document, a basic cover letter, a check to pay $20 in processing fees, and a self-addressed envelope the department can use to send the notarized version back. It took a total of just over two weeks from when I mailed out the original document to when I received it back, notarized by the Secretary of State.\n\n> \"The Consulate of Spain in San Francisco will consider applications for Visas BY APPOINTMENT ONLY. One appointment per person. To schedule an appointment visit our website.\"\n\nThe best official information about the various visa options available, plus a direct link to making an appointment online, can be found [here](http://www.vfsglobal.com/Spain/usa/SanFrancisco/allaboutyourvisas.html).\n\nBook your appointment early. I found that the earliest available slot was three weeks out, so this isn’t something you can plan to do last minute. I also had to reschedule my appointment (something they say you can do only once) because I wasn’t ready  yet, and the earliest reschedule time was also a few weeks out. So while the online appointment-making process is straightforward enough (and free), do it early and reschedule quickly should the appointment date start creeping up and it becomes at all apparent that you won’t be prepared in time.\n\n> \"The consular administration has full authority to evaluate, and request more documents than those submitted by the applicant. The latter is hereby informed that submitting the aforementioned documents does NOT guarantee automatic issuance of the visa. The documents accepted in order to process the visa, will not be returned.\"\n\nIn my experience, the consulate never requested any documents not listed in the application, so I presume they put this caveat in here just to give a heads up that they might do so if things get complicated.\n\n> \"ORIGINAL and ONE PHOTOCOPY of each of the following items must be presented\"\n\nFor paranoia sake, I actually brought _three_ photocopies of every item to the appointment. And I believe that a second photocopy came in handy for at least one or two of them (I can’t remember which). So, spend the additional ink and paper to print out these extra copies and take them with you in an envelope of backups just in case. I brought four envelopes in total: one with the originals, one with the required copies, and one with the double sets of backup copies.\n\n# The Requirements\n\n> \"National visa application form: The application forms must be signed and filled out in print.\"\n\nThis form is fairly straightforward. The typical catch for most people is probably how it asks for a \"Postal address of applicant in Spain\". If you haven’t spent time in Spain and arranged an apartment or some other accommodation already, then this seems like a catch 22. I’ve read elsewhere that you can probably get away with just listing the city to which you intend to move. Fortunately, I had already arranged an apartment in Spain by the time I applied so I listed its address here.\n\nThe form also asks for how many entries are requested (one, two or more than two). My presumption with this visa was that it permits unlimited entries, so I was confused why it would be asking. I marked \"more than two\" and the topic didn’t come up at all during my appointment. I’ll have to see whether this becomes relevant once I start traveling to and from Spain with the visa.\n\nIt also asks for a \"Date of intended entry into Spain\". Given that the consulate encourages (rightly) that you shouldn’t book your flight to Spain before the visa is approved, and that approval takes an unpredictable amount of time, this is also a bit tricky. I simply put a date that was a month out from my scheduled appointment plus the phrase \"or once issued\". The official asked me about this during my appointment and I just emphasized that I wanted to leave for Spain ASAP. This may have helped get my application processed faster as well because they knew I wanted to go immediately.\n\nA note on signatures, for this form and all other documents: the official made a point during my appointment that they had to be hand-written. I have the habit of signing documents electronically with a digitized version of my signature. But she required that I sign each document again in person since this wasn’t adequate for the consulate’s legal purposes. I’d recommend simply signing all originals by hand before making copies to avoid this hassle during the appointment.\n\n> \"Form EX-01: Form must be signed and filled out in print. Only available in Spanish\"\n\nThis form is also easy enough (if you can read Spanish or use a translation dictionary). There are a few checkboxes that basically give legal consent for various things, and you’ll want to simply check these off. The biggest point of confusion was whether I needed to check off any box beneath the \"INICIAL\" section of \"4) TIPO DE AUTORIZACIÓN SOLICITADA\". I checked the first primary box for \"INICIAL\" but left the others below it blank, which the appointment official said was fine.\n\n> \"Original Passport: Valid passport for a minimum of 1 year, with at least one blank page to affix the visa.\"\n\nIf you already have a passport, just bring it with copies (like for everything else). But be sure to check the \"minimum of 1 year\" requirement. I realized that my passport was actually set to expire within a year, so I had to go through the weeks-long process of mailing it in to get a new one. You can do this pretty easily from your local post office. Some even have staff specifically trained for passports, which is useful. But it will add more waiting time that you need to budget properly ahead of your scheduled visa appointment.\n\n> \"Two passport size photos: (White Background, 2x2in) One per application form.\"\n\nI got my photo taken and had these printed easily at a local FedEx Office branch. However, I printed a total of eight since I knew I would need two to apply for my NIE (Número de Identidad de Extranjero) in Spain, which is essentially basic government registration that enables a number of things such as opening a bank account.\n\nI paper-clipped these to the upper-left corner of the two forms above plus their copies, using a total of four photos. During the appointment, though, the official removed the photos from the Form EX-01 and its copy, so I ended up submitting only two (for the national visa application form and its copy, to which she stapled the photos).\n\n> \"Notarized document explaining why you are requesting this visa, the purpose, the place and length of your stay in Spain and any other reasons you need to explain, with a certified translation into Spanish.\"\n\nHere’s what I wrote for my statement of purpose letter:\n\n> September 1, 2014\n> \n> Consulate General of Spain \n> 1405 Sutter Street\n> San Francisco, CA 94109\n> \n> Dear Consulate General of Spain,\n> \n> I am requesting a non-lucrative residence visa to live in Spain.\n> \n> The purpose of my stay will be to learn the Spanish language and culture while I take time off of work to focus on my hobbies and explore the country. I plan to live in Barcelona for a year.\n> \n> I have included bank statements totaling [redacted] in savings, which should satisfy the 25,560€ (or $33,849.11 at time of writing) minimum income requirement for this type of visa. I have also included medical and criminal records that demonstrate how I have no contagious diseases or criminal history.\n> \n> Additionally, I have included the certificate of a Spanish health insurance plan that has no deductible and covers me in excess of 30,000€.\n> \n> Thank you for your consideration.\n> \n> Sincerely,\n> Mark Hendrickson\n\nI wrote the letter in English since a \"certified translation\" into Spanish was required, for this and other documents. Unfortunately, the official instructions are generally very silent on what that means. I emailed the consulate about it and they sent me back a separate document (not found online, as far as I can tell) both explaining the general translation requirements and listing specific local, pre-approved translators I would have to use.\n\nI called around to several of them, which are a mix of seemingly corporate translators and individuals who translate part-time. After having some awkward phone calls with a few (imagine: calling someone’s home phone number and having their spouse pick up first), I found a woman who was very responsive and available to quickly translate this document and the others listed for translation. I emailed her digitized copies of the documents and she translated them in a matter of days. Then she handed off the translated versions (stapled to printed copies of the originals plus her own cover letters explaining the translations) in person in exchange for $250 total.\n\nJust be sure that you go with a translator approved by the consulate since there are others providing translation services that may not be considered \"certified\" by the consulate even if they have other certifications and reassure you they’re applicable.\n\nAs far as notarization for this letter, I simply went to a local UPS Store with notary services and had them perform a simple notarization in person wherein I signed the letter in the presence of the notary and they produced a cover letter with relevant information and stamp.\n\n> \"**Proof of enough periodic income** (investments, annuities, sabbaticals and any other source of income) to live in Spain without working. The minimum income required is 25,560 Euros annually plus 6,390 Euros per each additional family member. All documentation must be certified translated into Spanish.\"\n\nThis requirement contains the crux of this type of visa: finances. Basically, you have to demonstrate that you’ll have enough dough to live in Spain independently.\n\nThe term \"periodic income\" is quite ambiguous here even though they list a few types of income sources. The general theme seems to be _reliable money_ in that the Spanish government doesn’t appear inclined to accept any type of income that may fluctuate, such as a paycheck (oh right, remember you’re not supposed to be working anyway?).\n\nLuckily, basic cash sitting in a bank account counts as income here. It’s a bit of an unwritten rule, but if you can simply print out bank statements showing that you have (and perhaps importantly, have consistently had) enough money in a savings account to meet the minimum income amount, you’ll satisfy this requirement. That’s what I did and they didn’t question any of the record details.\n\n> \"**Police Criminal Record clearance** must be verified by fingerprints. It cannot be older than 3 months from the application date with a certified translation into Spanish. The certificate must be issue from either:\na) State Department of Justice. Original clearance letter form signed (from the States where you have lived during the past 5 years). It must be legalized with the Apostille of the Hague Convention from the corresponding Secretary of the State.\nb) FBI Records, issued by the US Department of Justice – F.B.I. It must be legalized with the Apostille of the Hague Convention from the US Department of State in Washington DC.\nYou must also get a police record from the countries where you have lived during the past 5 years.\"\n\nThis requirement led me on a costly, time-consuming goose hunt. I went with option A using a service called Live Scan, which is apparently the only viable option for requesting and obtaining this clearance.\n\nThe in-person Live Scan service is relatively straightforward once you know where to go and which form to bring. There is a list of [locations in California](http://ag.ca.gov/fingerprints/publications/contact.php) that support Live Scan, namely UPS Stores. You go with the relevant form filled out, they electronically scan your fingerprints, and off your application goes. About two weeks later, it should show up in the mail as a simple letter saying (hopefully!) you don’t have any criminal record on file.\n\nNote that when counting the roughly two-weeks wait for obtaining an Apostille for this letter (as detailed above), the total time for obtaining this clearance with Apostille amounted to just over a month. So, as with everything here, get the ball rolling early.\n\nUnfortunately, I had to go through the Live Scan process twice having screwed it up the first time. It turns out there are at least two different types of Live Scan applications you can submit: one for regular \"record review\" (intended for individuals who want to personally review their record status) and [another specifically intended for visa applications and other immigration-related purposes](http://oag.ca.gov/fingerprints/visaimmigration). \n\nI failed to notice this latter option even existed when first applying for Live Scan, so while the resulting document was perfectly accurate, it wasn’t written in a valid format for my visa application. I heard this bad news over a week into waiting for Live Scan to process, which delayed my overall preparation time and increased my costs. Fortunately, the California Department of Justice was responsive to my inquiry emails; otherwise, I would have waited a week longer before uncovering the mistake.\n\n> \"**Medical Certificate** with a certified translation into Spanish, which should be a doctor’s recent statement (not older than 3 months in doctor’s or medical center’s letterhead) indicating that ‘the patient has been examined and found free of any contagious diseases according to the International Health Regulation 2005’. Must be signed by a M.D.\"\n\nThis turned out to be another, costly requirement due to confusion about how to go about obtaining such a doctor’s letter.\n\nEssentially, all you should have to do is schedule an appointment with a normal doctor, who will ask you some simple questions about your health and perform a routine checkup. You can then have have them write, print and sign a simple letter to satisfy this requirement.\n\nHowever, only after going through an entire checkup with a doctor at my regular health clinic was I told that she couldn’t write this sort of letter for me. It had something to do with how she wasn’t certified to write anything related to international law, meaning she could write the bulk of the needed statement but not the presumably crucial part about \"according to the International Health Regulation 2005\".\n\nI discovered this at the tail end of my appointment even though I had specifically told the clinic about my need to have this letter produced as a result of the appointment, and both the scheduling staff member and doctor had assured me beforehand that it would be possible. I ended up going back to the same clinic to have an entirely new checkup performed by a different doctor who _could_ write the full letter. But unfortunately this meant more time and (luckily, insurance-covered) money. So be sure to get very explicit and considerate recognition ahead of time by your doctor that they can produce what you need before placing yourself on the hook for medical costs by going through the checkup first.\n\n> \"Proof of having international medical insurance while in Spain, with a certified translation into Spanish.\"\n\nThis is one of the more ambiguous requirements. We all know what medical insurance looks like (right?) but \"international\"? What does that mean exactly?\n\nI emailed the consulate to find out and they responded simply that the insurance had to have zero deductible and coverage up to 30,000€. On those bases alone, my individual, private American insurance plan was inadequate. And I was still wary of getting a separate _American_ plan for fear that it wouldn’t cover me properly in Spain.\n\nFortunately, I have [a friend in Barcelona](http://www.premier-spain.com/) who has helped other expats obtain local health insurance for their own immigration needs. I contacted her and she sent me application forms for a standard (?) health insurance plan from the Spanish company ASISA. Within a few days, the application was fully processed and I had a new Spanish insurance plan, which appears to have no deductible or limit to how much they’ll cover (it beats me how that’s possible for the modest monthly price they charge, but hey, I’m American).\n\nThe downside here is that I’m now paying for two individual health plans – one for the US and one for Spain – but using the confirmation letter of my ASISA coverage (sent to me digitally and by default after my application was approved) worked without a hitch. I plan to keep both plans since if I get moderately sick in Spain, I want the option to get fixed up at a local hospital. And if I get majorly sick, I want the option to come back to the US for more intensive treatment.\n\n> \"Authorization form M790 C052, plus the Authorization fee. Please visit our website to check the most recent visa fees. We only accept cash (exact change) or money order. No personal check, no credit cards. The processing fee will not be returned even if the visa is not granted or cancelled.\"\n\nThis form handily comes with precise instructions, in English no less, about how to fill it out. So, there’s not much to worry about here.\n\nWhat did catch me a bit off guard was the expected application fees to be paid during my appointment. I brought $140 in cash per [the outline of fees](http://www.exteriores.gob.es/Consulados/SANFRANCISCO/es/ServiciosConsulares/Documents/Tasas/Tasas.pdf) provided by the consulate, but I ended up paying $14 more since I hadn't noticed that second fee. This was okay since I brought extra cash, which you should as well. The consulate official luckily had change but I’d recommend bringing a handful of ones, fives and twenties just to ensure you can produce whatever extra, exact cash payment may arise.\n\n# The Preparation\n\nIn total, I spent about 4-5 months preparing all of the above items. It’s hard to calculate how many hours were involved exactly but it was a bunch spread out over sprints of a few hours here and there.\n\nMy recommendation would be to start a full half year before you intend to apply, just so you don’t have to panic as I did at various moments when I realized that certain parts would take longer or involve more details than expected. \n\nDecide when you’d ideally like to move to Spain and then work backwards from there. If it takes up to six months to prepare and possibly four months to get approved, that’s nearly a year (10 months) from start to finish. Granted, you could get approved quickly like I did, cutting the processing time to under a month and the total time to something more like five months. But unless you’re already in a time crunch, it’s best to avoid the stress of constantly praying for the best-case scenario.\n\nA big aide in my preparation was a spreadsheet in which I laid out all of the requirements in rows and tracked their status in columns.\n\nThe rows included:\n\n- National visa application form\n- Form EX-01\n- Passport\n- Two passport-sized photos\n- Statement of purpose\n- Income proof\n- Criminal record clearance\n- Medical certificate\n- Proof of medical insurance\n- Authorization form M790 C052\n- Authorization fee of $140 in cash\n- Appointment confirmation\n\nAnd the columns included:\n\n- Obtained?\n- Completed?\n- Translated?\n- Notarized?\n- Physical Original Ready?\n- 3 Copies Printed?\n- Packaged?\n- Ready?\n- Notes\n\nNot every item requires every column. Notarization, for example, only affects two (the statement of purpose and criminal record clearance). But most require the majority and it’s hard to track the exact status of each item unless broken down into this level of spreadsheeted detail.\n\nAlso to organize everything, I made a practice of digitizing every document regardless of whether it was completed yet. That empowered me to reference the various documents on the fly as I worked my way between them, without having to dig up physical records based on my location.\n\n# The Appointment\n\nMy appointment was scheduled for the start of the consulate’s work day (10am), so when I arrived, it was pretty quiet. The San Francisco consulate is a modestly sized building, one that feels as though it was probably converted from a residence at one time. So there was no confusion as to where I had to check in, wait or go to meet with my assigned review official.\n\nThe staff also spoke perfect English and was generally friendly, at least based on what you’d expect from a government office. So it never gave me the vibe that I was being interrogated by a faceless organization, which reassured me I’d be okay should I need to inquire about anything while waiting for my application to process.\n\nMy official accommodated the fact I’d showed up for my appointment missing one document (the criminal record clearance) since it hadn’t been mailed back to me with an Apostille by the Secretary of State yet. She told me to simply swing by the consulate to drop it off without an appointment once it was in my possession. I don’t know how many of these missing items she would have tolerated to accept my initial application, but I got the impression that this sort of piecemeal application is not unexpected.\n\nThe in-person application experience was smooth enough. I waited maybe 15 minutes then went upstairs to sit down at a desk across from the official in her office. We spent maybe 20 minutes total going through the two packets of documents that I handed her: one with originals and one with copies. She lightly reviewed them and asked basic questions as she went. It was nothing amounting to an interview or interrogation of any kind. \n\nOnce she had stapled and collated a bunch of the documents, she told me to come back with the final document then simply wait for the application to process. I got the impression that she’d have to send the paperwork off to another department (one, I later found out, was based in Barcelona) as part of the process and, once she heard back from them, would relay the result to me.\n\n# The Costs\n\nOverall, in addition to the considerable amount of time involved, **I spent roughly $928 during the application process**:\n\n- $33 for eight passport-sized photos and printing at FedEx Office (two for application, two for anticipated [NIE application](http://en.wikipedia.org/wiki/NIE_Number) back in Spain, plus four spares)\n- $110 in password renewal fees\n- $6 to mail passport renewal application\n- $50 for first Live Scan application to receive a review of my criminal record from the CA Department of Justice\n- $50 for second Live Scan application (whoops)\n- $17 to overnight mail my criminal record and corresponding materials to the Secretary of State for an Apostille\n- $20 in fees for Apostille\n- $197 for doctor visit to obtain basic health clearance letter\n- $11 for statement of purpose notarization\n- $280 for certified Spanish translations ($250 plus tip)\n- $154 in Spanish consulate application fees\n\nOn top of that, I’m now paying about 46€ per month for Spanish health insurance.\n\n# The Approval\n\nThe consulate official both called and emailed me once my application was approved, notifying me that I needed to stop by the consulate once more within a month to drop off my passport, have them paste the visa inside of it, and return it to me within the same day. She also requested that I send her basic information about my flight back to Spain so she could match the visa start date with it.\n\nOnce the visa was obtained, I had 90 days to return to Spain and use it at the local police department to obtain a residency card, which would serve as the actual documentation permitting me to stay an entire year.\n\nAfter the first year, I should be able to renew the visa twice from Spain, first after a year and for two additional years, then two years later for an additional two years. Theoretically,  that means I should be able to use this visa for up to five years, extending it as desired from Spain.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "bringing-it-all-back-home",
    "title": "Bringing it all back home",
    "excerpt": "> \"Now, I been in jail when all my mail showed, that a man can't give his address out to bad company\" - Bob Dylan, Absolutely Sweet Marie Last summer...",
    "published": true,
    "publishedDate": "2014-01-30",
    "category": "technical",
    "readTime": 6,
    "tags": [],
    "body": "> \"Now, I been in jail when all my mail showed, that a man can’t give his address out to bad company\" - Bob Dylan, Absolutely Sweet Marie\n\nLast summer I drove across the USA from New York City to San Francisco, mostly by myself. I took my time, stopping in many places along the way over the course of a relaxed month. I didn't have a predefined itinerary; I just wanted to discover and react to what I saw.\n\nThe trip granted me stretches of time to both reflect and record, which accentuated an increasing tendency in both my life and our culture in general. One moment I was ruminating about Civil War history, the other I was cropping a photo of a Robert E. Lee commemorative plaque and writing some commentary to share online.\n\nThe recording was not only an instantaneous way to share a bit of my journey with friends, family and followers. More importantly in the long run, it was my way of creating a breadcrumb trail of moments that would later help me remember and make sense of them.\n\nInternet-connected software, aided by portable devices, has given us an unprecedented ability to create such live journals. But while enabling the online production of historical identities, the companies behind this software (e.g. Facebook, Twitter, Instagram, Foursquare, etc.) present two substantial shortcomings:\n\n1. They keep our own data primarily on their own computers.\n\n2. They provide, support and permit software for accessing our data only when it aligns with their business needs.\n\nI have noodled on this set of problems before, but an epiphany of sorts hit me while walking the streets of Asheville, North Carolina just about a week and a half into my trip.\n\nI had just finished posting a photo to Instagram, tweeting an observation on Twitter, and checking into a restaurant on Foursquare. All three of these actions constituted different ways to reflect and preserve the moment. But I was giving my impressions away to three separate companies that would keep the memories in  fragments on disparate servers over which I had no control. \n\nI had no practical way of bringing all these moments into a single place on which I could depend over the next five, ten, and fifty years. Nor could I access or create a cohesive representation of them, tailored to my liking. These cherished moments were to remain fragments of my identity, adrift in the proverbial cloud unless something changed.\n\nThe epiphany presented a core of a solution: **what if I had copies of all these moments on my Dropbox, alongside all of the other files I store there already?**\n\nDropbox (as well as other sync-based cloud storage solutions that have emerged in its wake such as Google Drive) is a beautiful tool because it bridges the local and network-based data realms. Any files you put into it will gain a dual property: stored both locally on your device, such as a laptop or desktop computer, and accessibly on the internet (i.e. the \"cloud\").\n\nIf Dropbox were to disappear tomorrow, you would still have your files on your computer. If your computer were to get destroyed, you would still have them on Dropbox.\n\nBut most importantly for the epiphany, these cloud storage solutions have APIs that make it possible to add, remove and view files within Dropbox accounts from other software on the internet. They can be used to copy all of the data I am currently giving away to online software companies to my Dropbox account for safekeeping.\n\nFrom this idea was born [Asheville](http://asheville.io), a nascent open-source project on which I have been hacking over the past several months. The project's goal is not only to provide a user-friendly solution for syncing one's content (such as photos, status updates, checkins, blog posts, and reviews) over to a cloud storage account on a continual basis. It is also intended to help people do more with their data once it has been synced, by providing them with ways to make their data available to any number of third party software services (or their own).\n\nIn the purest sense, the project is all about helping people establish proper stakes on the web. We live in the digital age but as individuals, the vast majority of us do not have proper digital homes. Many companies are vying to provide them for us but ultimately, they cannot without playing a zero-sum game with our data. We should be empowered to own our personal online data, maintaining close control so we can use it and open it up to others as we please. Asheville is intended to help make that possible for non-technical and technical people alike.\n\nThere are a lot more details about the project on Github, and there is still much more to do before it will be ready for actual usage. But I have already made a lot of headway on the initial user experience, creating an [Ember](http://emberjs.com)-based web application that provides users with real-time updates on their sync status. Next comes tying this interface to backend software that does the work of actual copying the data from various social networks, publishing platforms and other online services to cloud storage accounts.\n\nI have also just relaunched my personal website in preparation for dogfooding Asheville and showcasing the ways in which it can extend the application of your online data. This website is now built in Ember as well and is therefore [a proper JavaScript app](https://github.com/markmhx/markmhendrickson), not just a flat set of files with no way of processing and displaying data from external sources. Currently, it still shows only a set of blog posts  I have written, but soon I expect to add photo galleries, maps of places I have been, status updates and more. All of this will be powered in large part by the data I have already posted elsewhere online.\n\nIf you are interested in getting involved with Asheville, please do not hesitate to get in touch with me. Thanks already to [Jack Pearkes](http://jack.ly/) and [Ryan Barrett](http://snarfed.org/) for helping out. And if you would simply like to try the product once ready, [leave us your contact information](https://docs.google.com/forms/d/1i2iHhLVcfhYIEHPS5G7iD0gC4z-K-2e535GLGrj_qNE/viewform) for further updates.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "11-reasons-why-i-wont-click-on-your-link-bait",
    "title": "11 Reasons Why I Won't Click On Your Link Bait",
    "excerpt": "1. It blatantly panders to my predispositions. 2. I presume it was written by an underpaid intern. 3. I already saw the content on Reddit. 4. I'm...",
    "published": true,
    "publishedDate": "2013-12-06",
    "category": "essay",
    "readTime": 6,
    "tags": [],
    "body": "1. It blatantly panders to my predispositions.\n\n2. I presume it was written by an underpaid intern.\n\n3. I already saw the content on Reddit.\n\n4. I'm sick of watching animated reaction GIFs.\n\n5. The format screams unoriginality and desperation for visibility.\n\n6. Your subject matter is either mind-numbingly trivial or frustratingly oversimplified.\n\n7. My same friends keep falling for your tricks.\n\n8. You keep planting irrelevant sexual photos as thumbnails.\n\n9. You're dumbing society down, list-by-list, training us to become impatient with content that doesn't come in PowerPoint format or isn't scannable for things with which we easily agree or disagree, because really, who has time for sentences with conjunctions that encourage us take some time to actually *think* when we have other websites to visit so we can waste more time instead of getting our work done and on to something meaningful with our lives?\n\n10. You're making the rest of media envious of your apparent success, infecting them with your ways and eroding their already tenuous motivation to market enriching content.\n\n11. It irritated me enough to write this stupid rant of a parody.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "kanban",
    "title": "Kanban",
    "excerpt": "One of the most important aspects of product design and management is prioritization. There are only so many resources at a company's disposal at any...",
    "published": true,
    "publishedDate": "2012-12-11",
    "category": "technical",
    "readTime": 6,
    "tags": [],
    "body": "One of the most important aspects of product design and management is prioritization. There are only so many resources at a company's disposal at any given time that product efforts must be prioritized vigilantly lest a team end up spending considerable time producing sub-optimal or worse, misguided, work. This is especially true at a startup where resources are extremely constrained and product efforts can either suffer or benefit greatly from acute path dependency.\n\nI've found that the simplest yet most effective tool to prioritize product developments is [kanban](http://en.wikipedia.org/wiki/Kanban), or at least my interpretation of it. A well-organized kanban board can help a team visualize what product ideas currently are or are not getting attention, by what team members, and why. It encourages a \"lean\" mentality by putting ideas into a sequence of design,  development and validation only when there is current capacity to do so. And yet it also provides a staging area for longer-standing ideas that await their turn for production.\n\nYou can use digital software to create a kanban, such as [Trello](http://trello.com), but I actually prefer old-fashioned notecards, tape and markers. To create one, you simply need these materials and a decent amount of wall space. You'll be writing down product ideas onto the cards and then taping them up on the wall under predefined columns and sections depending on where they belong. A big advantage of doing this on the wall is that you can very readily use the board during meetings to communicate product direction without having everyone burrow into their laptops. A wall-based kanban system is also more customizable on-the-fly.\n\nProduct ideas, corresponding to notecards that primarily display their label (e.g. \"Daily Digest Email\"), start off in the left-most \"Icebox\" column and move rightward through \"Design\", \"Develop\" and \"Validate\" columns as they proceed along the production process. Ideas start in the icebox because all ideas deserve justification before they get any significant attention from the team, and as the name suggests, the icebox is where ideas stay frozen (i.e. no one is working on them). Whenever you or anyone else on the team thinks of a clever new product idea, it should be put up on the board in the icebox because then it can be considered for production.\n\nI like to divide the icebox into sub-columns for the organization of ideas by their primary purposes. A primary purpose should be determined for every idea, even if that idea serves several purposes, because otherwise it's hard to justify its enactment and later validate its success. You may find that the purposes you outline on the board vary from product to product, but for many products, you can boil them down to mainly user/customer \"Acquisition\", \"Activation\", and \"Engagement\". For example, the daily digest email example above should probably go under \"Engagement\" because the main hypothesis behind such an email would be that it could increase the engagement frequency and long-term retention of users. A different idea, such as \"Send Invitations Page\", could go under \"Acquisition\" because it's intended to help with viral distribution.\n\nYou can order product ideas under each column based on your current sense of their appropriate prioritization, but any such prioritization should be tentative since ideas ought to be pulled from the icebox as the result of iterative assessments about what's worth resources (perhaps as frequently as on a weekly or biweekly basis). It's also ideal to identify what measurable impact on the product will indicate success for an idea before it's taken out of the icebox and put into production (i.e. you could hypothesize that long-term user retention will increase 5% with the introduction of daily digest emails). Kanban is intended to help you prioritize by product ideas by potential impact, but this shouldn't be interpreted as creating a \"waterfall\" situation where ideas are prioritized concretely and deeply into the icebox. Otherwise, you'll find that you aren't properly making incremental product decisions based on the most up-to-date information at your disposal, and you could get yourself locked into unproductive product directions for months or more at a time.\n\nOnce it's determined that an idea should go into production – because it contains the greatest potential for addressing the most-pressing business needs (such as user engagement or acquisition) – its card gets moved from the icebox and into the \"Design\" column. This column in divided into two rows: \"In Progress\" on the top and \"Done\" below. An idea first goes into the \"In Progress\" row, which indicates that designers have begun actively working on its design. Once the designers have finished all anticipated design work for the idea, it moves to \"Done\", which is where it ought to stay, however briefly as possible, before moving into the \"Develop\" column.\n\nSimilarly to the \"Design\" column, the \"Develop\" column indicates when ideas are getting attention from developers, or engineers. The \"Develop\" column has two rows: \"In Tracker\" on top, and \"In Progress\" on bottom. Because I've used kanban to visualize product prioritization in conjunction with [Pivotal Tracker](http://pivotaltracker.com) for engineering prioritization, the \"In Tracker\" row indicates when a given product idea has been picked up by the engineering team and added to their own pivotal tracker projects, which allows them to break it down into specific engineering tasks. The presence of an idea in the \"In Tracker\" section of \"Develop\" rather than the \"Done\" section of \"Design\" indicates that its aims and designs have been explained and delivered to developers, and the idea is therefore in their court to proceed with execution. When they do start working on an idea (which should be quickly in a well-regulated kanban system), it moves to \"In Progress\" until complete.\n\nSince kanban is best used with continuous deployment, there is no \"Done\" row for the \"Develop\" column; cards simply move to the \"Validate\" column, which indicates that their ideas have gotten deployed to actual users and are awaiting validation. The \"Validate\" column can be split into two rows if you have both a beta and live environment (i.e. when ideas get pushed into beta testing, they should go under a \"Beta\" row, and when they go fully live, a \"Live\" row). Notice that cards aren't simply taken off the board once they've been deployed, since the validation column is intended to remind a team that they need to follow up on whether the idea they built actually had its intended effect.\n\nDeployed ideas stay in the validation column until it's determined that they succeeded according to plan, at which point they're removed from the board. But if they (perhaps more likely) failed or fell short, they are moved backwards through the board to whatever stage is deemed necessary (\"Icebox\" if the team gives up on the idea for now, \"Design\" if its failure is deemed a design shortcoming, or \"Develop\" if it's found buggy or incorrectly implemented). Because it's easy to come up with low standards for validating ideas post-production, you'd do best to use the exact hypothesis you stated originally for an idea when determining whether it validated successfully, even if that means leaving a card in the \"Validate\" column for an extended period of time to gauge its effect.\n\nWhen you begin using a kanban board to track the execution of product ideas, you'll notice the bottlenecks in your overall production process. Cards may begin to pile up in the design, develop or validate columns, indicating that you need more resources in those areas if you want to produce so much at a given time. And if you can't add the required resources, you'll know you need to scale back how much you bite off or break down ideas into smaller chunks, with the goal of keeping each of your resources at full (but no greater) capacity.\n\nIf you begin using such a kanban board – hopefully to discuss product direction at regularly scheduled meetings – you'll inevitably find ways to customize it according to the needs of your particular team and product. And these customizations should be made with an eye towards augmenting the board's ability to prioritize the highest-impact product ideas and speed up the process by which they go from design to validation. If you manage to do this, you'll gain peace of mind that you're making the best product choices possible and following through on them effectively.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "success-projection-for-startups",
    "title": "Success projection for startups",
    "excerpt": "Startups often get so consumed by their day-to-day challenges that they do a poor job actually projecting their longer-term success or lack thereof....",
    "published": true,
    "publishedDate": "2012-12-05",
    "category": "essay",
    "readTime": 6,
    "tags": [],
    "body": "Startups often get so consumed by their day-to-day challenges that they do a poor job actually projecting their longer-term success or lack thereof. As a result, many toil without knowing whether they're truly living up to the expectations they've set for themselves or not.\n\nEven though there is a lot of uncertainty surrounding early-stage companies, they'd do well to build working projection models that give them a sense of whether they're on a road to success or growing too slowly per business metrics that matter to them most.\n\nAll startup stakeholders (founders, employees, investors, etc) ultimately gauge their long-term success by the company's anticipated valuation at the future point of time when they decide to liquidate their equity holdings, perhaps several years or more after they begin working on it. Therefore, any projection of success should place this valuation as its end goal and work backwards from there to derive the more immediate factors that go into achieving that success and whether those factors are on the right course.\n\nA company's valuation is derived (at least theoretically in an efficient market) by the total amount of profit it will accumulate forever into the future, discounted to a present value. So, the first derivation from valuation is long-term profit, and since this profit consists of the company's expected revenues and costs, those come next.\n\nCosts can be derived by projecting headcount and other operational expenditures, perhaps by studying public information about other companies that have developed in the same way as you expect your company to do so, providing rough guidance as to how your particular and total costs may pan out over the years.\n\nRevenue can be projected through comparables as well, especially if you anticipate deploying a business model similar to that of a public company (e.g. you could study companies that make most of their money from display ads if that's what you also intend to do). However, you will likely learn the most from them about what kinds of per-unit rates they get from various monetization schemes (such as subscriptions, e-commerce fees, and advertising CPMs), leaving it to you to determine what kind of product usage volume you anticipate mapping against such expected rates.\n\nIf you are, like many consumer startups, intending to generate revenue from advertising, then the model's key questions are consequently: 1) how many active users do you expect to attain by a given date in the future, and 2) how active will they be per day, week, month or year. And this is because you'll need to estimate the size of an active audience that'll be at your disposal for advertisers. You may come up with innovative ways to increase ad rates, but your future revenue will be mainly tied to how large or small that audience becomes.\n\nWith this being the case, you need to focus on how quickly you are accumulating active users and increasing their engagement, and this is where the model starts to get concrete for even the newest of startups. The number of active users for a product at any given point, now and into the future, is determined primarily by its user acquisition rate (how many people are signing up or otherwise first engaging with a product per a given unit of time), its activation rate (what percentage of those people reach a point of appreciation for the product), and its retention rate (what percentage of those who activate continue to use the product repeatedly).\n\nEach of these factors (as well as others that correspond to non-advertising-based business models) is unique to a given product, and eventually you'll need to project them all if you want to complete the model. However, even if you're at a beta stage with only 50 testers, you *can* start projecting them one-by-one, making assumptions about the rest. You won't have a lot of statistical significance with such a small user base, and it's probable that your key metrics will change as you address a larger market. But it'll at least give you a *baseline* from which you can judge movements towards or away from your ultimate definition of success (i.e. how valuable you want the company to become and how quickly). And it'll keep you honest about whether you truly have enough data to establish knowledge about the business's momentum, and if you do, whether that data reinforce or contradict your more subjective intuitions about how well the startup is doing.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "three-types-of-design",
    "title": "Three types of design",
    "excerpt": "People tend to use the word \"design\" very loosely when talking about product, but it's actually quite important to distinguish between the types of...",
    "published": true,
    "publishedDate": "2012-11-29",
    "category": "technical",
    "readTime": 6,
    "tags": [],
    "body": "People tend to use the word \"design\" very loosely when talking about product, but it's actually quite important to distinguish between the types of design (and their respective designers) when seeking to understand what someone means by the term in any given context, as well as how they apply to the overall product development process.\n\nI tend to divide design into three main types: **product**, **interface**, and **visual**.\n\n# Product Design\n\nThe goal of product design is to generate and prioritize functionality that could potentially deliver value to users in correspondence with the product's stated purpose, or to modify that stated purpose when no such functionality has sufficient potential.\n\nA product designer spends their time mainly thinking about user flows and experiences, which is to say, how users ought to encounter the product at various points in their lifecycles, what they are enabled to do upon those encounters, and how that enablement provides users with additional value.\n\nSuch a design involves the least amount of illustration of the three types, but those in the form of low-resolution diagrams, flow charts, and even rough interfaces can help get the point across about how the functionality should work. Often the output of product design consists of verbal materials, such as outlines and essays that convey how the functionality will suit users' needs and psychological profiles.\n\nA good product designer is aware that prioritization is key to their work because there isn't enough time or resources for all promising ideas and the ones with the most promise must be tackled first. And the product designer must continually map this product prioritization to the company's most pressing business objectives.\n\n# Interface Design\n\nThe goal of interface design is to translate the conceptual functionality conveyed by the product designer and articulate how the user actually experiences  and manages to understand that functionality in the product, on a step-by-step basis.\n\nIf the product is a website, the focus is on arranging and defining various elements on each page that provides the user with information and input. If the product is a mobile application, then the medium is screen-by-screen, and if physical, its available materials.\n\nThe interface designer is most responsible for making the product as intuitively usable as possible so that the highest percentage of users derive the value promised by it. A good interface designer understands the constraints and opportunities afforded by their medium and plays the very empathetic role of envisioning and studying how people of all targeted backgrounds will learn (or fail to learn) how to use the product. And they're intent on ensuring that the interface elements come together in a cohesive whole that makes sense to users architecturally, delivering those elements as wireframes or other medium-resolution materials to the visual designer.\n\n# Visual Design\n\nThe goal of visual design is to ensure that the product conveys a sense of quality and elicits the proper emotional response from its users.\n\nVisual design is the most aesthetic and subjective design type, but it's also the most immediately recognizable one. While visual designers take their cues from product and interface designers, they are responsible for crafting and delivering an ethos for the product. They spend most of their time making interface elements both attractive and appropriately toned so as to reinforce the purpose and value of the product for users, and a good visual designer knows how to make a product pleasurable without making assets that are overly conspicuous.\n\nA visual designer spends the most time on detail, since they sit closest to the user's actual experience. And they deliver high-resolution images, animations or other user-ready elements that can be incorporated directly into the product.\n\n# Interrelation of types\n\nThese types can be treated as a hierarchy, in the sense that product design mainly informs interface design, and interface design mainly informs visual design. And as such, it's more important to execute successfully on the product design front than the other two, because decisions (both good and bad) made in that tier will cascade to the others. And it's hard, if not impossible, to make up for shortcomings in product design with amazing interface or visual design (or, likewise, to make up for poor interface design with a compelling visual design).\n\nHowever, it's not desirable nor even theoretically possible to focus exclusively on product design without investing in the other two types. There's no way to actually manifest your product, let alone in a way that consumers find appealing, without spending a decent amount of time actually thinking through its interface and visual considerations and generating outputs from them. In an extreme yet feasible scenario, you could have barebones interface and visual design paired with solid product design and you might eek out traction in a marketplace, but you'll be making life a lot more difficult for yourself.\n\nThe practical question that startups often face, then, is how much attention to give each of these types of design, especially when their general hierarchy is recognized. Does interface design get 50% of the attention as product design, and does visual get 25% of that? Or do they all get roughly equal treatment, or some other division? The answer basically boils down to how much usability friction users can be expected to tolerate (on the interface front) and how central the notions of quality and emotion are to the product's value proposition (on the visual front) at any given release point.\n\nIf you are distributing a product (perhaps a business tool) to people who will likely find value from even a poor user interface and don't care much about how their tools look and feel, then you probably don't have to spend as much time on interface and visuals. But if your product (perhaps a game) needs to sway inexperienced and skeptical new users that it'll dependably provide them with emotional satisfaction, then you may want to make a thorough investment in all three design types. The consideration, therefore, is ultimately a marketing one.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "the-postmvp-fork-in-the-road",
    "title": "The Post-MVP Fork in the Road",
    "excerpt": "A lot of startups rightly aim to release a minimum viable product MVP as their first initiative. The goal of an MVP helps a team rally around a...",
    "published": true,
    "publishedDate": "2012-11-28",
    "category": "technical",
    "readTime": 4,
    "tags": [],
    "body": "A lot of startups rightly aim to release a minimum viable product (MVP) as their first initiative. The goal of an MVP helps a team rally around a concrete product design that they can get out the door in a limited amount of time. And it's every startup's hope that once the MVP is released, early adopters will flock to it eagerly and tell their more mainstream friends by the millions.\n\nWhen most MVPs get released, however, this dream doesn't exactly pan out. Instead of experiencing healthy growth and engagement, the MVP gets tepid interest from the market. It's neither an entire flop, since there are people who adopt it and do seem to like it quite a bit. Nor is it a clear success, since these people number in the hundreds or thousands, and their numbers don't appear to grow very quickly.\n\nAt this point in the startup's lifecycle, it faces a fork in the road wherein it can decide just what to do about its MVP. Most startups think to themselves: \"OK, it looks like we have the beginning of something interesting even if it's not an instant hit. If we just iterate on the core product and make it incrementally better, we'll hopefully get it to the point where its value to the ordinary user reaches an inflection point and our active users will take off.\"\n\nThis is a very dangerous mindset to adopt post-MVP, because you're inherently conceding that your minimum viable product was, well, unviable. But you're also skirting that reality in practice. If the MVP contained the most important kernel of the experience you were hoping to provide users and that kernel itself didn't get most people who tried it excited (judged primarily by your retention rate), then your fundamental thesis was wrong. Adding extra features, design refinements, and performance enhancements on top of the MVP is not going to change the main lesson you've already learnt, which is that the core concept you've released does not resonate with the market you had in mind.\n\nThe other path, which far fewer startups take upon releasing their MVP to an unenthusiastic market, is to stop and honestly assess what they've already attempted with it. Instead of going head-first into a mode of iteration, startups on this path critique their MVP's core experience and develop hypotheses as to why it wasn't found compelling enough by the market that tried it. These startups either attempt the same MVP with a significantly different market, having decided that the one they initially approached didn't have matching needs. Or they stick to the same market and revamp the fundamentals of their product, essentially going back to the drawing board.\n\nInertia is the reason you don't see many startups make this hard decision to question the essence of their market or product. And this inertia is often created by a whole host of factors, such as founder vision, investor and employee buy-in, and press exposure. When you're running product for a startup, it's much easier to stay headstrong about your initial concept and ignore clear market feedback, since one perceives it as too difficult to reorient the entire game plan and messaging for the product (I was certainly guilty of this with Plancast). It's also relatively easy to rationalize that the market simply needs more time to come around, or that just a couple more features will make it \"click\" with people, especially if you have money in the bank and supportive advisors who don't want to dampen your enthusiasm for it.\n\nBut the bravest and most critical thing that product managers at startups can and must do is treat their products as the experiments they are, maintaining a critical and detached eye for what they're building, at least until those products reach a pace of undeniable growth and engagement that makes it clear a market clamors for them.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "does-your-product-provide-instant-gratification",
    "title": "Does your product provide instant gratification?",
    "excerpt": "Over the past year, and mostly as a result of my Plancast post-mortem, I've spoken  with dozens of entrepreneurs who are working on events-related...",
    "published": true,
    "publishedDate": "2012-11-27",
    "category": "technical",
    "readTime": 3,
    "tags": [],
    "body": "Over the past year, and mostly as a result of my Plancast post-mortem, I've spoken  with dozens of entrepreneurs who are working on events-related internet products.\n\nI had a call with another such entrepreneur today who has actually built and released a considerably robust offering, at least from a feature standpoint. The service makes it possible to interact with event data in myriad ways, from personal sharing of plans among close friends to impersonal broadcasting of events to a wide audience. It consists of not only a website but a mobile client and calendar client synchronization to boot.\n\nThe problem, however, was that any given person who encounters the service will be at a loss as to why they should use it, and this was the case for two reasons: \n\n1. **It's not clear which of the features is primary**. The product doesn't sell itself as useful for any particular use case; rather, it leaves it up to the user to figure a main one out for themselves, which they're unlikely to do unless they already have a clear, burning need.\n\n2. **No one feature in particular provides instant gratification**. Even if the new user does manage to determine their use case, none of the ones available provides value immediately. They all demand that the user invest considerable time, energy and open-mindedness before they can likely get any substantial value back.\n\nThese are problems I see often with events services because they can take many different (and often nuanced) forms. For example, they can be about delivering invitations, discovering social opportunities, interacting real-time, networking with other attendees, and more. As a result, product designers often try to stuff several of these forms into one product.\n\nEvent services also wrangle with the issue of delayed gratification, because if the event data (likely) refer to something in the future, you won't fully appreciate the service until you actually attend the event. It's hard to accelerate the value you get, especially since anticipating events also imposes a good deal of psychological overhead, which isn't pleasurable.\n\nHowever, these are problems from which I see many other types of products suffer as well. If you are building something and you can't pinpoint the point in time when many, if not most, new users can reliably achieve gratification upon their first usage (and by that I mean their first time visiting or registering for your website or downloading your app), then you probably have a structural problem with your value proposition. Especially in this day and age, when people mostly treat new internet services as nice-to-haves rather than needs, your product's value needs to be singular and immediately available or you probably won't get a second look.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "a-postmortem-for-plancast",
    "title": "A Post-Mortem for Plancast",
    "excerpt": "Nearly three years ago, I left my position at TechCrunchhttp://techcrunch.com/2009/03/10/hendrickson-were-gonna-miss-you/ to start my own Internet...",
    "published": true,
    "publishedDate": "2012-01-22",
    "category": "technical",
    "readTime": 13,
    "tags": [],
    "body": "Nearly three years ago, [I left my position at TechCrunch](http://techcrunch.com/2009/03/10/hendrickson-were-gonna-miss-you/) to start my own Internet business, with the idea of creating a web application that'd help people get together in real-life rather than simply helping them connect online as most social networking applications had done.\n\nPlancast was the service conceived a few months later from that basic inclination. Its approach was to provide a really easy way for people to take whatever interesting plans they had in their calendars and share them openly with friends, with the rationale that greater social transparency for this particular type of personal information would facilitate serendipitous get-togethers and enable a greater awareness of relevant events. Personally, I figured that knowing more about the events my friends and peers were attending would lead to a more fulfilling social and professional life because I could join them or at least learn about how they spent their time around town.\n\nAlong the way my team built a minimum viable product, [launched from obscurity here on TechCrunch](http://techcrunch.com/2009/11/30/plancast/), [raised a seed round of funding](http://techcrunch.com/2010/03/08/plancast-funding/) from local venture capitalists and angel investors, and worked like mad to translate our initial success into long-term growth, engagement and monetization.\n\nAlas, our efforts began to stall after several months post-launch, and we were never able to scale beyond a small early adopter community and into critical, mainstream usage. While the initial launch and traction proved extremely exciting, it misled us into believing there was a larger market ready to adopt our product. Over the subsequent year and a half, we struggled to refine the product's purpose and bolster its central value proposition with better functionality and design, but we were ultimately unable to make it work (with user registration growth and engagement being our two main high-level metrics).\n\nThis post-mortem is an attempt to describe the fundamental flaws in our product model and, in particular, the difficulties presented by events as a content type. It's my hope that other product designers can learn a thing or two from our experience, especially if they are designing services that rely on user-generated content. The challenges I describe here apply directly to events, but they can be used collectively as a case study to advance one's thinking about other content types as well, since all types demand serious analysis along these lines should one seek to design a network that facilitates their exchange.\n\n#Sharing Frequency\n\nSocial networks (by my general definition and among which I count Plancast) are essentially systems for distributing content among people who care about each other, and the frequency at which its users can share that content on a particular network is critical to how much value it'll provide them on an ongoing basis.\n\nUnlike other, more frequent content types such as status updates and photos (which can be shared numerous times per day), plans are suitable for only occasional sharing. Most people simply don't go to that many events, and of those they do attend, many are not anticipated with a high degree of certainty. As a result, users don't tend to develop a strong daily or weekly habit of contributing content. And the content that does accrue through spontaneous submissions and aggregation from other services is too small to provide most users with a repeatedly compelling experience discovering events.\n\nI run the service, and even I currently have only five upcoming plans listed on my profile, with a total of 500 plans shared over the last couple of years, in contrast to almost 2,800 tweets on Twitter over the same period of time. People often tell me \"I like Plancast, but I never have any plans to share\". With social networks, this is sometimes a case of self-awareness (such as when people say they don't know what to tweet), but often they're simply telling the truth; many Plancast users don't have any interesting plans on their calendars.\n\n#Consumption Frequency\n\nPeople also don't proactively seek out events to attend as you might suppose. I've gotten into the habit of thinking about people as divided into two camps: those who have lots of free time and those who don't.\n\nThose who do are often proactive about filling it, in part by seeking out interesting events to attend in advance. They are generally more inquisitive about social opportunities, and they will take concrete steps to discover new opportunities and evaluate them.\n\nThose who don't have much free time often desire to conserve it, so rather than seeking out or welcoming additional opportunities, they view them as mentally taxing impositions on a limited resource. For them, planning is a higher-risk endeavor, and usually they'd rather not plan anything at all, since if they're busy, they likely have a preference to keep their free time just that – free.\n\nIt's hard to generalize by saying most people are in one camp or the other, but suffice to say, there are many people in the latter. And for them, it's hard to get them excited about a service that will give them more options on how to use their time.\n\n#Tendency to Procrastinate\n\nEven putting this bifurcation aside, most people resist making advanced commitments before they absolutely need to make them. People fear missing out on worthwhile events but don't actually like to take the deliberate initiative to avoid such missed chances, which requires planning.\n\nThis can be attributed primarily to people's desire to keep their options open in case other conflicting opportunities emerge as the date and time of an event approaches. If they can afford to wait and see, they will. Therefore, their commitment will be secured and shared in advance only when they're particularly confident they'll attend an event, if they need to reserve a spot before it fills up, or if there's some other similar prerogative.\n\n#Incentives to Share\n\nReturning to the topic of sharing plans, it's not only a matter of having interesting plans to share but being compelled to actually share them. And unfortunately, people don't submit information to social networks because they love data set integrity or altruistically believe in giving as much as possible. They do it because the act of contribution selfishly results in something for them in return.\n\nMost social networks feed primarily on vanity, in that they allow people to share and tailor online content that makes them look good. They can help people communicate to others that they've attended impressive schools, built amazing careers, attended cool parties, dated attractive people, thought deep thoughts, or reared cute kids. The top-level goal for most people is to convince others they are the individuals they want to be, whether that includes being happy, attractive, smart, fun or anything else.\n\nThis vanity compels folks to share content about themselves (or things they've encountered) most strongly when there's an audience ready and able to generate validating feedback. When you post a clever photo on Instagram, you're telling the world \"I'm creative!\" and sharing evidence to boot. Those who follow you validate that expression by liking the photo and commenting positively about it. The psychological rush of first posting the photo and then receiving positive feedback drives you to post more photos in the hope of subsequent highs.\n\nSharing plans, unfortunately, doesn't present the same opportunity to show off and incur the same subsequent happy feelings. Some plans are suitable for widespread consumption and can make a person look good, such as attending an awesome concert or savvy conference. But, frustratingly, the vainest events are exclusive and not appropriate for sharing with others, especially in detail.\n\nThe feedback mechanisms aren't nearly as potent either, since coming up with a worthy comment for an event is harder than commenting on a photo, and \"liking\" a plan is confusing when there's also an option to join. The positive feedback of having friends join is itself unlikely since those friends have considerations to make before they can commit, and they'll tend to defer that commitment for practical purposes, per above.\n\nAdditionally, if a user wants to show off the fact they're at a cool event, there is little additional benefit to doing so before the event rather than simply tweeting or posting photos about it while at the event. An important exception is to be made for professionals who style themselves as influencers and want to be instrumental parts of how their peers discover events. This exception has indeed been responsible for much of our attendee-contributed event data among an early-adopter community of technology professionals.\n\n#Selectivity & Privacy Concerns\n\nVanity, of course, is not the only possible incentive for users to share their plans. There's also utility to getting others to join you for an event you'll be attending, but this turns out to be a weak incentive for broadcasting since most people prefer to be rather picky about who they solicit to join them for real-life encounters.\n\nWhile event promoters have a financial interest in attracting attendees far and wide, the attendees themselves mainly turn to their closer circle of friends and reach out to them individually. You don't see a lot of longer-tail plans in particular (such as nights out on the town and trips) because people are both wary of party crashers and usually uninterested in sourcing participants from a wide network.\n\n#The Importance of an Invitation\n\nOn the flip-side of this reluctance to share plans far and wide is the psychological need for people to get personally invited to events.\n\nPlancast and other social event sharing applications are rooted in an idealistic notion that people would feel confident inviting themselves to their friends' events if only they knew about them. But the informational need here is not only one of event details (such as what's going to happen, when, where and with whom). People often also need to know through a personal invitation that at least one friend wants them to join.\n\nWhen you have a service that helps spread personal event information but doesn't concurrently satisfy that need, you have a situation where many people feel awkwardly aware of events to which they don't feel welcome. As a result, the most engaging events on Plancast are those that are open in principle and don't solicit attendees primarily through invitations, such as conferences and concerts, where the attendance of one's friends and peers is a much less important consideration for their own.\n\n#Content Lifespan\n\nGetting content into a social network is not enough to ensure its adequate value; there's also an importance of preserving that content's value over time, especially if it just trickles in.\n\nUnfortunately, plans don't have a long shelf life. Before an event transpires, a user's plan for it provides social value by notifying others of the opportunity. But afterwards, its value to the network drops precipitously to virtually nothing. And since most users don't have enough confidence to share most plans more than one or two weeks in advance, plans are typically rendered useless after that length of time.\n\nContrast this expiration tendency with more \"evergreen\" content types, such as profiles and photos. Other people can get value out of your Facebook profile for years after you set it up, and the photos you posted in college appear to have even increased in value. Nostalgia doesn't even have to play a part; people's hearts will melt upon viewing [this puppy](http://pinterest.com/pin/62065301084425706/) on Pinterest, Tumblr, and other visually-heavy content networks for a long time to come. But how much do you care that [I attended a tech meetup](http://plancast.com/p/7crb/october-2011-ny-tech-meetup) in New York last October, even if you're my friend?\n\n#Geographic Limitations\n\nGeographic specificity is another inherent limitation to a plan's value. Unlike virtually all other content types (with the exception of check-ins), plans provide most of their value to others when those users live or can travel near enough to join.\n\nI may share plans for a ton of great events in San Francisco, but few to none of my friends who live outside of the Bay Area are going to care. In fact, they'll find it annoying to witness something they'll miss out on. Sure, they might appreciate simply knowing what I'm up to, but the value to that kind of surveillance is rather modest all by itself.\n\nThis is especially problematic when trying to expand the service into new locations. New users will have a hard time finding enough local friends who are either on the service and sharing their plans already, or those who are willing to join them on a new service upon invitation. People who encounter the service from non-urban locations have the hardest time, since there aren't many events going on in their area in general, let alone posted to Plancast. Trying to view all events simply listed within their location or categories of interest yields little for them to enjoy.\n\n#Looking Forward\n\nDespite all of these challenges, I still believe someone will eventually figure out how to make and market a viable service that fulfills our aims, namely to help people share and discover events more socially. There's simply too much unearthed value to knowing about much of what our friends plan to do to leave information about it so restricted to personal calendars and individuals' heads.\n\nAnother startup may come along that develops insight into an angle of attack we missed. Or, perhaps more likely, an established company with an existing event or calendaring product will progressively provide users with a greater ability to share their personal information contained within. On the calendaring side, Google is possibly the best-situated with Google Calendar and Google+, which together could make for a very seamless event sharing experience (one of the things we considered seriously for Plancast was deep personal calendar integration, but a sufficient platform for it simply wasn't available). On the events side, companies like Eventbrite, Meetup and Facebook have services that are primarily compelling for event organizers but already contain useful data sets that could be leveraged to create their own social event discovery and sharing experiences for attendees.\n\nPlancast managed to attract a niche audience of early adopters who found it to be among the most efficient ways to share and hear about events (thanks, users! you know who you are). Over 100,000 have registered and over 230,000 people visit each month, not to mention enjoy the event digests we send out by email each day. For that reason alone, and despite its growth challenges, we're going to keep it up and running for as long as possible and are hopeful we'll find it a home that can turn it into something bigger. It's my expectation that one day mainstream society will take for granted the type of interpersonal sharing it currently enables for just this small community, and I look forward to seeing how technological advancements overcome the aforementioned challenges to get us there.\n\n*This post was originally [published on TechCrunch](http://techcrunch.com/2012/01/22/post-mortem-for-plancast/) in January 2012.*",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "share-frequency",
    "title": "Share frequency",
    "excerpt": "When designing a social network that depends on users to contribute content from which they'll collectively derive value, one must consider certain...",
    "published": true,
    "publishedDate": "2011-08-12",
    "category": "technical",
    "readTime": 4,
    "tags": [],
    "body": "When designing a social network that depends on users to contribute content from which they'll collectively derive value, one must consider certain qualities of its supported content types to determine whether those types can provide enough ongoing value to keep users engaged.\n\nAmong these important qualities is the frequency with which people are compelled to create and share a given type of content. People are interested in sharing some types on a seemingly continual basis, spacing out contributions mere hours, minutes or even seconds apart. Conversely, there are types that make sense to share only occasionally when unique opportunities or needs arise.\n\nWhile the suitable frequency of each type varies between individuals, generalizations can be made for evaluation and comparison purposes. For example, status updates, which a given person might find him or herself compelled to produce several times per day, lend themselves to a greater frequency than blog posts, which the same person might publish only every few weeks.\n\nThe general frequency of a particular content type results from numerous factors that affect the costs and benefits of sharing it. All else being equal, types that are easy to produce, such as check-ins or one-off photos, enjoy a greater frequency than those that take more time and consideration, such as restaurant reviews or entire photo albums. Types that return more value to the producer, such a thoughtful answer to a question that earns social acclaim, also enjoy greater frequency than less beneficial types that require the same investment.\n\nIt also seems clear that people, due to their impatience, have a greater cost elasticity than benefit elasticity, in that a little less effort makes a bigger positive impact on frequency than a little more benefit. This asymmetry might help to explain why we've seen smaller, bite-sized types of sharing emerge, whereas we haven't seen as many new services that target types with higher costs yet higher yields.\n\nIt's also possible that with current feedback mechanisms (which provide superficial doses of social validation rather than more impactful, long-standing personal gains), there are simply more apparent opportunities to reduce costs than increase benefits, even if that results in a downward movement of publisher value (and likely consumer value) per share.\n\nEvery type of content has its own set of reasons for why it presents people with higher or lower costs and benefits, and a study of each is necessary to understand their resulting frequencies. When choosing one or more types for a new service, it's important to conduct this study to determine whether they'd yield a high enough frequency to engage users on a continual basis.\n\nHigher frequency generally leads to greater engagement if only because it enables the production of more content within a given period of time and, after all, content is the lifeblood of any social network and needs to accrue. If the value of content is also dependent at least partly on its recency (as is the case with virtually all types, to varying degrees), frequency is even more important because there must be enough new content available at any given time that users decide to engage with the service. The depreciation of existing content essentially needs to be counteracted by fresh content at a sufficient enough rate.\n\nThe need for a relatively high sharing frequency is particularly acute due to an increasing number of services vying for consumers time and attention. Each additional service drives up the minimum value users demand from the next, either as producers or consumers of content. An important question for social network designers, then, is what are the types of content that will provide enough net publishing value that they elicit frequent contributions from their target demographics, especially as their opportunity costs rise.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "homesteading-on-the-indie-web",
    "title": "Homesteading on the indie web",
    "excerpt": "I had the pleasure of attending IndieWebCamphttp://indiewebcamp.com/ in Portland last month, a BarCamp-style conference where techies get together to...",
    "published": true,
    "publishedDate": "2011-07-30",
    "category": "technical",
    "readTime": 10,
    "tags": [],
    "body": "I had the pleasure of attending [IndieWebCamp](http://indiewebcamp.com/) in Portland last month, a BarCamp-style conference where techies get together to brainstorm ideas about how they can help people own and control their online identities.\n\nThe so-called indie web movement, a spiritual cousin to the open source and standards movements, is rooted in a desire for digital freedom, primarily from monopolies that threaten to restrict and violate the common Internet user's online existence. It calls for practical means to protect this existence by preventing or disrupting the control that any one company has over a person's online identity, either from a functionality or data point of view.\n\nIt's a thought-provoking movement for a number of reasons, not least because it finds itself screaming into the wind, so to speak. Most Internet users, with the proliferation of social networks, increasingly place their digital lives in the hands of proprietary services run by mostly private — and always self-interested — companies. These users don't own the identity and content they publish to these services in a way that insulates them from their vague terms of service and the application thereof. Nor can they continue to enjoy those services (at least in the same manner) if the companies shut them down, redesign them undesirably or fail to improve them. Yet, only a small minority of users actively worry about these problems and usually only once they've been stung by account deactivation, incessant downtime, censorship, privacy leaks, or critical design shortcomings.\n\nThere's a moral tone to the indie web movement, not just an insistence that users ought to control their online identities for the practical purpose of avoiding conflicts with their service providers. Proponents argue that the Internet needs to maintain its decentralized nature and resist consolidations of power lest technological progress gets stymied, data gets lost, hoarded or corrupted, and users get disenfranchised en masse. There's a tension here, since private companies that treat their users as [virtual sharecroppers](http://nomoresharecropping.org/) are clearly responsible for much of the progress occurring on the web today, and their services are making it dramatically easier for everyone, including the technically illiterate, to participate online.\n\nThere were two particular challenges to the indie web movement that struck me while attending the conference. The first had to do with identifying the relevant and recognizable needs of the average Internet user to obtain better control over their online identity. Indie web proponents lodge a disparate number of valid complaints against proprietary services, each with its own merit but none that would be recognized by mainstream audiences as a massive, immediate problem on its own.\n\n[Tantek Çelik](http://tantek.com/), the conference's lead organizer and my gracious host, cited the famous downtime of services like Twitter and Tumblr as reason for decentralization, as well as the tendency of acquired services to get shut down. Others cited the desire to more easily export and manage the content they post to services so it can be used on their personal computers and published elsewhere on the web. For others still, it was primarily an issue of personalization and the ability to interact with numerous online services and their respective functionality with more flexibility and fluidity.\n\nAll of these are pain points that are best articulated by technologists who take the time to understand them but are surely felt by \"normals\" as well. They don't, however, seem top of mind enough to compel millions of ordinary Internet users to take concrete steps to address them, at least with today's solutions. Downtime is frustrating but most people learn to work around it; shuttered services disappoint loyal users but most likely faced their demise due to popular disinterest; and most people don't know what else they want out of the services they use, at least substantially enough to seek alternative solutions.\n\nThis complacency poses a critical motivational problem for the primary decentralization scenario proposed by those in the indie web movement, wherein users (both early- and late-adopter alike) take the initiative to host their identity and personal content independently of any proprietary service. The idea here is that everyone should register their own [second-level domain](http://en.wikipedia.org/wiki/Domain_name) and put up a personal website of some sort, just as I've registered markmhendrickson.com and centralized my online identity there. This site could be a simple, static presence or advanced enough to exchange information with proprietary services so that interactions can take place with friends or followers. Theoretically, these proprietary services could get cut out entirely over time, and independent personal websites could begin communicating with each other directly, effectively mapping social networking relationships onto the Internet in a distributed, peer-to-peer fashion.\n\nIn addition to the marketing challenge of compelling individuals to establish these independent sites, there's the technical challenge of bringing this distributed system to life and making it possible for normal people to get involved. The technical challenge can be divided on one side into the infrastructural issues of decentralizing the real-time communications that currently take place within centralized services (such as forging social relationships, posting content to streams, and interacting with that content). On the other side, there are the technical issues of setting each user up within the decentralized system and making sure they have the tools needed to participate without getting tied to any single provider.\n\nEach IndieWebCamp attendee spent the second day of the conference working on a self-chosen project that would aid the movement. I took it upon myself to devise a tool that would perhaps solve the second half of this technical challenge while also communicating to mainstream users why they ought to set up their own domains. My project was primarily user-centric, since it deferred many of decentralization's intricate engineering decisions and instead focused on motivating users to overcome their default complacency and break ground on their own online homestead.\n\nI established several main requirements for this tool:\n\n- It had to simplify for users the process of registering a domain name and a basic web host, both of which had to be treated as commodities and substitutable at any time. While it's not possible or feasible for users to literally own their domain and hosting, the next best thing is to minimize the differentiation power of these services by abstracting them away.\n\n- It had to automate the process of setting up an initial website, or homestead, on the newly registered domain and host, as well as to automate the processes of updating or extending it later on. While the software for the website had to be fully hosted by the user and open-sourced for maximum control, it could be assisted by the tool on an ongoing basis through code and data pushes.\n\n- The user couldn't be expected to use FTP, a command line interface, a file system, or any other technologies beyond the browser because doing so would severely limit its accessibility. User interactions had to be limited to filling out web forms and clicking on things.\n\n- The financial and time burden of using the tool to both set up and maintain a homestead needed to be minimized as much as possible.\n\n- Users couldn't be required to reenter their personal information or manually upload content they've already shared elsewhere.\n\n![]()\n\nThe tool's initial user experience is outlined by the wireframe above. The marketing appeals directly to a person's need for control, since that's ultimately what users are expected to obtain in a decentralized system, it likely resonates with an underlying fear that their current online identity may be in disarray, and it's a vague enough proposition to allow many solution details.\n\nThe page then addresses four of the most identifiable needs under the tent of controlling one's online identity. Obtaining a personal URL allows a user to more easily point people to their information online; ranking well-curated personal information highly on Google allows a user to control what people find out about them when searching their name; listing all of a user's social networking profiles in one place brings order to identity fragmentation; and backing up a user's online content from numerous sources provides peace of mind. The area at the bottom that lists other people's websites is meant to provide social validation for these propositions.\n\nTo get started, the user needs to enter just their desired URL, an email address and a password (with the desired URL checked against a domain registrar's API, assuming one exists). Requests for other values, such as the user's name, are omitted since they can be gathered from the user later on. The goal here is to have them engage with the setup process as painlessly as possible.\n\n![]()\n\nUpon entering this basic information, the user is prompted to connect their new homestead to any number of their online services. A link to each of these services, once connected, will show up on the user's homestead. Content posted to them can also be pulled, either once or continually, for redisplay or simply backup on the user's homestead, depending on what kind of service it is.\n\nFor example, when a user connects their Facebook account, they can choose to have all of their photos and status updates automatically republished to their homestead. Not shown are possible options to simply back up these but not republish them. By connecting with any of these services, the tool can also automatically determine the user's name, portrait and any other details to display on the homestead.\n\n![]()\n\nThe final setup step consists of actually paying for the desired URL, with the assumption that the tool could arrange for free hosting. This part of the mockup isn't fleshed out much, but basically the page would show the appropriate form once the user has chosen their preferred payment method.\n\n![]()\n\nThe result is a profile page not terribly unlike those you'd find on most social networking sites but hosted on the user's own domain and consisting of information about and from the user from a variety of sources. Their service profiles show up on the left along with their portrait and bio, and content they've decided to import into their homestead shows up aggregated on the right.\n\nThis is meant to be just a start. There are a number of ways the design and functionality of a given user's homestead could be advanced. The layout and theme could be customizable. The user could add the ability to post content directly to their homestead and then have it syndicate out to other services. They could even start creating connections with other homesteaders by adding them as friends or the like, all referenced by their own URLs.\n\nPerhaps an open-source ecosystem could even emerge that provided plugins and other modifications to the core software package, eventually enabling social experiences that rival those of proprietary services, with feeds, messages, tags and more. The central accomplishment here would be in enabling large numbers of people to claim independent online presences with the potential to play increasing roles in their online lives. Once enough people have done so, it'll be much easier to weave a indie web between their homesteads and insulate them from the decisions or fate of any particular company.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "content",
    "title": "Content",
    "excerpt": "People are particular about the forms of communication they employ when expressing themselves, through social networks or any other media, because...",
    "published": true,
    "publishedDate": "2011-07-07",
    "category": "technical",
    "readTime": 5,
    "tags": [],
    "body": "People are particular about the forms of communication they employ when expressing themselves, through social networks or any other media, because different forms possess different powers of conveying information.\n\nWhen in the physical presence of others, we can communicate verbally, visually or tactilely with our words, gestures or touch. Words are usually chosen to communicate abstract concepts, finger pointing is best suited to convey direction, and hugs provide the quickest route to imparting fondness.\n\nWhen afar, we can send each other letters, speak to each other over the phone or route a message through a friend. The message might ostensibly be the same despite the form it takes, but a letter will likely impress a greater sense of consideration, a phone call will impart nuances by way of intonation, and a routed message will include the implicit validation of its intermediary. The transmitter must choose their form carefully if they want to get the intended message across because each form has its own abilities and disabilities to deliver information.\n\nLikewise, social networks are constructed around particular forms of communication and consequently limited to the characteristics of those forms. The various forms can be considered types of *content*, because shared information persists within a given network and is intended to benefit its consumers by entertaining or edifying them. As such, it's important to consider the types of content people can share within a network as key to its communicative value.\n\nAll social networks collect identification information from their members and publish it back out as static content. Usually this consists of a member's name and portrait as well as their location and one-line biography. Particularly identity-centric networks collect a lot more static, or evergreen, information such as employment and educational history, music and movie interests, and contact details. The sum of this content is displayed primarily on a single page, which serves to anchor the user's identity within a network and provide a reference point to others. Therefore, networks share the profile as a fundamental content type.\n\nSocial networks almost universally publish some manner of relationship content, too. Friendships, follows, subscriptions, and the like indicate that pairs of people have a relationship between each other that's worth recording and making known. And the types of relationships that can be captured depend on the model a given network has implemented and how that model has been communicated throughout the service. This content – which is often showcased on profile pages but importantly delivered through notifications as well – constitutes yet another fundamental type that varies only in implementation.\n\nThe content differences between social networks, however, mainly come from the types of information that users are able and encouraged to submit as discrete objects. These types are manifold: photos, videos, graphics, status updates, blog posts, articles, documents, books, events, travel plans, travel advice, questions, answers, bookmarks, pokes, reviews, deals, goods for sale, money, vital stats, purchases, gadgets, badges, check-ins, short-form messages, gifts, songs, audio clips, polls, webpages, brands, applications and more. This content is posted proactively by users and its immediate destination is often a feed or profile page. It will likely be repurposed for other consumption points, such as search or syndication, too.\n\nThere is also a host of reactive content types that social networks variably support. These include, most commonly, comments or replies and gestures that indicate approval or disapproval of shared content, such as likes, reposts, favorites or votes. These reactive types are designed to permit direct interaction around pieces of content, allowing the publisher and any other established participant to garner feedback and increase the impact of their contributions. Furthermore, reactive content can be generated in response to other reactive content, thereby extending chains of interaction to deeper levels.\n\nSome social networks support many of these proactive and reactive content types while others specialize in just one or a few. Support may also differ in subtle yet important ways between two or more networks, allowing those networks to convey substantially different information and consequently present dramatically different value propositions to their members.\n\nComparisons aside, every network must be designed around a combination of content types that can be used to fulfill the identifiable communication needs of its producers and consumers. On one side of the equation, a sufficient number of people must be interested in producing a given type of content because it allows them to express themselves in a way they find valuable. On the other, a sufficient (and most likely larger) number of people must be interested in consuming that content because it benefits them in a recognizable way.\n\nSocial network designers must identify not only certain communication needs and their corresponding content types but the frequency and size of those needs as well. Network participation requires commitment on the part of its members, lest they forget or resist leveraging it when their needs arise. And the only way to earn that commitment is to satisfy members' content needs either frequently in small ways or occasionally in big ways.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "relationship",
    "title": "Relationship",
    "excerpt": "When people connect with others on a given social network, they are conscientious about whom they will connect with, because an exchange of...",
    "published": true,
    "publishedDate": "2011-06-04",
    "category": "technical",
    "readTime": 5,
    "tags": [],
    "body": "When people connect with others on a given social network, they are conscientious about whom they will connect with, because an exchange of information, both immediate and ongoing, will result from the connection.\n\nJust as in offline life, people don't like to send and receive information to and from random people; their relationship with those people is crucial. The things you say to those you encounter on the street will differ from the things you say to familiar people in your own home. Conversely, your interest in what strangers have to say will differ from your interest in what your friends can tell you.\n\nThe types of relationships that people experience aren't simply divided between friends and strangers; they are manifold and impossible to label with complete precision. Strictly speaking, no particular relationship gets formed between two pairs of people because nuances invariably come into play. You may be office mates with both Tim and Joe, but you're a bit fonder of Joe because he invites you to lunch.\n\nRelationships also aren't perfectly symmetrical. While you think warmly of Joe, he might think you're kind of a jerk and only asks you to join him because he's interested in your sister. Consequently, any label and assumption of symmetry you assign to a given relationship will constitute an approximation at best.\n\nHowever, approximations are useful when trying to identify the type of relationships a given social network should or does facilitate, because individuals themselves map their relationships to approximate groups. And despite the efforts of designers to diversify the types of relationships that thrive on their networks, consumers tend to view each social network as primarily suitable for only one of their groups.\n\nUnderstanding a group to be simply a set of people who share the same approximate relationship to each other, we can identify an array of such groups that might be facilitated by social networks.  On a high level, there are expansive groups of people you've met and people with whom you've simply communicated. There are also people you admire and people you want to impress.\n\nMore specifically, there are acquaintances from colleges, companies and organizations. There are peers in your industry and collaborators on your specific projects. There are close friends whom you see weekly as well as old friends from high school you see once a year. There are family members and teammates. And there are folks you may or may never have met but who share the same interests as you.\n\nWhatever the group and however specific, it needs to have enough members who both find the group important and desire better ways to share information with each other to warrant a dedicated network. And its importance is often tied to the group's size and its predominance in members' lives. Facebook initially took off among college (and then high school) students because it intensified the already intense relationships that existed within academic communities. Likewise, Twitter and LinkedIn initially thrived by bolstering professionally important relationships within the Silicon-Valley-centric tech scene.\n\nFurthermore, when someone encounters a new network, it's important that they can actually identify which of their relationships it will facilitate and how they will benefit as a result. Otherwise, they are presented with the communications equivalent of a hammer without a nail; they won't know what to do with the social network and it will seem pointless. Similarly, if you signal that the network is meant for a particular type of relationship they don't have, want or care for - or if they feel as though they don't have an unaddressed communication need for that relationship - they won't feel compelled to participate.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "three-pillars",
    "title": "Three pillars of social networking",
    "excerpt": "Social networking is a precondition for new modes of information exchange, not an end in and of itself. By definition, a social network is simply a...",
    "published": true,
    "publishedDate": "2011-05-02",
    "category": "essay",
    "readTime": 5,
    "tags": [],
    "body": "Social networking is a precondition for new modes of information exchange, not an end in and of itself.\n\nBy definition, a social network is simply a set of connections between different people represented by a computer system. These representations wouldn't provide value to anyone if they didn't enable the exchange of information in novel ways. When you friend someone on Facebook, connect with them on LinkedIn, or follow them on Twitter, you aren't doing it for academic purposes; you're doing it to communicate. You don't care about the improved integrity of the social network; you care about the ways in which you can use it to interact with people you care about.\n\nThese networks share important characteristics with one another but crucially differ in even more important ones. The differences are more important, if also more poorly understood, because they allow each of them to present unique ways of exchanging information. If that weren't the case, society wouldn't need more than one social network.\n\nThere are three categories into which these differences can be broken down to better understand market demand for various social networks. First, there is the question of **relationship**, or the significance of the people forging connections to each other on a given network. Second, there's the question of **content**, or the type of information that can be shared across the network. And third, there's the question of **mechanism**, or how that information can be published or consumed, which affects not only its production and distribution but its meaning as well.\n\nThese categories form three pillars of effective social networks. Weaknesses can and inevitably will be tolerated within any given pillar, but each must be strong overall or the network won't constitute a compelling way to exchange information. Additionally, new social networks must differentiate themselves from existing ones by establishing at least one (but not necessarily all) of these pillars differently, thereby giving people a reason to adopt another network.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "how-to-pitch-a-tech-blogger",
    "title": "How to pitch a tech blogger",
    "excerpt": "I've been asked privately quite a few times over the last couple of years how one should pitch their startup to a tech blog like...",
    "published": true,
    "publishedDate": "2010-09-26",
    "category": "technical",
    "readTime": 5,
    "tags": [],
    "body": "I've been asked privately quite a few times over the last couple of years how one should pitch their startup to a tech blog like [TechCrunch](http://techcrunch.com), [GigaOm](http://gigaom.com), [VentureBeat](http://venturebeat.com) or [ReadWriteWeb](http://readwriteweb.com). So I've decided (quite selfishly) to write a post about the subject instead of repeating myself or re-forwarding emails.\n\nThis comes from my experience as both a tech writer (for TechCrunch, ~1.5 years) and internet startup entrepreneur (for [Plancast](http://plancast.com), also ~1.5 years), so I've been able to see things from both sides of the table, particularly when it comes to PR for newly founded startups. As such, these are principles that I primarily recommend to unproven entrepreneurs with unknown companies who want to launch publicly for the first time. Once an entrepreneur or their company gains visibility, their approach to PR will evolve and the press may end up coming to them for news instead of the other way around.\n\n# The Story Is Key\n\nWhen you pitch a blogger – or any writer for that matter, whether they work for The New York Times or your local paper – it's crucial to recognize their desire to identify and then write a *story*. And by story, I mean something that starts, continues, completes or encapsulates a narrative. Bloggers have no interest in merely reporting facts detached from meaning. And they certainly don't want to report facts that actually have insufficient significance to their readers. Bloggers dread the idea of someone coming along and justifiably saying \"so what?\". Good narratives prevent that. Great narratives are thought-provoking and get further developed in readers' minds.\n\nNow, you obviously don't have the power to directly dictate which narrative a blogger will craft as the result of your pitch (no matter how many pay-for-publish conspiracies you've heard). But it's important to think about a narrative for your company or product, because you *can* and *should* steer the blogger towards it. Why? Because bloggers are strapped for time and don't possess the same depth of domain expertise as you. Lay out a narrative that jibes well with their preconceptions and they'll likely run with some form of it.\n\nIt helps to recognize some of the more common types of narratives. If you read through the headlines on [Techmeme](http://techmeme.com/), you'll find that most fit into at least one of the following:\n\n- **Competitive or Political Drama** - aka \"company X releases product Y to kill company Z\"\n\n- **Gossip** - \"CEO of company X gets tangled up in Y\"\n\n- **Insight** - \"trend X will change the world because of A, B, and C\"\n\n- **Evolution & Confluence** - \"service Y is like X for Z, capitalizing on the recent developments of A and B\"\n\n- **Success** - \"company X has created super impressive technology Y, is growing fast, or has made lots of money\"\n\n- **Failure** - \"company X is dying or has messed something up\"The idea is to figure out which type you want to adopt and then craft the facts of your announcement into a compelling and succinct narrative that conforms to it. You'll likely opt for type #4 or #5, but don't hesitate to spice it up with a bit of #1 or #3 (the story can have sub-narratives, but expect the blogger to lead with only one). This isn't an exercise in stretching the truth or making stuff up; there's a reason why you've built what you've built or done whatever you're announcing. Weave that reason into a bigger story while avoiding as many buzzwords as possible.\n\nWhen framing your narrative, you'll do well to remember that bloggers are creatures of comparison. They'll immediately try to compare your product or announcement to another they've already seen, and if they find a close match, they'll pass on it. You should get out in front of this reaction by emphasizing the characteristics of your announcement that [make it unique](http://www.sethgodin.com/purple/). But don't insist that it is incomparable; on the contrary, be forward about drawing comparisons that will highlight the significance of its uniqueness. The writer should come away from your pitch thinking \"I've seen cows before, and this is indeed a cow, but it's purple! All of the other ones I've seen are only black and white\" **not** \"This guy insists this purple thing is not a cow but it obviously is. It might be worth writing about the fact that it's purple but I'm not sure; it feels as though I'm being pitched another cow\".\n\n# Relationships Matter\n\nThis may sound like psychological manipulation directed towards selfish ends (i.e. sales) but if that's how it feels, you're doing it wrong. The goal here is to help the blogger, not exploit them. When you help them (with well-articulated material for a story), they help you (with a story that will publicize your business). As with all transactions, it relies on a relationship, however temporary. And the success of that relationship will depend on how much trust and rapport you've established.\n\nA lot of times when entrepreneurs are ready to pitch, they go looking for a friend who knows and can refer them to a writer. The idea here is to leverage someone else's relationship to validate themselves transitively. This is all fine and good, and it's certainly better than submitting a story to a writer cold. However, it's much better to begin building a direct relationship with them well before the pitch.\n\nOne of the beautiful things about the internet is that you can develop relationships with people without ever meeting them. Get on your favorite bloggers' radars by commenting thoughtfully on their posts, retweeting and replying to them on Twitter, and submitting promising tips to them for stories that have nothing to do with your company. If you blog, take the time to write pieces that link to their pieces; they'll most likely read them and take note of your name. If you happen to live in their area, introduce yourself and chat with them casually at an industry event without giving an elevator pitch unless they ask.\n\nThe point is to achieve some level of familiarity and validation before ever pitching them on a story, not to become their best friend. In fact, you don't want to be too overeager or complimentary, otherwise they'll perceive you (rightfully) as a suck-up.\n\nWhen you're ready to pitch, make sure you're not wasting their time with material that can't be delivered as an interesting story. A litmus test is whether you'd honestly be interested in reading about your announcement if you weren't the one behind it. And when presenting the story, keep it real. Certainly don't embellish or lie about anything. Build trust by throwing in a few facts that, if published, might not make you look so good. If you must, just ask the blogger to please not publish them and they won't, but you'll gain credibility in their eyes.\n\n# A Straightforward Procedure\n\nAs far as the mechanics of delivering a pitch, it's best to ping a blogger about the announcement you'd like to make about a week beforehand. Describe it in one paragraph (no more, no less), suggest the time you'd like them to write about it, and ask them if they're interested and want to hear more. If they respond in the affirmative, send them a few more paragraphs with details and some visuals (e.g. screenshots or demo video) or private access to an alpha product, if relevant. **Do not send them a press release; it will only insult their intelligence**. \n\nTry to be flexible on the timing if they're busy, and if you must pitch the same announcement to more than one blogger (not advisable for unknown startups who should bolster the value of their story with exclusivity), be completely forthright about it and your reasons for doing so. Resist the urge to propose an embargo; [they only cause frustration](http://techcrunch.com/2008/12/17/death-to-the-embargo/).\n\nOnce a blogger has written about you, don't embarrass them by being the first to comment with \"thank you for writing about us!\". Do your part in promoting the piece by getting friends and family to retweet, post to Facebook, etc. And space things out before pitching them again so they don't grow tired of you or the subject. \n\n# Your Company's Best Representative\n\nIf this procedure sounds simple enough, you can craft the most compelling story for your company or product, and you have the time necessary to build these relationships, then you shouldn't hire anyone else to handle PR for you. It'll only be a waste of money, and you'll get less than optimal results. In any case, bloggers much prefer to work directly with executive-level representatives than PR firms, so you'll be doing them a favor.\n\nAs you scale your business, or if you find any of this particularly daunting, then perhaps you should seek professional guidance. But otherwise take this as an opportunity to develop a new skill set and relationships that'll serve you well even beyond your current startup.\n\nGood luck!",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "some-latenight-ideas-for-blippy",
    "title": "Some late-night ideas for Blippy",
    "excerpt": "I feel a certain kinship towards the founders of Blippyhttp://blippy.com/. Not because I know them well I've met Philip Kaplan aka Pud only once but...",
    "published": true,
    "publishedDate": "2010-01-18",
    "category": "essay",
    "readTime": 5,
    "tags": [],
    "body": "I feel a certain kinship towards the founders of [Blippy](http://blippy.com/). Not because I know them well (I've met Philip Kaplan aka Pud only once) but because they're pushing the limits of what people are willing to share about themselves online. While we at [Plancast](http://plancast.com/) are encouraging folks to be more open about their future whereabouts, the team behind Blippy is hoping that people are ready to share their purchases with the world.\n\nBoth of our services are also very new, and as to be expected with new web services, there's still lots of work to be done on both. In the spirit of tech camaraderie, I thought I'd offer up a few (unsolicited) suggestions for Blippy.\n\n- **Give us digests**. The current user experience is primary centered around a mostly reverse chronological, FriendFeed-like stream of purchases. This is okay but I'd prefer to check Blippy as often as I check Mint (which is to say, once a month). And when I do, I'd like to see an overview of sorts that breaks my friends' spending habits down. Tell me what their biggest and smallest purchases were; their strangest purchases; their spending habits (have they been splurging on clothes? buying a lot of airline tickets?); and overlap in their spending (who's buying the same things? what are the trends among my friends?). Pretty graphs might help. Maybe incorporate some maps so I see where about town people are spending their money. Who knows, I might visit once a day if the data updates constantly with new trends.\n\n- **Provide more info about the purchases**. Right now each purchased item is displayed in tiny blue type. Blow that up if it's available and give me context (a URL to where I can buy/view more info), an image, and a description. Show me who else I care about has also bought it.\n\n- **Let us condense/hide comments**. I realize that much of the interaction onsite right now is around the comments people make on purchases. But I'd personally rather locate an interesting purchase *then* choose to view the comments around it.\n\n- **Add a \"Want\" button**. The \"like\" button is a step in the right direction, but perhaps a \"I Want This\" button would be more valuable. It signals a higher level of interest in the purchase, leaving simple \"oh that's cool\" expressions for the comments. You could have profiles list not only purchases users have made but the items of their friends they want. Analyze this data in aggregate to see who starts purchasing trends (fashion being an obvious area).\n\nMy 2 cents.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "plancast-in-public-beta",
    "title": "Plancast in public beta",
    "excerpt": "I'm happy to announce that the site I've been working on for the past half year or so - Plancasthttp://plancast.com/ - is now available in public...",
    "published": true,
    "publishedDate": "2009-11-15",
    "category": "technical",
    "readTime": 2,
    "tags": [],
    "body": "I'm happy to announce that the site I've been working on for the past half year or so - [Plancast](http://plancast.com/) - is now available in public beta. If you haven't tried it out yet, please give it a whirl and [send us your thoughts](http://plancast.com/contact). I suggest you get started by posting a few plans that may be tucked away in your personal calendar. Share them on Plancast and you'll be surprised by how positively other people respond after hearing about them.\n\nWhat does Plancast do, you ask? It helps you share your upcoming plans with friends and learn about what others will be doing in the future. Imagine how awesome it would be if we all had a better idea of what everyone was up to in the next few hours, days, weeks, or months. Thinking about grabbing drinks with friends tonight? Going to a concert tomorrow? Heading to a conference next week? Taking a trip next month? Great, it takes just seconds to share each of these plans. Your plans will reach not only your subscribers on Plancast, but your friends on Twitter and Facebook as well, if you so desire. The service a great way to spread the word about the informal social activities you do every week, whether or not you're looking for people to join you.\n\nIt's been an amazingly fruitful journey getting to this point, and things are especially exciting now that the site is finally live and in such good shape. I launched an \"alpha\" version at the beginning of September, but since it was so rudimentary, I only sent it to a handful of people for testing. With this more functional beta version (released just this past weekend), I'm encouraging everyone to check it out and invite some friends along. We're already seeing a broad range of people take a liking to it.\n\nI'm also happy to announce that I've brought aboard [Jay Marcyes](http://marcyes.com/) as a co-founder. Jay is a programming beast, not to mention a thoroughly nice guy, who was previously working full-time on another consumer internet app called [Noopsi](http://noopsi.com/). I'm truly lucky to have him, and I encourage you to get acquainted [on Twitter](http://twitter.com/jaymon). For ongoing updates about Plancast in general, you can follow [the official Twitter account](http://twitter.com/plancast) as well.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "facebooks-social-graph",
    "title": "Facebook's social graph",
    "excerpt": "The social graph on Facebook has been the company's biggest asset, but over time it has become perhaps its biggest liability as well. When users want...",
    "published": true,
    "publishedDate": "2009-07-31",
    "category": "technical",
    "readTime": 2,
    "tags": [],
    "body": "The social graph on Facebook has been the company's biggest asset, but over time it has become perhaps its biggest liability as well.\n\nWhen users want to find their friends online, they think of Facebook first. For many users, \"Facebook\" is nearly synonymous with \"social networking\". They wouldn't think of using any other \"social\" service because, after all, their friends are all on Facebook. As far as the social networking industry is concerned, this dedication constitutes a massive customer lock-in, because no matter how much better you can make a social application, you'll start off not only without the preestablished connections enjoyed by Facebook; you'll also be fighting against the reluctance of Facebook users to try an application outside of the Facebook ecosystem in uncharted territory where most of their friends do not exist.\n\nThe Facebook developer platform (which includes the ability to write widget-like applications for placement on Facebook.com, as well as the ability to extract data about users for integration into applications on other domains) narrows this gap only slightly. For all of Facebook's talk about wanting to open up, its platforms APIs and policies empower third-party developers with only so much data and user access. Compared to the power that Facebook wields as chief overseer of its data and users, outside developers can query just a sliver of its social graph. And of that sliver, they can only store certain data in certain ways for certain periods of time. The restrictions add up so that Facebook integration delivers but minor, complementary benefits to most third-party sites.\n\nTo break things down a bit, the platform can be divided into push and pull components. Many of the APIs are designed to let you pull data about Facebook's users and leverage that data in your applications. Others are designed to let you push data from your application back to Facebook, usually for sharing user activity with friends there. These push mechanisms are the most critical for most third-party developers, because users want to retain contact with their Facebook friends and share activity with them. The data you pull from Facebook about users is generally less interesting, if only because it's pretty generic. Unfortunately, the push mechanisms are pretty weak since they don't let you reliably send data to individual friends of users, whether through Facebook's proprietary messaging system or email notifications. Your best bet is to rather bluntly dump something into the homepage stream and pray that it catches enough friends' eyes to make an impact.\n\nAll of this is to say that Facebook still has a huge competitive advantage over other social networking companies (whether on-platform or off) because it controls a valuable social graph – and particularly the email addresses that come along with it. However, the social graph is not a divinely produced thing. And it's not a permanent, exclusive good. On the contrary, I believe the social graph is deteriorating on Facebook and starting to be reproduced elsewhere in better form.\n\nThe main problem is that people's real-world social graphs change often and automatically, while their virtual representations on Facebook change mostly uni-directionally and manually. In other words, friends come and go in real life; but on Facebook, they usually just come. Friend lists tend to get bloated over time because users have a harder time defriending each other virtually than in real life. And even if they are going to defriend each other virtually, it has to be a deliberative effort, unlike in real-life when you just stop seeing certain people.\n\nThis problem is particularly acute for Facebook, because its earliest adopters were college students or high school students who have undergone significant changes in their lives over the last few years. They no longer see many of the people who they once friended in school. And they aren't inclined to remove these friendships from Facebook because they're lazy, fatigued or simply too polite.\n\nThe ill effects of this discrepancy would have been tempered had Facebook stuck to its original value proposition of static profiles. However, Facebook has undergone a major shift from a static directory to a dynamic communication channel. This shift is embodied by its decision to remake its homepage into a Twitter-like stream of directly published content. When you open up Facebook these days, you're bombarded with little bits of information about your Facebook friends' lives. It's no longer primarily a place to browse people's profiles (and associated photos) like Wikipedia pages.\n\nDon't get me wrong, I love the \"real-time web\" as spawned by Twitter and advanced by FriendFeed. But Facebook has hoisted this dynamic paradigm onto a user base that didn't expect it, didn't ask for it, didn't prepare for it, and perhaps doesn't want it. \n\nI've already [discussed](http://www.techcrunch.com/2009/02/07/why-facebook-isnt-poised-to-steal-twitters-thunder/) why this last factor is such an issue. But assuming the idea of micro-sharing does grow on Facebook users, they haven't established the right audiences for it. Friendships haven't been made on the basis of content consumption; they were made first to simply acknowledge your friends and later to gain access to their profiles (once Facebook opened up for non-students and became a less trusting environment). Sure, the news feed was introduced rather early on and aggregated information about those who users decided to friend. But the inability to post content directly and immediately to all of your friends' news feeds created an important sense of distance between you and them – and made it easier to coexist on the site with those friends who weren't really your friends anymore, or those who you didn't ever care to hear from much.\n\nAs a content producer, my predefined social graph on Facebook makes me reluctant to publish there, because I don't feel as though my friends have indicated an interest to see my constant updates. The problem I have as a content consumer is just the flip-side: when I load up Facebook, I see content produced by people who I don't particularly want to hear about or from. \n\nFacebook has provided various ways to sort friends into lists and hide individuals from your stream, but these tools are daunting and perhaps ultimately futile. I spent 20 minutes alone last night organizing just my friends with first names that start with letters A-C. With almost 800 friends, I'm reluctant to keep going. And I imagine that most Facebook users don't even have the wherewithal to try in the first place.\n\nFacebook may try to address this content audience problem by introducing a Twitter-like follower model. The site already asks you when friending someone new whether you want to see that person's updates in your home stream. But users won't be doing this retroactively, and it adds complexity to an already complex site. Privacy and distribution controls simply aren't going to solve the problems of an over-encompassing social graph.\n\nWhat does this all mean? Well, Facebook's golden goose (the social graph) may not be so golden after all. It changes as users change. And it's not really even a singular thing. People have multiple social graphs; Facebook just tries to roughly represent them all by clumping them together. When it comes to profile access, you may want to leverage a different set of connections than when it comes to status message streams. Facebook may have to make a decision as to which particular social graph it wants to represent for its (constantly growing and diversifying) user base. It may not work for the company to be all things social for all people.\n\nIt also means that there's a massive opportunity for other social sites to give Facebook users a fresh start with fresh new social connections. I'm biased here, of course, since I'm working on social software. But this opportunity is seen in the rise of Twitter, which can attribute much of its success to the mere fact that it's *not* Facebook. When you sign up for Twitter, you can determine anew who you care about - whether that's your new friends or coworkers, or celebrities, businesses and media outlets. Facebook will no doubt remain a dominant social network for quite sometime, but it's dominance does not preclude the rise of other, independent social applications and services.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "worldly-developments",
    "title": "Worldly Developments",
    "excerpt": "I'm happy to announce some exciting news regarding my startup. I've officially incorporated as Worldly Developmentshttp://worldlydevelopments.com/...",
    "published": true,
    "publishedDate": "2009-06-21",
    "category": "technical",
    "readTime": 2,
    "tags": [],
    "body": "I'm happy to announce some exciting news regarding my startup. I've officially incorporated as [Worldly Developments](http://worldlydevelopments.com/) (yay for Delaware). Just a teaser of a website currently, but something I can print on my new business cards.\n\nThe reason for incorporation? I've raised some (micro-)seed funding from [fbFund](http://developers.facebook.com/fbFund.php). Since that requires a legit corporate bank account and the filing of other important paperwork, my mom has also assumed the role of Chief Financial Officer (employee #1! thanks Mom). \n\nFbFund isn't just an investment vehicle; starting this summer, it's also an incubation program in Palo Alto. So while I planned on moving to San Francisco earlier this month, that's been put off until the end of summer so I can enjoy the office space they've provided for us just off University Ave. It's one of the old Facebook offices, and all of the participating startups (~20) started moving their stuff in there just a few days ago.\n\nFrom now until mid-August I'll be participating in the fbFund program, which basically means hacking away at my application as I would anyway, except with additional support/mentorship/resources provided by Facebook and others. I've already met a fair number of the participants and organizers, and I must say, it's a refreshing change of pace to work around like-minded people again instead of coding solo in my bedroom or at the cafe.\n\nI also just got back from a quick trip to Japan and China as part of [GeeksOnAPlane](http://geeksonaplane.com/). Thanks to [Dave McClure](http://500hats.com) for bringing me along as a media partner of sorts (I relayed what we learned as a return guest writer for TechCrunch; see [these](http://www.techcrunch.com/2009/06/14/geeksaplane-briefing-on-the-chinese-tech-industry-at-startonomics-beijing/) [posts](http://www.techcrunch.com/2009/06/10/geeksonaplane-learnings-from-tokyo/) in particular). The people in the traveling group were amazing, both personally and professionally, as were the people we met along the way. If you haven't visited East Asia, I highly encourage you to do so.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "the-web-needs-its-own-app-store",
    "title": "The web needs its own App Store",
    "excerpt": "I've been thinking a bit about the Apple App Store recently, perhaps because I'm starting to think more about developing a mobile application that...",
    "published": true,
    "publishedDate": "2009-05-19",
    "category": "technical",
    "readTime": 2,
    "tags": [],
    "body": "I've been thinking a bit about the Apple App Store recently, perhaps because I'm starting to think more about developing a mobile application that will complement my site.\n\nOne of the amazing things about the App Store – perhaps the most amazing thing – is how easy Apple makes it for users to pay for apps with small amounts of money (micro-transactions, if you will). Are you a developer that wants to sell an application for just $.99 a pop? Easy. Just create your app, submit it to the store, and start earning money. You don't even have to worry about going the advertising route. The strategy instead is primarily about providing a free version of your app that then gets users warmed up for your paid version.\n\nApps in the App Store aren't that different from web apps. They are both internet-enabled at their core and provide rich communication and data retrieval services for their users. Sure, iPhone apps run in Objective C instead of Javascript, etc. But that doesn't matter to the regular user who decides to pay for an app on their iPhone. Both are apps either based, or quickly retrieved, from the cloud.\n\nOne of the problems that most internet startups face is the looming question of \"how the hell am I going to make money on my site even if it becomes successful?\" The internet industry is often mocked because it doesn't readily provide dependable answers, at least not ones that are not easily undermined by general economic factors.\n\nNo web developer wants to deface their sites with Adsense or banner ads. Online advertising is (with few exceptions) anti-consumer and difficult to do \"right\" (as in, design to really capture users' attention and generate modest rates of return). Wouldn't it be great if we could just design sites that sell like iPhone apps?\n\nImagine visiting a site's landing page, getting a description and some screenshots, and then seeing a button that says \"$1.99 for full access\". If, as a user, you could click on that button, enter a password, and then immediately have lifelong access to the site – I bet you'd do it. $1.99 is a price lots of people are willing to pay, even if there's a risk involved with trying out the site and then finding out you don't like it.\n\nI don't know nearly enough to understand why this hasn't happened, even though I've read a fair amount about how online micropayments for sites have been heralded for years but have never materialized. Apple has made things happen on the iPhone because they've created a closed system that makes it really easy for end users and developers alike. As a user, you give your credit card information to Apple once and they only charge it when it makes sense (since charging every micropayment individually would drive up transaction fees).\n\nWhy can't someone do this for the web? It seems to me we just need a company that a) people trust with their credit card information and transactions, and b) knows how to build the right distributed technology that will get adopted by developers. Google comes to mind since it's tight with developers and very trusted among users. Maybe browsers like Mozilla are better situated, however, since they control the \"device\" within which users browse the internet (just as Apple controls the iPhone). Then again, Google has its own shiny new browser (Chrome) so they'd have that going for them as well.\n\nSuch a payment system could revolutionize how websites are monetized - and consequently, how they are made. Businesses and their users would both stand to benefit greatly. Here's to hoping this happens as the web matures.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "narrowing-scope",
    "title": "Narrowing Scope",
    "excerpt": "Since my last post, I've narrowed the scope of my project down a great deal. The main reason was to make it more accessible to first-time users who...",
    "published": true,
    "publishedDate": "2009-05-18",
    "category": "article",
    "readTime": 2,
    "tags": [],
    "body": "Since my last post, I've narrowed the scope of my project down a great deal. The main reason was to make it more accessible to first-time users who would probably otherwise be overwhelmed by the number of things you could do on the site. There are related problems associated with trying to bite off too much from the very beginning:\n\n- It's hard to communicate the immediate value to users, especially ones that join the site before any of their friends\n\n- It's extremely difficult to predict what kind of service users are going to respond to, and which features in particular are necessary. Better to release something straightforward, see how users like it, and evolve by coupling user feedback with theory\n\n- When there's only one developer (me), it can feel overwhelming to build out a complicated system all at once. After awhile, there's a strong desire to launch *something* so it feels like you're making tangible progress. Milestones are key when you're the only one laying out your priorities\n\n- Once you've released something - even if it's only a smaller part of your broader vision - it's easier to explain to other people what you're building. It's no longer just about pie in the sky ideas - you can send them a link, have them see what it does, and then explain where you plan to take it.\n\n- There's an intrinsic value to building something simple. Users don't want to think too hard about how to use applications, and if your application only does one thing - and it does it well - they're more likely to come back.\n\n- When you release a web app these days, you need to market it effectively, otherwise you'll get drowned out by all the other options people have online. One effective way to market is to make your site compatible with the places people already visit, namely social networks like Twitter and Facebook (at least in my case). It's easiest to piggyback off of these sites if you create something particular that enhances them; then you can go from there and develop something more sophisticated that stands on its own.\n\nLast week I launched the narrowed site into private beta, inviting only a dozen or so people. I've already received a laundry list of features/change requests, and I'll start opening the site up more broadly when I feel as though I've addressed them adequately.\n\nI'm also happy to report that I've been chosen as a finalist for fbFund, which affords me some money to advertise on Facebook. More details and a list of the other finalists [here](http://www.techcrunch.com/2009/05/18/facebooks-fbfund-09-names-first-batch-of-winners).",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "the-unfolding-legacy-of-twitter-for-software-design",
    "title": "The unfolding legacy of Twitter for software design",
    "excerpt": "The immense amount of ink spilled for Twitter these days signals two main things. First, that web innovation in general is going through a...",
    "published": true,
    "publishedDate": "2009-03-30",
    "category": "technical",
    "readTime": 4,
    "tags": [],
    "body": "The immense amount of ink spilled for Twitter these days signals two main things. First, that web innovation in general is going through a transformative period, one in which we don't see a lot of breakout technologies because the industry is struggling to redefine itself in the wake of economic collapse and the exhaustion of innovation. Twitter stands out because it's a counter example to this trend, a company that's going mainstream and perplexing people all at the same time. It's a paradigm changer; people are simultaneously obsessed *with it* and confused *by it*.\n\nBoth the confusion and the obsession will pass in time, just as it did for Facebook, the last internet rockstar to emerge before Twitter. Facebook is no longer the buzz maker it was about two years ago and Twitter will no longer intrigue us two years from now. And like Facebook today, it's primary mark on the web landscape will have been made, even though it will remain a powerful force on the web and continue to innovate.\n\nThe making of this mark is the second reason why there's so much attention lavished on Twitter right now, especially by technology pundits who pay constant attention to the effects of breakout services on their peers and descendants. The mark is both simple and profound, and it consists of demonstrating the potency of so-called \"microblogging\" for the distribution of social information.\n\nFacebook may be credited with popularizing the \"news feed\" - a continually updated stream of information about people you care about - but Twitter boiled the news feed down to its essence. On Twitter, the news feed doesn't extract changes from secondary profiles and associated applications. It's not deducing news about your friends by passively monitoring their activity elsewhere, as the Facebook news feed did almost exclusively until very recently.\n\nNo, on Twitter, the users contribute directly into the news feed itself. The news feed is the main feature, not a method of surfacing the most contemporary information in a system. And the content that users add is very basic: simple strings of text no longer than 140-characters in length. Sure, Twitter could have allowed users to post images, movies and other types of data into the feed. But it's creators - partly restricted by the desire for all tweets to be SMS-compatible, and partly influenced by the legacy of blogging - kept things stripped down to their basics.\n\nTwitter remains a stunningly simple application. That's its strength, but the simplicity also creates an opportunity for other services to apply Twitter's model to other ends. Facebook most notably just appropriated Twitter's user experience with the redesign of its homepage. Apparently, Facebook thinks that the Twitter model (combined with the related FriendFeed model) is the best way for friends to exchange information of all types - not only status updates but links, images, videos, and more. And months prior to that, Yammer did something similar for the workplace by releasing an enterprise microblogging service.\n\nThis is just the start. Over the next few years, we are going to see social services across the spectrum appropriate and expand upon the basic functionality of Twitter, because there are needs that Twitter doesn't (and can't) fulfill, either onsite or through its API. Since all software is becoming social, expect the Twitterification of software in general.\n\nWhy is the Twitter way of communicating so powerful - and consequently, why will others borrow from it? Microblogging is passive, it's distributed, and it's easy. In other words, people can digest and respond to tweets as they please. There's no technological or sociological pressure for them to consume or act on information in ways that are disproportionate to their interest level. When you post a tweet, it gets blasted out to many recipients all at once, unlike email which is architecturally designed for a limited audience. And each tweet demands very little from its users - just a simple thought or observation.\n\nSo, Twitter has set the standard. It's currently proving that its model can appeal to mainstream audiences, who actually appear capable of grokking its utility (which was not always a given). But this is just the beginning - just as \"social networking\" features pervade services of all kinds these days, microblogging will also become ubiquitous - and it will assume different forms depending on the various needs at hand.",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "my-first-week-as-an-entrepreneur",
    "title": "My first week as an entrepreneur",
    "excerpt": "Today marks the completion of my first week working on my first startup/getting-the-lay-of-the-land. So, I thought it would be a good time to share...",
    "published": true,
    "publishedDate": "2009-03-22",
    "category": "technical",
    "readTime": 4,
    "tags": [],
    "body": "Today marks the completion of my first week working on [my first startup](/getting-the-lay-of-the-land). So, I thought it would be a good time to share some of the things I've learned already about what it's like to build your own web service from the ground up and as a full-time job. \n\nI'm going to try to provide this type of update on a regular basis for two main reasons: 1) so others can get a sense of what it's like to start a dot com these days, especially if they are thinking about doing it themselves; and 2) so I have a record to look back on later to see just where I've been and how my thoughts may have changed along the way.\n\nWhen I decided to strike out on my own, I expected that the hardest thing would be to keep up morale, both for myself and for anyone who joined me. It seems as though a lot of startups wither away because their founders give up hope on their ambitions. The threat of demoralization appears the greatest for startups that are trying to do something really new and innovative, something for which the market can't even indicate demand yet. Even with less radical ideas, there are always naysaying thoughts nagging away at the back of the entrepreneur's mind; for example, \"if this was such a good idea, why isn't someone else already doing it?\", \"if this were *even possible*, wouldn't others be already doing it?\", \"hasn't X and Y companies already kind of tried this?\", and \"couldn't company Z easily move into this space and wipe out the market opportunity for my fledgling startup?\".\n\nEven though it's only been a week, these are thoughts that I have had to handle carefully. On the one hand, it's necessary to constantly question the core value proposition of your startup. If you don't, then you're bound to build something that people don't need or want. Whether I'm in the shower or walking down the street, I'm frequently turning ideas over in my head, looking for weaknesses in them, and trying to expose poorly made assumptions that could turn into Achilles' heels. \n\nOn the other hand, a startup founder needs to be stubbornly optimistic, lest he or she succumb to the overwhelming number of (legitimate and illegitimate) doubts that may arise. As [the debate](http://www.techmeme.com/090322/p4#a090322p4) from this week over Zuckerberg's decision to look beyond user feedback brings into focus, its important for a founder to say \"yes\" even when (many) others say \"no\". If the man on the street dictated product design, [we'd all be driving Volvos](http://scobleizer.com/2009/03/21/why-facebook-has-never-listened-and-why-it-definitely-wont-start-now/). Great ideas (perhaps by definition if not just in general) shouldn't be easily appreciated until they've been executed, and sometimes not even in the short-run after they've been executed. An entrepreneur needs to internalize this belief and learn how to endure resistance and skepticism from those who don't readily share it.\n\nCurrently, I have a user base of just one, so I don't have to defy millions of faithful users when making product decisions. Mostly doubts arise when I've shared my ideas with friends, family and basically anyone willing to listen. Some of the time people get the value proposition instantly and it clearly resonates with them. It's immensely satisfying (despite what I just said above) when people respond with \"wow cool, that sounds exactly like something I would use.\" \n\nOther times, people scratch their heads and almost reflexively assume the role of devil's advocate: \"so this is kind of like a cross between X and Y websites...right?\" or \"shouldn't this just be a facebook app?\" (the modern day equivalent of calling someone's website idea a gizmo). It's important to listen to those who still need convincing, because there will be a lot more of them and their concerns usually inform your concerns. But it's equally important not to let their skepticism deflate your enthusiasm for the project. I've found that the best way to reassure myself in these situations is to think about what it might have been like for the founders of Google, Facebook, or Twitter to sit down at the beginning of their projects and get feedback from friends (\"why do we need another search engine?\", \"why would I want to put my personal information online and then tell a site who my friends are?\", and \"who cares if I'm brushing my teeth or watching the basketball game?\").\n\nKeeping up morale is particularly important when you're flying solo, because you don't have a cofounder who will constantly reassure you of your decision to follow a path to no certain end. Also, when you're at a regular job, you may not know if the company will succeed but you can be fairly certain that you will succeed in your delegated role. Your projects are usually well-defined and limited in scope, and as long as you get them done well, you have something to put on your resume and feel good about when you head home at night. \n\nBut when you're starting a company, the goals are not defined for you and you're wrapped up in the success or failure of the enterprise as a whole. If you spend two years working on a startup that ultimately falls through, I imagine it's a much greater personal burden than spending two years working at someone else's company that goes belly up.\n\nSo far, morale has been good for me. I've already experienced a bit of the \"rollercoaster\" effect that I've heard others describe, where emotions swing from high to low, and back to high – sometimes on an hourly basis. But my project is getting increasingly more exciting as it develops, even though the first weeks are all about baby steps (drawing mockups, conducting general research, checking out developer documentation, hacking together the first pieces of code, etc.).\n\nThere are also definite benefits to working for yourself - and from home, as I currently am. There's none of that (often unnecessary) pressure to please anyone but yourself, and I suffer less from *unhealthy* stress. Sometimes it feels a bit like I'm on vacation, but then I remember that I'm actually working longer and on weekends now. It just feels like vacation because the work is thoroughly pleasurable, at least so far. I also don't have to deal with the formalities of a normal job, like going into an office or taking breaks only when it makes sense for the organization as a whole. When you work for yourself, you can roll out of bed and immediately start getting stuff done. And if you feel like 2:30pm is the perfect time to take a break and hit the gym, you can do it without feeling like anyone's judging you for leaving in the middle of the day.\n\nSo that, in a rather large nutshell, is what I've experience so far. The prototype (codenamed \"Magellan\" - thanks [Jason](http://jasonnazar.com)) is coming together, and I hope to have the first version ready to share with friends and family by sometime in June. I also plan to start inviting people off the waiting list before soon after that.\n\nFor related reading from someone with much more experience than me, check out Paul Graham's essays, especially [this one](http://www.paulgraham.com/13sentences.html).",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  },
  {
    "slug": "getting-the-lay-of-the-land",
    "title": "Getting the lay of the land",
    "excerpt": "As Erick Schonfeld was kind enough to announce in a post on TechCrunch last week, I've left my position at TechCrunch to build and run my own startup.",
    "published": true,
    "publishedDate": "2009-03-17",
    "category": "technical",
    "readTime": 4,
    "tags": [],
    "body": "As Erick Schonfeld was kind enough to announce in [a post on TechCrunch](http://www.techcrunch.com/2009/03/10/hendrickson-were-gonna-miss-you/) last week, I've left my position at TechCrunch to build and run my own startup.\n\nIn many ways, this is what I intended to do all along, at least ever since I graduated from college almost two years ago. After joining TechCrunch in 2007 (as an intern anticipating that I'd only be there for a couple of months), I quickly realized that it would be a great place to lay the groundwork for my own venture. Working there keeps you in constant connection with the consumer internet technology scene, since you're always reading news and analysis (especially as a writer) and meeting people from all parts of the industry (PR, management, investment, development, etc.). Of course, it doesn't hurt to work for a brand that has great name recognition in the Valley, especially when you're returning from four years in Maine - basically Siberia, as far as people around here are concerned.\n\nI learned *a lot* about new media, internet technology, the culture around internet technology, and the inner workings of a startup during my time at TechCrunch - lessons that perhaps I'll explore in a later post. But now my sights are set on building a viable web service (and later, a profitable business) in a down economy...something that with any luck won't fall to [the wayside](http://www.techcrunch.com/tag/deadpool/) like so many of the startups I witnessed at TechCrunch. It's not going to be easy; in fact, I'm sure it's going to be one of the toughest things I ever try to pull off. The reassurance is that even if I fail, I will have learned and experienced much along the way.\n\nOk, so enough sappy reflection and introspection. What am I actually trying to build? Or as my friends and family keep asking, \"What's your website about?\"\n\nLet me start to answer that question with a description of how *the idea* for my startup came about. When I moved back to the Bay Area after living in a tightly knit community at Bowdoin, I had a new set of needs - most of them social. And like many needs, they could only be fulfilled by gathering information, not just once but on a continual basis. For example, I wanted to know:\n\n- Which of my friends now live in the Bay Area, and where exactly do they live and work? Do any of them live together?\n- Who do my friends know in the area that I might like to know? Do they live near me, and what kind of people are they looking to meet?\n- Where do my friends and their friends like to hang out on the weekends? Where do they go out to eat, and where do they do other things like going to the gym or perhaps volunteering in their free time?\n- What kind of plans do my friends and their friends have coming up in the near future? Are they thinking about going somewhere fun in San Francisco, or do any of them plan to go running around Palo Alto?\n- Are any of my friends actively involved in particular interest groups? Maybe a few get together with others to, say, play tennis or hit up the farmers market every weekend?\n\nQuestions like these are just begging to be answered by web services - especially by the type of those we've seen sprouting up in the past few years - because they all call for social information. Unfortunately, no web service adequately answered them in 2007, and still none does today. Sure, we have a plethora of sites intended to help you figure out what to do and where to go in your area. But those with the most data are not personal enough (i.e. they don't help you see the world through your existing connections), and those that *are* personal lack data, and the proper architecture for that data.\n\nSo, on a high level, I've set out to build a service that will answer the questions above and many others, a service that will help you engage more actively in your community. Call it a city or location-based social network if you want, but hopefully you'll see that those terms tend to misrepresent what I have in mind. I'm not looking to set up a site where you simply post a profile for others around you to view and write things on. I'm looking to set up a site that makes it easy for you to share information about who and what you know, and what you do, around the area in which you live. And conversely, a site that makes it uber-easy to digest useful local information shared by others.\n\nNo service does this to my satisfaction yet, but there are many related sites out there. After all, the desire to meet people and learn about what's going on around you isn't new. Here's a list of the names currently scribbled on my whiteboard:\n\n- **Loopt, Brightkite, et al.** - Services that detect your current location via a mobile device and then broadcast that location to your friends are all the rage right now. Perhaps we'll encroach more on each other's territory down the line, but I don't really care about helping users find out that their buddy is in the bar next door. I care more about providing you with social information from and about the area in which you live.\n\n- **Yelp, Goodrec, Citysearch** - Local review sites are great since they have a ton of information. But unfortunately, the information comes mainly from the public at large. Goodrec is a step towards personalization and simplification, but reviews and recommendations need to be even more socially focused (Whrrl had the right idea but [didn't execute successfully](http://www.pelago.com/blog/announcements/2009/03/whrrl-v20-has-arrived/).\n\n- **Facebook, MySpace, TheScene** - \"Traditional\" social networks, no matter how innovative, define themselves broadly. They aren't interested, for the most part, in local discovery. Look at how Facebook abandoned network pages. And new sites like TheScene ostensibly help you go local but simply aren't innovating much in how people publish and share information (for this, just look at the ripples that a deceptively simple service like Twitter has made).\n\n- **Match.com, Okcupid, Mixtt, Engage, etc.** - Dating sites are still holding down the fort when it comes to local discovery services. One problem - they suck, and they only serve one particular need (ok ok, it's a good need to serve, but even that need can be served better). It's great to see sites like Engage and Mixtt try to innovate by making things more social, but so far their efforts haven't worked out all that well.\n\n- **MeetUp, Upcoming** - Sites that help you meet up with interest groups and attend local events. Good. But who are all these strangers?\n\n- **Outside.in and other local news sites** - Local news is also good, but if there's anything that's easy to find online, it's news. And local news is often far more boring than national news, so it's an uphill battle to build a service that just revolves around this.\n\n- **Craigslist** - Amazingly great and amazingly bad at the same time. I'd like to think that this site isn't a testament to how the last 10+ years of web technology advancements don't matter when it comes to local classifieds.\n\nThose are the services on my mind as I start the process of creating something new and improved. What did I miss?",
    "createdDate": "2026-01-19",
    "updatedDate": "2026-01-19"
  }
]