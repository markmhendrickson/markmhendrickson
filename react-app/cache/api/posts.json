{
  "url": "https://markmhendrickson.com/api/posts.json",
  "posts": [
    {
      "slug": "six-agentic-trends-betting-on",
      "title": "Six agentic trends I'm betting on (and how I might be wrong)",
      "excerpt": "The structural pressures that underpin my work, and what would invalidate them as the AI industry evolves.",
      "body": "Everyone involved in AI right now is, implicitly or explicitly, trying to predict where things are going and how those changes will reshape our lives and work. The volume of speculation is enormous, and much of it is contradictory. That is unavoidable. No one can know with confidence what the next couple of years will bring. The space is moving too quickly, the interactions between technologies are too complex, and second-order effects dominate in ways that are difficult to model ahead of time.\n\nStill, if you are operating in this space, especially if you are building something with AI or for AI, it is not enough to remain agnostic. You have to choose a set of core theses about how the world is likely to evolve and build coherently around them, knowing that some will be wrong and others will matter more than expected. These theses are less about precise prediction and more about identifying structural pressures that seem unlikely to reverse.\n\nWhat follows are the central assumptions I am currently operating under. They are not claims about inevitability, and they are not meant to cover every possible future. They are the trends that, if they continue even partially, shape how I think AI systems will be used, where friction will accumulate, and what kinds of infrastructure will become necessary. My work ([Neotoma](/posts/truth-layer-agent-memory), a truth layer) is best understood as a response to these assumptions. It is not the reason for them, but it is built in anticipation of the world they imply.\n\n---\n\n## 1. Agents will become stateful economic actors\n\nOver the next two years, agents are likely to move beyond assistive, prompt-centric interactions and become meaningfully stateful actors. No breakthrough in general intelligence is required. Cheaper inference, more capable tool APIs, and broader tolerance for agents running unattended are enough.\n\nThe societal shift is real. We are used to tools that do nothing until we act. When agents persist goals, coordinate with each other, and take irreversible actions over time, the question of who is responsible becomes harder to answer. More work is delegated to non-human actors; [the boundary](/posts/we-are-all-centaurs-now) between \"I did this\" and \"my agent did this\" softens. Norms around trust, liability, and dependency will have to adapt. The technology enables the change; society has to decide how to live with it.\n\nWhy is this trend likely? The marginal cost of keeping agents alive is collapsing faster than the cost of rebuilding context. As inference gets cheaper and orchestration matures, it is more efficient to persist an agent's state than to reconstruct it from scratch. Tool APIs increasingly assume continuity: credentials, caches, intermediate artifacts. Persistence is rewarded over statelessness.\n\nIn that world, memory ceases to be a convenience feature. It becomes part of the system's state, comparable to a database rather than a chat log. When that state is proper and trustworthy, new things become possible at scale: long-horizon plans that span days or weeks, coordination across many agents and tools, and delegated work that is only feasible when state can be trusted and extended over time.\n\nNeotoma is built for that. It treats memory as explicit system state: typed entities, events, and relationships in a deterministic graph, not prompt residue or embedding similarity. An agent's history can be replayed, inspected, and reasoned about as part of the system itself.\n\nWhat to watch over the next year:\n1. Agent frameworks advertising long-running, background, or resumable execution as a core feature.\n2. Teams discussing agent state corruption or drift as a distinct bug class rather than restarting agents as a fix.\n3. Product interfaces exposing agent history as something inspectable rather than ephemeral.\n4. Teams running multiple agents that need a single source of truth for entities and decisions.\n\n---\n\n## 2. Agentic errors will become economically visible\n\nAs AI output increasingly flows directly into billing, compliance, client deliverables, and automated workflows, the cost of errors is likely to shift. What is currently a diffuse inconvenience becomes explicit economic impact.\n\nWhen errors start to show up in postmortems, contracts, and court filings, society gains a sharper picture of who bears the cost and who gets blamed. Organizations will face pressure to prove how decisions were made and what the system knew at the time. That pressure will ripple into professional norms, insurance, and regulation. Individuals and small teams may be held to standards that were originally designed for large institutions with audit trails. The upside is more accountability and fewer silent failures. The downside is that the bar for \"explainable\" and \"auditable\" may rise faster than many are ready for.\n\nThe structural reason this trend is likely is that AI is moving closer to decision-making edges, not just advisory layers. As AI output becomes embedded downstream in systems that trigger payments, commitments, or external communication, errors inherit the cost structure of those systems. Organizations cannot continue treating failures as \"model quirks\" once they propagate into irreversible actions.\n\nToday, mistakes are often shrugged off with regeneration or prompt tweaks. Tomorrow, those same mistakes will waste money, damage reputation, or create legal exposure.\n\nWhen errors become priced, organizations stop asking whether outputs were helpful. They start asking how those outputs were produced, what information they relied on, and whether the process can be replayed or audited.\n\nAs a corollary, tolerance for approximate or ambiguous memory erodes. The bar for what counts as good enough rises first where harm is visible, then that standard drifts outward. Once mistakes are costly, memory that you can correct and trace becomes infrastructure, not a convenience.\n\nNeotoma aligns with this shift by enforcing provenance at the memory layer. Facts are stored with source attribution, timestamps, and ingestion events. Corrections are additive rather than destructive, allowing teams to reconstruct exactly what an agent knew at the time of a decision instead of guessing based on partial logs.\n\nWhat to watch over the next year:\n1. AI-related failures appearing in postmortems, client disputes, or legal contexts.\n2. Teams explicitly asking \"what did the agent know at the time?\" after mistakes.\n3. Traceability or audit requirements being added to AI workflows retroactively.\n4. Public incidents attributed to AI memory errors; language shifting from \"hallucination\" to \"system failure\" in postmortems.\n5. Teams asking for \"undo this fact\" or \"revert what the agent believes\" without full resets.\n6. \"What does the system believe and how has it evolved?\" framed as a query over a consistent graph rather than a RAG call.\n\n---\n\n## 3. Audit and compliance will drift down-market\n\nA related trend: the pressure to prove how work was produced and what the system knew will not stay confined to large enterprises. Wherever errors carry a real cost—economic, legal, or reputational—the demand for defensibility and record-keeping follows. As AI becomes embedded in professional work, consultants, agencies, regulated freelancers, and small AI-native teams will face the same expectations.\n\nThe structural reason is liability diffusion. As AI use becomes normalized, responsibility does not disappear. It spreads. Clients, insurers, and regulators respond by seeking compensating controls. Audit pressure moves down-market not because small teams want it, but because risk follows usage.\n\nOnce questions about how work was produced become routine, memory without provenance becomes a liability rather than a convenience. Structured timelines, entity-level recall, and source attribution start to function as defensive infrastructure.\n\nNeotoma aligns with this shift by treating memory as something that can be reconstructed in time rather than inferred retrospectively. Entity resolution, temporal ordering, and provenance are not add-ons. They are core to the model.\n\nWhat to watch over the next year:\n1. AI usage disclosures appearing in contracts, statements of work, or professional guidelines.\n2. Requests for documentation of AI-assisted decisions from clients or insurers.\n3. Individuals or small teams proactively storing AI interaction records defensively.\n4. Regulation that explicitly requires record-keeping or explainability for certain AI uses.\n\n---\n\n## 4. Platform memory will remain opaque\n\nLarge AI platforms are likely to continue shipping memory features that are useful but fundamentally opaque. Their incentives favor engagement, retention, and model optimization rather than user-controlled provenance or guarantees of correctness.\n\nThe societal effect is a split between those who can afford to care and those who cannot. People and organizations that need strong guarantees (audit, correctness, portability) will either pay for alternatives, build their own, or accept risk. Everyone else will rely on platform memory and live with the trust gap. That divide can reinforce existing inequalities. The well-resourced get transparent, portable memory; everyone else gets convenience with opaque terms. Over time, norms about what \"my data\" and \"my history\" mean may diverge by context and by who you are. Civic and professional expectations (e.g. that you can show your work or export your records) may apply only to some.\n\nThe structural reason this persists is incentive misalignment. Platforms optimize for aggregate outcomes across millions of users, not for the correctness guarantees required by any individual workflow. Exposing memory semantics, correction rules, or replay guarantees constrains iteration speed and increases liability. Opaqueness is not accidental. It is protective.\n\nMemory may improve, but it will remain difficult to inspect, export, replay, or reason about formally, especially across tools. Corrections will often be silent, implicit, or model-specific.\n\nThis creates a growing trust gap. Users may rely on platform memory for convenience while simultaneously distrusting it in contexts where consequences matter.\n\nData sovereignty adds a separate pressure: enterprises and individuals are increasingly insisting that agent memory stay in their environment, either on-prem, in their tenant, or under their control, rather than in a vendor's cloud.\n\nNeotoma is built for that gap. Its local, inspectable, user-controlled design is the alternative for workflows where correctness and provenance matter. You own the data and the semantics; you can export, correct, and reason about what the system knows.\n\nWhat to watch over the next year:\n1. Memory features that improve recall but stay undocumented or non-exportable.\n2. Users asking what the system actually knows – such as a comprehensive view of what it believes, remembers, and has inferred, not just raw chat or exports – and getting no clear answer.\n3. Workarounds (e.g. exports, third-party sync, manual replication) growing rather than shrinking.\n4. RFPs or requirements specifying that agent memory must stay on-prem or in the user's tenant.\n\n---\n\n## 5. Tool fragmentation will persist\n\nDespite recurring narratives about consolidation into a single AI platform or workspace, knowledge work is likely to remain fragmented. Professionals already operate across multiple models, editors, copilots, document systems, and agent frameworks.\n\nThe structural reason is that AI tools are complements, not substitutes. Each optimizes for a different part of the workflow: ideation, execution, coding, retrieval, communication. Marginal improvements do not collapse the stack. Low switching costs and rapid model iteration further discourage consolidation.\n\nAs tool sprawl increases, the core problem shifts from interface fragmentation to state fragmentation. Context lives in too many places at once, and no single surface can realistically own it.\n\nNeotoma sits beneath this fragmentation rather than trying to resolve it. By exposing memory through a protocol interface rather than a single UI, it allows multiple tools and agents to read from and write to the same underlying state without forcing convergence on a single workflow or vendor.\n\nWhat to watch over the next year:\n1. Professionals switching models or tools mid-task without migrating context cleanly.\n2. Repeated complaints about \"losing context\" between tools.\n3. Teams standardizing workflows that explicitly span multiple AI products.\n\n---\n\n## 6. Agentic usage will become metered\n\nAgent execution is also likely to become increasingly constrained by cost. The structural reason is straightforward: compute is becoming a visible line item. No radical economic restructuring is required.\n\nAs AI spend grows, organizations introduce budgeting, attribution, and optimization. Once costs are visible, metering follows naturally.\n\nWhen usage is priced, inefficiency and drift stop being abstract concerns. Recomputing context, misremembering prior decisions, or repeating work becomes visible waste.\n\nNeotoma's deterministic memory model becomes relevant here because it separates durable memory from transient context. By enabling replay instead of regeneration, it treats memory as an optimization surface rather than a side effect of inference.\n\nWhat to watch over the next year:\n1. Teams tracking agent or model usage costs per task or workflow.\n2. Budget-aware agents that alter behavior based on remaining spend.\n3. Optimization efforts focused on reducing redundant inference rather than improving prompts.\n\n---\n\n## How these trends impact key demographics\n\nThese trends act as activation conditions for distinct impacted demographics. Neotoma does not become important through persuasion. It becomes important when reality removes alternatives.\n\n**AI-native individual operators and high-context knowledge workers** are the first: founders, consultants, researchers, and solo builders using AI deeply across thinking and execution. Adoption is gated by stateful agents, economically visible errors, and dissatisfaction with opaque platform memory. Once outputs matter externally (to clients, collaborators, or revenue), the inability to answer \"what did the system know when this was produced?\" becomes untenable. Neotoma becomes attractive as a personal system of record that can coexist with multiple tools.\n\n**AI-native small teams and hybrid product or operations teams** are the second. Individuals can compensate for fuzzy memory. Teams cannot. Once each person's agents remember slightly different facts or assumptions, coordination costs compound. Tool fragmentation accelerates this, audit pressure legitimizes shared memory, and metered usage converts drift into budget waste. In this environment, Neotoma functions less as a productivity layer and more as shared cognitive infrastructure.\n\n**Developer integrators and AI tool builders** who embed agents into products or platforms are the third. For them, memory failure is a production failure. As agents become autonomous, opaque recall becomes untestable and unacceptable. When memory errors are reframed as system failures rather than quirks, builders begin looking for memory primitives that behave like databases rather than conversations. Neotoma becomes relevant here as a substrate, not a feature.\n\nAcross all these demographics, adoption is conditional and stepwise, not hype-driven.\n\n---\n\n## What would falsify this view\n\nAny serious vision of the future should be falsifiable. Without clear signals that would prove it wrong, it is not a thesis but a belief. This matters directly for product strategy, because building toward a future that does not materialize leads to elegant irrelevance rather than adoption.\n\nThe most significant falsifier would be large AI platforms delivering memory that is genuinely portable, inspectable, replayable, and trusted across tools. Not memory in a marketing sense, but memory that is user-owned, exportable, semantically explicit, and stable across contexts. If platform-native memory becomes authoritative in practice (meaning users and organizations trust it as the canonical record of what was known and when), the need for an external truth layer collapses. In that world, Neotoma's core differentiation erodes rather than compounds.\n\nA second falsifier would be meaningful consolidation into a single dominant AI workspace that owns execution, memory, and tooling end-to-end. If fragmentation pressure disappears because one surface successfully absorbs the stack, the leverage of shared memory substrates declines sharply.\n\nA third falsifier would be agents remaining short-lived, tightly supervised, and cheap to reset, with failures continuing to be handled primarily by restarting rather than diagnosing state. If long-running agents do not materialize and resetting remains the dominant recovery strategy, deterministic memory remains optional rather than necessary.\n\nFinally, if audit and liability pressure fail to move down-market (if AI remains advisory rather than consequential for most professionals), then provenance-heavy memory remains overkill for longer than anticipated.\n\nWatching for these counter-signals is as important as watching for confirmation. They provide early warning that the assumptions driving adoption are weakening and that strategy should adapt accordingly. A vision that cannot be falsified cannot be corrected, and a product built on such a vision risks becoming well-designed for a world that never arrives.\n\n---\n\n## Memory as critical, open infrastructure\n\nThis is not a prediction that the world becomes more philosophically committed to truth or correctness.\n\nIt is a prediction that agents become stateful, errors become expensive, platforms remain opaque, tools remain fragmented, audit pressure spreads, and usage becomes priced.\n\nIf even part of this trajectory holds, memory stops being a UX feature and becomes infrastructure that is necessarily open. In that world, systems that treat memory as deterministic, inspectable state are no longer visionary. They are simply the cheapest way to keep complex systems from failing in opaque and irrecoverable ways.\n\nNeotoma is not the driver of that change. It is one plausible response to it.\n",
      "published": true,
      "publishedDate": "2026-02-18",
      "category": "essay",
      "readTime": null,
      "tags": [],
      "createdDate": "2026-02-18",
      "updatedDate": "2026-02-18",
      "summary": "- Agents will become stateful economic actors and memory will become system state, enabling long-horizon plans and coordination at scale.\n- Agentic errors will become economically visible and tolerance for approximate memory will erode as the bar for defensibility and audit rises.\n- Audit and compliance will drift down-market and the pressure to prove how work was produced will reach consultants, agencies, and small teams.\n- Platform memory will remain opaque and a trust gap will grow between those who need guarantees and those who rely on platform convenience.\n- Tool fragmentation will persist and state fragmentation will matter more than interface fragmentation, with memory as a protocol beneath it.\n- Agentic usage will become metered and deterministic memory will enable replay over regeneration, turning memory into an optimization surface.",
      "shareTweet": "Six trends I'm betting on for the agentic future. Key takeaways in the post—and what would prove me wrong. https://markmhendrickson.com/posts/six-agentic-trends-betting-on",
      "heroImage": "six-agentic-trends-betting-on-hero.png",
      "heroImageStyle": "keep-proportions",
      "alternativeSlugs": [
        "agentic-future-betting-on"
      ]
    },
    {
      "slug": "we-are-all-centaurs-now",
      "title": "We're all centaurs now",
      "excerpt": "The humanness isn't in typing every word. It's in deciding what's worth saying.",
      "published": true,
      "publishedDate": "2026-02-14",
      "category": "essay",
      "readTime": 6,
      "tags": [
        "ai",
        "writing",
        "centaurs",
        "build-in-public"
      ],
      "body": "Last spring, I committed what felt like a professional transgression. As a general manager at a crypto startup, I used Cursor to prototype a token detail screen—something that had been sitting in our backlog for months. Within an hour, I had a working demo. The UI was janky, it didn't conform to our design system, but it *existed*. And that existence felt significant.\n\nI sensed skepticism from my team. The feedback I perceived, spoken and unspoken, was that I'd broken process. Skipped important steps. The prototype showed something, sure, but it didn't represent proper team-wide thinking. It felt like they saw it as a curiosity, not a contribution.\n\nI kept going anyway. I built a small project to generate content and documentation about tokens and asset classes, then integrated it directly into our web app as tooltips and links. This time it wasn't just a prototype—it was production code that real users would interact with. And this time, the resistance felt more pointed.\n\nI perceived that people thought I had no place pushing code, let alone AI-generated code. I was using a \"black box\" to do work that shouldn't be delegated to machines, at least not by a non-\"engineer\". The word that kept surfacing in my mind was *irresponsible*. I felt like I was using a cheat code, and worse, like I might not even know enough to understand why it was wrong.\n\nHere's the thing: I was the GM. I had the authority to push that work through. But I couldn't shake the feeling that I might not have been able to do it without that authority. And I spent months questioning whether I'd done the right thing.\n\n## The Vindication\n\nThat was April and May of 2025. This is February 2026.\n\nIn the intervening months, something shifted. AI-driven coding went from suspicious novelty to industry standard. The discourse moved from \"Is this as good as humans?\" to \"How do we manage systems with superhuman capabilities?\" The tooling improved, the models advanced, but mostly, people just… tried it. And realized it worked.\n\nMy intuition was entirely vindicated. What I'd discovered wasn't a shortcut—it was a different mode of operation. The low-level details I'd been criticized for not writing myself turned out to be exactly the kind of work that *should* be delegated. Because delegating them freed me to work at a higher level of abstraction, to think more strategically, more creatively.\n\nIt's not that different from managing a team. When you lead people, you don't write every line of code yourself. That doesn't make you less creative—it makes you *more* creative, because you're spending your cognitive resources on questions of design, strategy, direction, and most importantly, philosophy.\n\n## History Repeating\n\nNow I'm working on a new startup. I'm building a product, developing a platform, and cultivating a public voice again. And I'm using AI to write blog posts, to express myself, to publish actively.\n\nLast week, a friend shared feedback on one of my posts. Something in it made him feel like it was AI-generated. He described his reaction as a \"brain itch\"—that moment of recognition that pulls you out of the content. He sent me [a link](https://www.0xsid.com/blog/aidr) arguing that all writing should be \"organic\"—handwritten, unprocessed, preserving what a person actually thinks.\n\nAnd immediately, I felt it again. That same self-doubt. That same shame. Maybe I *am* short-circuiting something essential. Maybe the creative element is lost when I'm not the one writing every sentence. Maybe I'm using another cheat code.\n\nBut then I stopped and thought about how I actually write these days.\n\n## The Real Process\n\nMy writing doesn't start with fully formed ideas waiting to be transcribed. It starts with the contours of interests and questions. When something provokes my curiosity, I open a conversation with an AI agent. I ask it to help me analyze the concept. I load an article and ask for a summary, then Q&A it, bouncing between the source material and the conversation. I ask for corrections, for synthesis, for reports.\n\nThis is a learning process. A powerful, leveraged learning process. And that report or analysis—that's essentially a blog post to myself. The jump from there to public expression is smaller than you'd think. I just need to transform it so someone without my initial context can access both the topic *and* my developed viewpoint.\n\nSo I work with the agent to convert the analysis into a draft. I iterate on phrasing, positioning, structure. I ask for candidates and choose between them. I provide style guidelines and refine them over time. The exact wording often isn't what I first came up with. But the ideas are mine. The judgment is mine. The direction is mine.\n\nAnd crucially: I'm writing *because* I can do this quickly. I'm running a one-person startup. The difference between five hours and one hour on a blog post is four hours I can spend building product. Without AI assistance, I wouldn't be blogging at all—or I'd be blogging far less.\n\nIt's the same trade-off as last year: existence versus non-existence. Something good enough that gets out there versus something perfect that never happens.\n\n## The Pattern\n\nI think we're going through for writing what we went through for coding last year. The same cultural moment. The same questions about authenticity and responsibility. The same anxiety about what makes something \"human.\"\n\nAnd I suspect this pattern will repeat as AI penetrates more domains. Each time, we'll question whether we're losing something essential. Each time, we'll discover that what we thought was essential—the low-level execution—was actually just what was *possible* for us to do. And that when we delegate it, we free ourselves to work at the level where human creativity actually lives: meaning, values, judgment, direction.\n\nThe humanness isn't in typing every word. It's in deciding what's worth saying.\n\n## The Embrace\n\nThis doesn't mean anything goes. I'm not arguing for putting out work you don't approve or oversee correctly. But there's a lot of subjectivity in what \"correctly\" means. And especially in a startup mentality, the risk of publishing something AI-assisted and imperfect is usually lower than we think. The risk is that you harm your reputation. But the upside is that you're iterating toward quality and authenticity faster than if you'd waited for perfection.\n\nEvery piece you create with AI gets you closer to understanding how to channel yourself through the technology more effectively. The words might not all be yours, but the voice can be. And increasingly will be, as you develop confidence in directing these tools.\n\nWe need to embrace the cyborg nature of this moment. Not retreat from it. Not treat it with wariness. But develop real confidence in our ability to guide these systems as extensions of ourselves.\n\nWe're all [centaurs](https://youtu.be/N5JDzS9MQYI?si=4ZARzcn5aPqnDeZH) now. Half human, half AI. The question isn't whether to accept that—the integration is already happening. The question is whether we'll do it proactively, with intention, channeling our values and judgment through these tools. Or whether we'll do it reluctantly, apologetically, always wondering if we're cheating.\n\nI spent months last year questioning my intuition. I'm not doing that this time. The work I'm putting out is work that reflects my thinking, serves my goals, and wouldn't exist without this partnership. That's enough.\n\nThe future isn't about preserving some notion of pure, unassisted human creativity. It's about becoming fluent in a new mode of creative expression—one where the human contribution is strategic direction rather than tactical execution.\n\nAnd that, it turns out, is exactly where human creativity has always lived anyway.\n",
      "heroImage": "we-are-all-centaurs-now-hero.png",
      "heroImageStyle": "keep-proportions",
      "heroImageSquare": "we-are-all-centaurs-now-hero-square.png",
      "createdDate": "2026-02-14",
      "updatedDate": "2026-02-14",
      "summary": "- Using AI to prototype and ship code as a non-\"engineer\" GM felt like a transgression in 2025, but that mode of work is now industry standard.\n- Delegating low-level execution to AI frees you to work at a higher level: design, strategy, direction, and philosophy.\n- The same cultural anxiety about authenticity is playing out for AI-assisted writing as it did for coding.\n- The human contribution is meaning, values, judgment, and direction, not typing every word.\n- Embracing the centaur model (half human, half AI) with intention beats doing it reluctantly or apologetically.",
      "ogImage": "og/we-are-all-centaurs-now-1200x630.jpg"
    },
    {
      "slug": "agentic-wallets-mcp-bitcoin",
      "title": "A Bitcoin wallet MCP server for L1 and L2",
      "excerpt": "The future of crypto wallets is agentic. Wallets that expose execution surfaces let agents monitor, reason, and execute within policy while you keep sovereignty and approve only what exceeds your limits.",
      "published": true,
      "publishedDate": "2026-02-09",
      "category": "technical",
      "readTime": 6,
      "tags": [
        "agentic-wallets",
        "mcp",
        "bitcoin",
        "neotoma",
        "truth-layer"
      ],
      "body": "This weekend I pulled together an MCP server for a Bitcoin wallet: tools that AI agents can call over the Model Context Protocol. The [repo](https://github.com/markmhendrickson/mcp-server-bitcoin) exposes 93 tools across Layer 1 and Layer 2. One mnemonic drives both.\n\nI was previously general manager of [Leather](https://leather.io), a crypto wallet that also supports Bitcoin and Stacks. At Leather I saw that human-facing self-custody wallets mostly reached people willing to absorb the attention and complexity (e.g. degens and developers). That meant key hygiene, fee awareness, confirmation flows, and the rest. The cognitive load kept the real addressable market narrow.\n\nAgentic wallets change that. When the primary interface is agents that reason and execute within policy, the user approves only what matters. The friction drops and the set of people who can practically hold their own keys grows.\n\nSame two chains. Different surface.\n\n## What the server exposes (L1 and L2 in one surface)\n\nThe server is a single MCP process. Clients send tool names and JSON arguments over stdio and get back structured results. Destructive actions (sends, sign-and-broadcast, deploy) support `dry_run` and do not broadcast by default. The server never returns keys or mnemonics.\n\n### Layer 1 (Bitcoin)\n\n**Core Bitcoin:**\n\n- Address derivation for P2PKH, P2SH-P2WPKH, P2WPKH, and P2TR with public keys and paths.\n- Accounts with balances per address type ([mempool.space](https://mempool.space) for UTXO data); wallet balance and BTC prices (USD, EUR).\n- Single and multi-recipient sends (amount in BTC or EUR); preview transfer with fee estimate before sending.\n- Sweep (send max) and UTXO consolidation.\n- PSBT sign, decode, and batch sign; message sign and verify (ECDSA legacy and BIP-322).\n- Fee tiers from mempool.space and fee estimation by input/output count and address type.\n- UTXO listing with filters (address type, min value, confirmed only) and per-UTXO details.\n\n**Ordinals and inscriptions:**\n\n- List inscriptions with pagination; inscription details (genesis, content type, sat ordinal, rarity, location).\n- Send inscriptions (full UTXO or split so only the inscription's sat range goes to the recipient).\n- Extract ordinals from mixed UTXOs; recover BTC from the ordinals address (sweep non-inscription UTXOs); recover ordinals that landed on the payment address back to the taproot address.\n- Create single or batch inscriptions with commit/reveal fee estimates.\n\n**Transaction and wallet management:**\n\n- Transaction history for BTC and Stacks; status for a single tx.\n- Speed up pending BTC via RBF; cancel pending BTC (RBF send-to-self).\n- Network config and API endpoints; switch mainnet/testnet; add custom network.\n- List all supported tool names and descriptions.\n\n**Ledger (Bitcoin app):**\n\n- Get BTC addresses from a connected [Ledger](https://www.ledger.com) device.\n- Sign PSBT with the Ledger Bitcoin app.\n\n### Layer 2 (Stacks)\n\nThe same mnemonic derives Stacks keys (path `m/44'/5757'/0'/0/0`). [Hiro](https://hiro.so) Stacks API for chain data and broadcasting.\n\n**Stacks:**\n\n- Addresses and public keys; accounts with STX balance, locked amounts, nonces.\n- Balance including fungible and non-fungible tokens.\n- STX transfer (micro-STX) with optional memo; preview transfer with fee and balance check.\n- SIP-10 fungible and SIP-9 NFT transfers via contract calls.\n- Clarity: call public function, deploy contract, read-only call.\n- Sign serialized Stacks tx (SIP-30), sign message, sign SIP-018 structured data; nonce and fee estimation.\n- On-chain profile update ([schema.org/Person](https://schema.org/Person)) for BNS names.\n- Transaction queries with filters (type, block range, unanchored) and by contract.\n- Mempool: list pending transactions, mempool stats, dropped transactions.\n- Block explorer: recent blocks, block by height or hash, Stacks blocks for a given Bitcoin block.\n- Contract events: events for a contract, or asset events for an address.\n- Token metadata: SIP-10 and SIP-9 metadata and holders.\n- Network info and health/status.\n\n**Swaps, DeFi, and bridge:**\n\n- Supported pairs and protocols ([ALEX](https://alexlab.co/), [Bitflow](https://www.bitflow.finance), [Velar](https://www.velar.co)).\n- Swap quote (estimated output, rate, fees) for all three; execute swap via ALEX DEX. Bitflow and Velar support quotes and pair discovery; you could add execution via protocol SDKs (e.g. Velar SDK returns contract-call params).\n- Swap history from on-chain activity.\n- sBTC balance and bridge deposit/withdraw info.\n- Stacking: current PoX status, cycle info (blocks remaining, percent complete, estimated time remaining, participation rate), initiate solo stacking, revoke delegation.\n\n**BNS and market data:**\n\n- [BNS](https://docs.stacks.co/docs/stacks-blockchain/bns) lookup (name to address), names owned by address, register BNS name.\n- Multi-asset prices (e.g. [CoinGecko](https://www.coingecko.com)); price history for charting.\n- Portfolio summary (BTC + STX in USD); all assets and collectibles (inscriptions, Stacks NFTs).\n\n**Ledger (Stacks app):**\n\n- Get Stacks addresses from Ledger.\n- Sign Stacks transaction with Ledger Stacks app.\n\n## Safety and design\n\n⚠️ This MCP server is experimental and not safe for meaningful funds. Use only with wallets you are prepared to lose. No one has battle-tested or audited the code. I treat it as a research artifact to explore agent-native wallet surfaces.\n\nDestructive operations default to `dry_run: true`. Preview and estimate tools exist for every send path. Keys stay out of version control and out of tool responses. The run script loads `.env` from repo root.\n\n**Wallet key variables (keep secret, never commit):**\n\n- **`BTC_PRIVATE_KEY`** — WIF-encoded Bitcoin private key; if set, takes precedence over mnemonic.\n- **`BTC_MNEMONIC`** — BIP-39 seed phrase; the server uses it to derive Bitcoin and Stacks keys (same mnemonic, path `m/44'/5757'/0'/0/0` for Stacks).\n- **`BTC_MNEMONIC_PASSPHRASE`** — Optional BIP-39 passphrase to use with `BTC_MNEMONIC`.\n\n**Safety and limits (env or .env):**\n\n- **`BTC_NETWORK`** — `mainnet` or `testnet` (default `testnet`).\n- **`BTC_MAINNET_ENABLED`** — Set this to allow mainnet sends (safety flag).\n- **`BTC_DRY_RUN`** — When set (default), destructive ops (sends, sign-and-broadcast, deploy) do not broadcast; set it to `false` to allow real transactions.\n- **`BTC_MAX_SEND_BTC`** — Optional cap on send amount in BTC; the server rejects requests above this.\n- **`BTC_MAX_FEE_SATS`** — Optional cap on fee in satoshis per transaction.\n- **`STX_ACCOUNT_INDEX`** — Stacks derivation account index (default `0`).\n- Config otherwise drives the fee tier (fixed rate or mempool.space tier: hour, half-hour, fastest).\n\n## How it fits my agent stack\n\nI run agents on a three-layer architecture. The layers are cleanly separated so that memory, reasoning, and action stay in the right place.\n\n**Truth layer:** This is the memory substrate. It holds typed, structured data: holdings, flows, transactions, contacts, tasks, and the rest. In my setup the canonical store is [Neotoma](/posts/truth-layer-agent-memory). It uses event sourcing and reducers, with full provenance and entity resolution. Agents read from it. They never write truth directly. All updates flow through domain events produced by the execution layer.\n\n**Strategy layer:** This is where goals, constraints, and tactics live. Strategy documents, tactical playbooks, and operations manuals sit here. Agents use this layer to reason: they read world state, evaluate priorities and risk, and produce decisions and commands. Strategy is pure cognition. No side effects. State in, decisions out.\n\n**Execution layer:** This is where external actions happen. It takes commands from the strategy layer and performs side effects through adapters: email, calendar, DNS, and in this case the Bitcoin and Stacks wallet MCP. The wallet server is one execution adapter among many. It never mutates the truth layer. It does the thing (send, sign, swap) and the rest of the stack records what happened via domain events. Commands in, events out.\n\nI define and maintain the strategy. Agents read from the truth layer and call MCP tools to execute. I do not use point-and-click crypto UIs for routine operations. I only step in to approve actions that exceed my pre-set limits.\n\nShort-term my use cases are one-off: paying for services, rebalancing portfolios through manual prompting. Longer-term I want those flows automated. Agents would monitor, reason, and execute within policy. I would see explanations and approve when needed.\n\n## How I'm approaching the build\n\nI'm dogfooding the server in my own workflows first. I'm testing each surface (sends, PSBTs, Ordinals, Stacks transfers, swaps) gradually with small amounts and dry runs.\n\nI've wired it into the same stack where I already use [truth and strategy layers](/posts/agentic-search-and-the-truth-layer#where-ive-hit-limits). Agents can combine wallet actions with calendar, email, and data. External users aren't in scope yet.\n\nMy goal is to validate the shape of an agentic wallet surface and to make my own Bitcoin and Stacks operations agent-driven instead of manual.\n\nTo run it: clone [mcp-server-bitcoin](https://github.com/markmhendrickson/mcp-server-bitcoin) (or add as submodule at `mcp/btc_wallet/`), add the server to your MCP config (use the `run_btc_wallet_mcp.sh` script path), and use a test wallet with dry run on.\n",
      "heroImage": "agentic-wallets-mcp-bitcoin-hero.png",
      "heroImageStyle": "keep-proportions",
      "heroImageSquare": "agentic-wallets-mcp-bitcoin-hero-square.png",
      "createdDate": "2026-02-07",
      "updatedDate": "2026-02-09",
      "summary": "- I built an experimental MCP server with 93 tools for Bitcoin L1 and Stacks L2, one mnemonic for both.\n- Same two chains as human wallets, but agent-callable: less friction for people who want to hold their own keys without the full cognitive load.\n- L1 covers core Bitcoin, Ordinals and inscriptions, tx management, and Ledger; L2 covers STX, Clarity, swaps, sBTC bridge, stacking, BNS, market data, mempool monitoring, block explorer, contract events, token metadata, network stats, and enriched stacking cycle progress.\n- Destructive actions default to dry run; no keys or mnemonics are ever returned; preview and env limits keep execution policy-gated.\n- The wallet MCP is an execution adapter in my three-layer stack; I define strategy, agents execute, I approve when actions exceed my limits.",
      "ogImage": "og/agentic-wallets-mcp-bitcoin-1200x630.jpg"
    },
    {
      "slug": "why-agent-memory-needs-more-than-rag",
      "title": "Why agent memory needs more than RAG",
      "excerpt": "Retrieval for agent memory should be driven by structure, not similarity. Learned hierarchies beat RAG but are brittle. Schema-first design gives the same advantage without putting the LLM in the critical path.",
      "published": true,
      "publishedDate": "2026-02-06",
      "category": "Technical",
      "readTime": 5,
      "tags": [
        "neotoma",
        "agent-memory",
        "architecture",
        "rag",
        "build-in-public"
      ],
      "body": "[RAG (retrieval-augmented generation)](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) augments an LLM by retrieving relevant passages from an external corpus, often via embeddings and similarity search, then feeding them as context so the model can answer from up-to-date or domain-specific data.\n\nIt works well for document search. For agent memory, it falls apart.\n\nA new paper, \"Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation\" (Hu et al., Feb 2026; [see paper](https://arxiv.org/abs/2602.02007)), from King's College London and the Alan Turing Institute, explains why and points to a better approach.\n\n## Why RAG falls short for agent memory\n\nStandard RAG assumes a large, mixed corpus: embed text, retrieve [top-k](https://en.wikipedia.org/wiki/Nearest_neighbor_search) by similarity, concatenate as context.\n\nAgent memory is the opposite: a bounded, coherent stream where the same fact appears in many phrasings. Applying RAG here creates three problems:\n\n1. **Redundant top-k.** You ask \"When did I last see the dentist?\" In a document corpus, top-k might return a few relevant paragraphs from different sources. In agent memory, many chunks say almost the same thing (\"Scheduled dentist March 15,\" \"Dentist appointment March 15,\" \"Booked dentist for March 15\"). Top-k fills with repetition. The paper calls this \"collapse into a single dense region.\" Similarity fails to separate what is *needed* from what is merely *similar*.\n2. **Pruning breaks evidence chains.** You ask \"Did we resolve the invoice dispute?\" The answer depends on a chain: \"Invoice #123 was disputed,\" then \"We agreed to a partial refund,\" then \"Paid the agreed amount.\" Post-hoc pruning might keep \"Paid invoice #123\" and drop the earlier turns. The model then answers \"Yes, resolved\" without knowing there was a dispute. Pruning fragments temporally linked evidence and produces wrong answers.\n3. **Similarity ignores structure.** You ask \"What's the status of the Barcelona trip?\" You need the project, the task (e.g. book flights), and the outcome. Similarity returns chunks that mention \"Barcelona\" or \"trip\": maybe a random mention, a past trip, a task from a different project. You needed a structural path (this project, these tasks, these outcomes). Similarity doesn't encode that. Structure does.\n\n## Structure over similarity\n\nA better approach is to use structure to drive what gets loaded, not similarity. Type entities (tasks, contacts, transactions, events) and retrieve by schema, entity IDs, relationships, and timelines. Keep observations and derived outputs as whole units; don't prune inside evidence blocks. Same input and same schema yield same output. No LLM in the critical path.\n\n## What the paper shows\n\nThe paper's system (xMemory) builds a four-level hierarchy (messages to episodes to semantics to themes) with embeddings and LLM summaries. It beats five other systems (Naive RAG[^1], A-Mem[^2], MemoryOS[^3], LightMem[^4], Nemori[^5]) on LoCoMo and PerLTQA, the benchmark datasets for long-conversation memory and personal long-term question answering. The paper doesn't require embeddings or LLMs; it requires structure. You can get there with a learned hierarchy (xMemory) or with deterministic, schema-first design. The paper also documents fragility in LLM-generated structure (A-Mem, MemoryOS): formatting deviations, failed updates. Deterministic, schema-first structure is a more reliable base.\n\n## xMemory vs Neotoma\n\nNeotoma is the [structured memory layer](/posts/truth-layer-agent-memory) I'm building: schema-first, deterministic, built for provenance and replay. Both systems move beyond RAG; they differ in how they build structure.\n\n**xMemory** builds a four-level hierarchy (messages to episodes to semantics to themes) with embeddings and LLM summaries. Episodes are contiguous blocks; semantics are reusable facts; themes group semantics for high-level access. A sparsity-semantics objective balances theme size. Too large causes redundant retrieval; too small fragments evidence. Retrieval is top-down: select a compact set of themes and semantics, then expand to intact episodes (and optionally messages) only when that reduces the reader model's uncertainty. No pruning inside units. On those benchmarks it beats the five baselines on quality and token use. The paper notes that LLM-generated structure (e.g. in A-Mem, MemoryOS) is brittle: formatting deviations, failed updates. Because xMemory builds its hierarchy with LLM summaries, it adopts the same brittleness.\n\n**Neotoma** builds structure without LLMs in the critical path. Entities are typed; relationships and timelines are explicit; retrieval uses schema, entity IDs, relationships, and time ranges. Same input and schema yield the same output. Schemas still evolve. Unknown fields land in a preservation layer. A deterministic pipeline can promote high-confidence fields to the schema. An LLM can suggest new fields or types as pending recommendations, applied only via tooling or human approval. Inference stays advisory: schema changes go through tooling or human approval; extraction and reduction stay deterministic; the schema remains source of truth. The paper's critique applies when the model *drives* structure, not when it suggests and humans or tooling apply. Ingest-to-retrieve stays deterministic.\n\n### Comparison\n\n| | xMemory | Neotoma |\n|--|--------|--------|\n| Structure source | Embeddings + LLM summaries (episodes, semantics, themes) | Schema-first, deterministic extraction and reducers |\n| Hierarchy | Four levels (messages, episodes, semantics, themes), guided by sparsity-semantics objective | Typed entities, relationships, timelines (no fixed \"levels\") |\n| Retrieval | Top-down: representative selection on graph, then uncertainty-gated expansion to intact episodes/messages | By schema, entity IDs, relationships, timelines |\n| Redundancy control | Representative selection + expand only when uncertainty drops | Structural queries return what you ask for; no similarity collapse |\n| Intact units | Yes (no pruning inside episodes/messages) | Yes (observations and entities kept whole) |\n| Determinism | No (LLM-generated structure varies) | Yes (same input, same schema, same output) |\n| Brittleness | Paper cites LLM formatting deviations, failed updates in similar systems | Schema and code are explicit; no LLM in the critical path |\n\n### Relative advantages\n\n**xMemory** excels when the input is a conversation stream and you want structure without defining schemas. Example: long-running chat with an assistant where you ask \"what did we decide about the trip?\" or \"when did I last mention the budget?\" xMemory builds episodes, semantics, and themes; retrieval is token-efficient. It also fits fast prototypes (support tickets, meeting notes) where you don't want to author schemas yet. You accept hierarchy drift and don't need auditability or first-class query by entity.\n\n**Neotoma** excels when you need traceability or your data is already structured. Example: auditable decisions (payments, agreements, task outcomes) where same inputs and schema must yield the same snapshot. Schema changes are versioned and applied deterministically; no LLM in the path. It is also the right fit for typed entities (tasks, contacts, transactions, events) with relationships and timelines. Query by entity type, ID, relationship, or time range. Neotoma treats those as native; xMemory would require serializing to text and loses first-class access.\n\n## Iterative structuring in conversation\n\nStructure often emerges in dialogue: \"add a task for that,\" \"record that we agreed to pay 500,\" and the agent acts. The two systems handle that differently.\n\n**xMemory:** The conversation is the primary object. What the agent does (e.g. \"I've created a task for the dentist\") stays in the message stream and flows into episodes, semantics, and themes. You get a better learned hierarchy but no separate, queryable entity graph. Structure lives inside the hierarchy.\n\n**Neotoma:** The conversation is one source of observations. When the agent creates or updates a task, contact, or transaction, those operations produce observations and entity snapshots. New fields from the dialogue can land in a preservation layer and be promoted to the schema when confidence is high. The dialogue and the structured graph stay in sync because both write into the same store.\n\n**Differing retrieval.** xMemory supports semantic retrieval over the hierarchy. Natural-language questions (\"what did we decide about the dentist?\") return themes, semantics, or intact episodes. It does not support structural retrieval (no entity types with IDs and relationships). That leads to expected failures in three kinds of cases:\n\n- **Evidence spread across turns.** \"Did we resolve the invoice dispute?\" The dispute, the negotiation, and the payment may live in different episodes or themes; retrieval can surface one or two and miss the rest, so the model answers incorrectly or incompletely.\n- **Set queries.** \"What tasks are due before Friday?\" or \"Show all payments to contact X.\" There are no task or transaction entities to filter; you get semantic matches (messages that mention \"task\" and \"Friday\" or \"contact X\"), not a definitive list, so results are partial or noisy.\n- **Relationship traversal.** \"Which tasks in project Y are still pending?\" Without a project-task graph, retrieval returns conversation snippets that may omit some tasks or projects; you cannot reliably enumerate by relationship.\n\nNeotoma supports both. You can ask semantic-style questions when the data lives in the store. You also get structural retrieval by entity type, ID, relationship, and time window, so set queries and relationship traversal return complete, first-class results. The tradeoff is that you need schemas and a store that accept those observations.\n\n## Structure over similarity, schema-first over brittleness\n\nFor agent memory, similarity over raw text fails. Retrieval has to be driven by structure: how you decompose and organise the stream, not how many chunks match a query. The paper shows that a learned hierarchy (xMemory) beats naive RAG and that LLM-generated structure is brittle.\n\nHowever, a deterministic, schema-first path gives you the same structural advantage without that brittleness. I'm building [Neotoma](https://github.com/markmhendrickson/neotoma) on the latter so ingest and retrieval stay reproducible and the schema stays source of truth.\n\n[^1]: **Naive RAG:** embed memories, retrieve fixed top-k by similarity, no hierarchy. No separate project; baseline defined in the [paper](https://arxiv.org/abs/2602.02007).\n[^2]: **A-Mem:** agentic memory for LLM agents; Zettelkasten-style links and agent-driven updates to a memory network. [Project](https://github.com/agiresearch/A-mem).\n[^3]: **MemoryOS:** hierarchical short/mid/long-term storage with update, retrieval, and generation modules for personalized agents. [Project](https://github.com/BAI-LAB/MemoryOS).\n[^4]: **LightMem:** lightweight memory inspired by Atkinson-Shiffrin stages; topic-aware consolidation and offline long-term updates. [Project](https://github.com/zjunlp/LightMem).\n[^5]: **Nemori:** self-organizing episodic memory with event segmentation and predict-calibrate for adaptive knowledge. [Project](https://github.com/nemori-ai/nemori).\n",
      "heroImage": "why-agent-memory-needs-more-than-rag-hero.png",
      "heroImageStyle": "keep-proportions",
      "heroImageSquare": "why-agent-memory-needs-more-than-rag-hero-square.png",
      "createdDate": "2026-02-05",
      "updatedDate": "2026-02-06",
      "ogImage": "og/why-agent-memory-needs-more-than-rag-1200x630.jpg",
      "summary": "- RAG fails for agent memory because of redundant top-k, pruning that breaks evidence chains, and similarity that ignores structure.\n- The paper validates structure over similarity: retrieval should follow the organisation induced by decomposition and aggregation, not a ranking over raw spans; xMemory does it with a learned hierarchy, Neotoma with schema-first design.\n- xMemory gives semantic retrieval over a four-level hierarchy but no structural retrieval; Neotoma gives both and stays deterministic, with optional advisory LLM suggestions for schema changes.\n- Semantic-only retrieval leads to expected failures when evidence is spread across turns, when you need set queries (e.g. tasks due before Friday), or when you need relationship traversal (e.g. tasks in project Y).\n- Deterministic, schema-first structure is more reliable than LLM-generated structure; inference can suggest schema changes without breaking determinism if it stays advisory and tooling or humans apply."
    },
    {
      "slug": "building-structural-barriers",
      "title": "Building structural barriers that incumbents can't copy",
      "excerpt": "You don't protect it through secrecy or patents. You build something they structurally can't pursue.",
      "published": true,
      "publishedDate": "2026-02-05",
      "category": "essay",
      "readTime": 5,
      "tags": [
        "strategy",
        "neotoma",
        "positioning",
        "defensibility"
      ],
      "body": "My dad asked me a good question this week: \"How do you protect your intellectual property when Big Guys could develop their own version?\"\n\nThe short answer: you don't protect it through secrecy or patents. You build something they structurally can't pursue.\n\n## The myth of the copyable startup\n\nThe standard startup fear goes like this: you build something, it works, and then Google/Microsoft/OpenAI ships the same thing in six months with better distribution.\n\nThis fear assumes incumbents can copy anything. They can't.\n\nNot because they lack engineers or capital. They have both. They can't copy certain architectures because their existing business models, technical stacks, and organizational incentives make it structurally expensive or impossible.\n\n## Five structural barriers that actually work\n\n### Incentive misalignment\n\nBuild something that would cannibalize a profitable revenue stream.\n\nExample: A privacy-first memory system that runs entirely on user-controlled infrastructure directly conflicts with ad-supported business models. The incumbent would have to choose between their existing margin pool and the new product. They typically choose the existing margin.\n\n### Architectural constraints\n\nBuild on foundations that require rewrites of core systems.\n\nExample: Event-sourced, deterministic workflows with full provenance require immutable data structures and hash-based entity IDs. Platforms built on mutable document stores or eventual consistency models would need to rebuild their entire data layer. That's a multi-year rewrite, not a feature sprint.\n\n### Business model conflicts\n\nBuild something that requires a different pricing model or customer relationship.\n\nExample: Mid-market annual contracts (€3k–€15k per year) with deep workflow integration don't fit self-serve, usage-based billing platforms. The sales motion, support model, and product expectations are fundamentally different.\n\n### Distribution mismatch\n\nBuild for a customer segment the incumbent can't reach effectively.\n\nExample: Sovereignty-conscious power users who actively avoid platform lock-in won't adopt features from the very platforms they're trying to escape. The incumbent's distribution advantage becomes a liability.\n\n### Timing and focus windows\n\nBuild fast enough that by the time they could copy, your compounding advantages make it irrelevant.\n\nExample: If you can ship deterministic workflows, cross-domain entity resolution, and extensible object schemas in 12 months while the incumbent is navigating internal prioritization, compliance reviews, and competing roadmap pressures, you'll be three generations ahead before they start.\n\n## Neotoma as a case study\n\nI'm building [Neotoma](/posts/truth-layer-agent-memory) ([GitHub](https://github.com/markmhendrickson/neotoma)) as a privacy-first, deterministic memory substrate for AI tools. It demonstrates all five barriers.\n\n**Privacy-first architecture (incentive misalignment).** Model providers make money from telemetry, training data, and platform lock-in. A system where users own their memory and can use any model directly conflicts with their revenue model. They could build it, but they'd be undermining their existing business.\n\n**Deterministic, event-sourced design (architectural constraints).** Neotoma uses immutable observations, hash-based entity IDs, and deterministic reducers. This guarantees that the same operation always produces the same final state. Platforms built on mutable stores or eventually-consistent systems would need to rebuild their data layer from scratch.\n\n**Cross-platform interoperability (business model conflicts).** Neotoma works with ChatGPT, Claude, and Cursor through MCP. Supporting competitor platforms directly conflicts with single-model lock-in strategies. Incumbents optimize for keeping users inside their ecosystem, not enabling them to leave.\n\n**Sovereignty positioning (distribution mismatch).** The target customers are people who explicitly want control over their data and memory. They won't adopt memory features from the platforms they're trying to avoid. Distribution advantage becomes distribution liability.\n\n**Architectural compounding (timing).** Every entity resolved, every schema extended, every workflow made reproducible increases the cost of replication. The value isn't in any single feature. It's in the network of typed relationships, the audit trail, and the compounding quality of deterministic memory.\n\n## General principles for defensible startups\n\nWhen evaluating startup ideas, ask these questions.\n\n**Would copying this hurt their existing revenue?** If yes, they won't do it fast. If no, you're competing on execution speed alone.\n\n**Does this require architectural changes they can't make incrementally?** If they can add it as a feature, they will. If it requires a rewrite, you have time.\n\n**Does this require a different customer relationship or sales motion?** If their existing go-to-market doesn't fit your ICP, they can't reach your customers effectively even if they build the same thing.\n\n**Is your distribution actually an advantage for this product?** Sometimes incumbents have anti-distribution for specific customer segments.\n\n**Can you compound advantages faster than they can mobilize?** Speed matters, but it's speed of compounding, not speed of shipping features.\n\n## What doesn't work\n\nThese don't create structural barriers.\n\n**Complexity alone.** Hard to build isn't the same as structurally hard to copy. If it's just engineering complexity, they have more engineers.\n\n**Better UX.** UI is easy to copy. Design systems can be replicated in weeks.\n\n**Network effects (early stage).** Network effects take time to compound. In the first year, you don't have them yet.\n\n**Patents.** Tech patents are expensive to enforce and easy to work around. Focus on structural barriers instead.\n\n**Secrecy.** Once you ship, the approach is visible. Secrecy buys you months at most.\n\n## The goal isn't to outrun them\n\nThe goal is to build something they can't pursue without making choices they won't make.\n\nYou're not trying to be faster forever. You're trying to construct a position where speed stops mattering because the structural constraints do the defending.\n\nFor Neotoma specifically: OpenAI could build a privacy-first, cross-platform memory system. But doing so would require them to give up telemetry, platform lock-in, and single-model revenue streams. They're structurally disincentivized from making that trade.\n\nThat's not fear-based positioning. That's understanding the board.\n\n## Takeaway\n\nWhen someone asks \"what if Big Guys copy you?\", the answer isn't \"we'll move faster\" or \"we have patents.\"\n\nThe answer is: \"We built something that would require them to make structural changes they're incentivized not to make.\"\n\nThat's the only moat that lasts.\n",
      "heroImage": "building-structural-barriers-hero.png",
      "heroImageStyle": "keep-proportions",
      "heroImageSquare": "building-structural-barriers-hero-square.png",
      "createdDate": "2026-02-05",
      "updatedDate": "2026-02-10",
      "summary": "- Incumbents face structural constraints that prevent them from copying certain architectures, even if they have more resources.\n- Effective barriers arise from incentive misalignment, architectural constraints, business model conflicts, distribution mismatch, and timing windows.\n- Neotoma demonstrates these principles: privacy-first architecture directly conflicts with incumbent revenue models, deterministic workflows require complete rewrites, and cross-platform support undermines lock-in strategies.\n- The goal is not to outrun replication through speed or secrecy, but to construct systems whose value compounds through principles incumbents are structurally disincentivized to pursue.\n- When evaluating defensibility, ask: would copying hurt their existing revenue, require non-incremental architectural changes, or demand a different customer relationship they can't serve?",
      "ogImage": "og/building-structural-barriers-1200x630.jpg"
    },
    {
      "slug": "agentic-search-and-the-truth-layer",
      "title": "Agentic retrieval infers. It doesn't guarantee.",
      "excerpt": "Your AI finds things. It also misses things, overwrites them, and often can't say where an answer came from. Why that happens and what would fix it.",
      "published": true,
      "publishedDate": "2026-02-04",
      "category": "essay",
      "readTime": 5,
      "tags": [
        "neotoma",
        "agentic-search",
        "truth-layer",
        "rag"
      ],
      "body": "[Boris Cherny (creator of Claude Code at Anthropic) tweeted](https://x.com/bcherny/status/2017824286489383315) that Claude Code moved from RAG plus local vector DB to agentic search. It works better, he said, and is simpler, with fewer issues around security and privacy. Other tools take a different path. Cursor, for example, uses cloud-based embeddings to index the codebase and search by semantic similarity.\n\nSo we have at least two retrieval paradigms: embedding-based search (pre-indexed, vector similarity) and agentic search (on-demand tool use). They are not the same. Each has different tradeoffs. Both are retrieval strategies. A truth layer is something else. It persists canonical entities, maintains provenance, and supports deterministic queries. It's about state, not retrieval. This post compares a truth layer to both retrieval models. It also ties in the limits I've hit when relying on retrieval alone.\n\n## Where I've hit limits\n\nI use Cursor as my central interface for all of my digital workflows, not just coding. Email triage, task management, finance queries, content planning, transactions, contacts. They all run through the same agent with access to the same repo. Agentic search across files often works well. The agent finds context, infers connections, and gets things done.\n\nBut I've hit limits. The agent infers; it doesn't guarantee. Here's what that looks like:\n\n- **Large datasets, incomplete recall.** On-demand search misses things or truncates across thousands of transactions or hundreds of contacts. Retrieval re-derives each time. There's no structured store to query for complete results.\n- **Irrecoverable overwrites.** An agent overwrites a contact or task and the previous state is gone. No rollback. Writes are in-place. There's no versioning or append-only trail to trace and roll back.\n- **No cross-tool access.** I can't use the same records from Claude.ai or ChatGPT. Retrieval is provider-bound.\n- **Non-reproducible answers.** Same question, different answer. I can't reproduce a result for verification or debugging. Retrieval is non-deterministic.\n- **No traceability.** When the agent gives a wrong number or claim, I can't trace it back to source files or records. Retrieval has no provenance.\n- **Unstable canonical identity.** The agent may treat \"Acme Corp\" and \"ACME CORP\" as the same in one session and different in the next. Retrieval re-infers each time. There are no persistent canonical IDs or merge rules.\n\n## Two retrieval paradigms, one state paradigm\n\nEmbedding-based search and agentic search both get information to an agent. They are not the same. Embedding-based search (e.g. Cursor) pre-indexes a corpus and answers via vector similarity. The index can be cloud-hosted and updated. Agentic search (e.g. Claude Code) skips a persistent index and uses tools to search and read on demand. Different implementations, different tradeoffs: privacy, staleness, simplicity.\n\nWhat they share is retrieval. The agent finds things at query time. A truth layer is not retrieval. It is persistent, structured state: canonical entities, provenance, deterministic queries.\n\nWe're comparing one state paradigm (truth layer) to two retrieval paradigms (embedding-based and agentic). The table below lines up all three. Where both retrieval columns share a limit (e.g. no provenance), that's a similarity between them relative to a truth layer. It's not an equation of the two.\n\n| Domain | Embedding-based search | Agentic search | Truth layer |\n|--------|------------------------|----------------|-------------|\n| Document retrieval | Pre-indexed similarity, semantic match | On-demand search, inference | Entity resolution, dedup, provenance |\n| Multi-source aggregation | Index scope and freshness depend on build | Live search across sources | Unified graph, deterministic merge |\n| Entity lookup | Similarity over embeddings; no canonical ID | Per-session inference | Canonical IDs, rule-based merge |\n| Timeline queries | Only if indexed; no native time model | On-demand assembly | Pre-computed, schema-driven |\n| Provenance and audit | None | None | Immutable audit trail |\n| Cross-platform | Tied to provider/index | Provider-specific tools | Same data across tools |\n\nBoth retrieval approaches optimize for convenience and flexibility. A truth layer optimizes for consistency and verifiability.\n\n## What a truth layer provides\n\nA structured memory layer is built around different primitives:\n\n1. **Persistent canonical identity.** Stable entity IDs across sessions and tools.\n2. **Deterministic merge logic.** Rule-based combination of observations, not per-session LLM inference.\n3. **Provenance and audit.** Traceable lineage from source to answer.\n4. **Idempotence.** Same inputs yield same outputs.\n5. **Cross-platform truth.** Same memory across ChatGPT, Claude, Cursor.\n6. **Clear privacy model.** User control, no provider training use, clear data boundaries.\n\nThese are not incremental improvements over agentic search. They are a different design. Best-effort retrieval and orchestration versus verifiable, replayable state. The choice depends on what you need.\n\n## What retrieval can approximate (agentic or embedding-based)\n\nThree examples show retrieval (agentic or embedding-based) approximating the capabilities above. In each example, the agent gets something that looks right for the moment. In each, the same limits show up: no persistent canonical identity, no provenance, no guarantee that \"same query\" yields \"same result\" across sessions or index rebuilds. The examples below use agentic terms (tools, on-demand search). Embedding-based retrieval can approximate the same behaviors via semantic search over an index and hits the same limits.\n\n**Example 1: Session-scoped entity resolution.** The agent has tools to search files, email, and cloud. It has instructions to treat mentions of the same entity as one. You ask: \"What's my total spend with Acme Corp?\" The agent searches bank exports, receipts, invoices. It finds \"Acme Corp\", \"ACME CORP\", \"Acme Corporation\", infers same entity, sums amounts. That looks like entity resolution for this query and session. What goes wrong: ask again tomorrow and the number may differ. The agent may miss a file (truncated search, wrong path) and undercount. Or it may treat \"Acme Corp\" and \"Acme Industries\" as the same and overcount. No way to verify. No audit trail, no stable IDs. Different sessions may disagree.\n\n**Example 2: On-demand timeline assembly.** The agent has broad file and date access. You ask: \"What were my major expenses in Q3 2024?\" The agent searches, parses dates, assembles a chronological list, filters by \"major.\" You get a timeline-like answer without a dedicated timeline system. What goes wrong: \"Major\" is inferred each time. One session excludes a €500 item. The next includes it. Documents with non-standard date formats get dropped or misordered. The agent may truncate (\"here are the top 10\") when there were 15. Same query, different results, every time.\n\n**Example 3: Hybrid memory layer.** A provider ships agentic search plus lightweight memory. The agent extracts structured snippets, stores them, and retrieves them later. It processes a receipt, stores `{vendor: \"Acme Corp\", amount: 150, date: \"2024-07-15\"}`. A later session retrieves this and merges with live search results. That looks like structured memory. What goes wrong: a later extraction overwrites the snippet. No versioning, no rollback. The same vendor appears as \"Acme Corp\" in stored memory and \"ACME CORP\" in a fresh search. Duplicates accumulate. The provider changes the feature or schema and your stored snippets vanish. No way to trace a wrong number back to its source.\n\nIn each example, the behavior approximates what a truth layer provides. The limits are inherent to retrieval. Whether the agent uses embedding search or agentic search, you still get session scope and inference-based merge. You still get no provenance and no cross-platform guarantee. A truth layer addresses those by persisting state instead of re-retrieving it.\n\n## When retrieval excels (agentic or embedding-based)\n\n**Exploratory discovery.** \"Find anything in my downloads or notes about the Barcelona apartment.\" You don't know where it lives or what it's called. Agentic search across files, folders, and formats surfaces relevant snippets. No schema required. The agent infers and assembles.\n\n**Rapid cross-source summarization.** \"What did we decide in the last three emails with the contractor?\" Search inbox, extract thread, summarize. One session, one answer. You don't need that summary to persist or match exactly next time.\n\n**Ad hoc code and docs traversal.** \"Where do we handle Stripe webhooks?\" Search codebase, README, internal docs. Layout varies by repo. Agentic search adapts. No unified graph needed.\n\n**Single-document or single-thread triage.** \"Summarize this PDF\" or \"What's the ask in this email?\" Context is bounded. Inference is sufficient. No entity resolution or cross-session state.\n\n## When a truth layer excels\n\n**Complete recall over large datasets.** \"List every transaction with vendor X in the last two years.\" With thousands of rows, agentic search may miss records, truncate, or hallucinate aggregates. A truth layer queries a structured store. You get all matching records or a precise count.\n\n**Cross-session consistency.** The agent creates a follow-up task in session one. You open a new session tomorrow. The task must be there, linked to the right contact and email. Agentic search has no persistent graph. A truth layer does.\n\n**Audit and provenance.** \"Where did this number come from?\" Trace it to source records, import dates, and derivation rules. Agentic search returns inferred answers. A truth layer returns answers with lineage.\n\n**Entity resolution at scale.** Hundreds of contacts, some duplicates (name variations, merged companies). Thousands of transactions referencing the same vendor under different spellings. A truth layer maintains canonical IDs and merge rules. Agentic search re-infers each session and may disagree.\n\n**Deterministic replay.** Same query, same result, every time. Critical for reporting, compliance, or debugging. Agentic search is non-deterministic. A truth layer is idempotent.\n\n**Recoverability from bad writes.** An agent overwrites a contact, merges two tasks into one, or \"corrects\" a transaction based on wrong inference. With agentic search and direct file writes, the previous state is gone. No undo. A truth layer uses append-only or versioned writes. You can trace what changed and roll back. Mutations are explicit operations, not silent overwrites.\n\n## Why the distinction matters\n\nRetrieval (embedding-based or agentic) is session-bound. It doesn't by itself give you persistent identity, provenance, or cross-session consistency. Its value is flexible, on-demand access. A truth layer's value is persistent, cross-session truth. Deterministic, auditable entity resolution is hard. Neither embedding similarity nor ad hoc agentic search is equivalent. Provider-hosted agents face incentives that conflict with user-controlled, privacy-first memory. Their memory and tools tend to be product-specific.\n\nThe Cherny tweet reflects a real shift. RAG plus vector DB was complex and had privacy implications. Agentic search simplified retrieval for Claude Code. Cursor and others take a different retrieval path (cloud embeddings). Both retrieval paradigms solve \"how does the agent find things?\" Neither solves \"how do we get stable identity, provenance, and verification?\" A truth layer targets the latter. Retrieval and state layers will coexist. They solve different problems.\n\n## What I'm building\n\nI'm building [Neotoma](https://github.com/markmhendrickson/neotoma), a structured memory layer that takes the truth layer approach: entity resolution, timelines, provenance, determinism, cross-platform via MCP. I'm dogfooding it in my own agentic stack to see where these primitives matter in practice. Embedding-based search and agentic search are two retrieval strategies. Neither gives you persistent identity or verifiable state. A truth layer does. I'm building the latter.\n",
      "heroImage": "agentic-search-and-the-truth-layer-hero.png",
      "heroImageStyle": "keep-proportions",
      "heroImageSquare": "agentic-search-and-the-truth-layer-hero-square.png",
      "createdDate": "2026-02-04",
      "updatedDate": "2026-02-09",
      "summary": "- Retrieval (embedding-based or agentic) infers; it doesn't guarantee. A truth layer is persistent, structured state: canonical entities, provenance, deterministic queries. It's about state, not retrieval.\n- Two retrieval paradigms (pre-indexed similarity vs on-demand tool use) share the same limits relative to a truth layer: no persistent canonical identity, no provenance, no cross-session consistency, no cross-platform access.\n- In practice, retrieval leads to incomplete recall at scale, irrecoverable overwrites, non-reproducible answers, no traceability, and unstable canonical identity (e.g. \"Acme Corp\" vs \"ACME CORP\" across sessions).\n- A truth layer provides persistent canonical identity, deterministic merge logic, provenance and audit, idempotence, cross-platform truth, and a clear privacy model. Different design goals, not a spectrum.\n- Retrieval can approximate entity resolution, timelines, and structured memory within a session but re-derives each time; a truth layer persists state instead of re-retrieving it.\n- The choice depends on whether you need best-effort retrieval and orchestration or verifiable, replayable state. Retrieval excels at exploratory discovery and ad hoc summarization; a truth layer excels at complete recall, cross-session consistency, audit, and recoverability from bad writes.",
      "ogImage": "og/agentic-search-and-the-truth-layer-1200x630.jpg"
    },
    {
      "slug": "barcelona-guest-floor",
      "title": "Barcelona guest floor",
      "excerpt": "Your own floor in the heart of Barcelona. Quiet street in Gràcia, kitchen, king bed, rooftop terrace. Welcoming to all.",
      "published": true,
      "publishedDate": "2026-02-04",
      "category": "article",
      "readTime": 4,
      "tags": [
        "barcelona",
        "rental",
        "gràcia",
        "guest floor"
      ],
      "body": "Thanks for your interest in staying with us during your visit to Barcelona, Spain!\n\nWe live in a townhouse on a quiet side street near the heart of the city, with a top-floor designed to host friends and family in a separate unit with its own private kitchen, living room, bedroom and bath.\n\nThis guest floor is directly accessible upon entry into the building via an elevator that goes to all floors, including a rooftop terrace that you're invited to enjoy during your stay.\n\n## Our home is\n\n- LGBTIQA+ friendly\n- Kid friendly\n\n## Contact\n\nIf interested, contact us via email or WhatsApp:\n\n**Ana:** [+34 680 658 030](tel:+34680658030) or [ana.serrano.solsona@gmail.com](mailto:ana.serrano.solsona@gmail.com)\n\n**Mark:** [+34 722 513 042](tel:+34722513042) or [markmhendrickson@gmail.com](mailto:markmhendrickson@gmail.com)\n\n## What this place offers\n\n- Wifi ([speed test result](https://www.speedtest.net/es/result/18737866754))\n- Air conditioning and heating\n- Shared patio or balcony\n- Crib\n- Dedicated workspace\n- Window AC unit\n- Luggage dropoff allowed\n- Hair dryer\n- Free laundry (washer and dryer)\n- Living room with sleep-able sofa\n- Kitchen with full appliances\n- King-sized Tempur-Pedic bed\n- Spacious bathroom with walk-in shower\n- Elevator access\n- Rooftop terrace with barbecue\n- Bike-share program credentials\n- Ninebot KickScooter MAX G30E II Powered by Segway\n- Apple Home Pod speaker\n- Nespresso coffee machine\n- Peaceful side street location\n- Space for exercise (yoga)\n\n## Availability and prices\n\nPlease contact us for direct bookings and availability with the following pricing in mind, for payment via cash or crypto.\n\n### Standard rates (low season: November–March, excluding holidays)\n\n| Stay | Price |\n| --- | --- |\n| Per night (up to 6 nights) | €200 |\n| 1 week (7 nights) | €1,100 (then €125/night up to 2 weeks) |\n| 2 weeks (14 nights) | €1,400 (then €100/night up to 3 weeks) |\n| 3 weeks | €1,800 (then €75/night up to 4 weeks) |\n| 1 month / 4 weeks | €2,200 |\n\n### Shoulder season (April–May, October)\n\n| Stay | Price |\n| --- | --- |\n| Per night (up to 6 nights) | €240 |\n| 1 week (7 nights) | €1,300 |\n| 2 weeks (14 nights) | €1,700 |\n| 3 weeks | €2,200 |\n| 1 month / 4 weeks | €2,990 |\n\n### Peak season (June–September)\n\n| Stay | Price |\n| --- | --- |\n| Per night (up to 6 nights) | €280 |\n| 1 week (7 nights) | €1,600 |\n| 2 weeks (14 nights) | €2,000 |\n| 3 weeks | €2,600 |\n| 1 month / 4 weeks | €3,990 |\n\n*Special events:* Additional premiums apply during [Mobile World Congress](https://www.mwcbarcelona.com/) (March 3–7, +40%), [Primavera Sound](https://www.primaverasound.com/) (June 1–5, +30%), [Festa Major de Gràcia](https://ajuntament.barcelona.cat/gracia/ca/el-districte-i-els-seus-barris/la-vila-de-gracia/festa-major) (August 15–21, +25%), [Festa Major de Sants](https://barcelonasecreta.com/en/sants-festival-2025-barcelona/) (August 23–31, +25%), and [La Mercè](https://www.barcelona.cat/lamerce/en) (September 23–28, +25%). Stays overlapping events are priced night-by-night for the overlapping dates. Monthly stays: March €2,600, June €4,410, August €5,110, September €4,410 (event days prorated).\n\n**Additional information:**\n\n- Electricity and water utilities are included at no extra cost.\n- Housecleaner will prepare the guest floor ahead of your arrival though not during your stay unless requested for an extra cost of €50 per cleaning.\n- You also have the option to book via our [Airbnb listing](https://www.airbnb.com/) if you'd like to stay for over one month.\n\n## Photos\n\n### Bedroom\n\n![Bedroom exterior view](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F586a4d66-fd0a-4ae7-bc4f-844b45b2892c%2Fdormitorio_exterior.jpg?table=block&id=c8f26fe7-0d05-4081-9ffe-5ade93310a04&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Bedroom interior](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2Fa2ad780f-dfc1-4ad2-aa3c-206b7c725dbf%2Fdormitorio_interior.jpg?table=block&id=17bd7078-3396-4034-878d-5997d6bbfae9&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Bedroom](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2Fee144e0c-1f55-4c92-bde0-8bee0ba6d7ae%2Fdormitorio.jpg?table=block&id=11b545b0-a18f-4cd2-9a78-c6257ee3e2dc&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n### Bathroom\n\n![Bathroom vanity](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F4057ee1e-df6e-4cd1-94b0-31cb126545eb%2Ftocador.jpg?table=block&id=2c989f1c-e6d1-43e5-99c9-bc29026b610e&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Bathroom sink and shower](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F5777853a-4f29-49e8-9df7-05c9561cca80%2Flavabo_ducha.jpg?table=block&id=aa98b51e-4408-4f29-9320-86b3c1ade0ea&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Bathroom sink](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F3211cfb0-b081-42f1-9430-cd37c783ce13%2Flavabo.jpg?table=block&id=7c25ee12-7be0-4a79-9bb7-ec23185c3992&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n### Kitchen\n\n![Kitchen cabinets](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F605fe0b7-d6bd-41f2-b38a-0cae1ca101ab%2Fcocina_armarios.jpg?table=block&id=9f87e304-f045-4b45-8d97-09ca67f7feff&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Kitchen drawers](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F41da9c71-e632-4b2f-9fba-c3e88ee1d3a9%2Fcocina_cajones.jpg?table=block&id=0fb1c1f5-925d-446e-9ff1-3bbd5e94b5fe&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Kitchen refrigerator](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F66b03206-ce45-47a0-964d-50997fccba8e%2Fcocina_nevera.jpg?table=block&id=e3613bf0-01ec-4716-a4c9-c44b8db66632&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Kitchen](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F31f68117-8703-400d-bf93-1648337c2c90%2Fcocina.jpg?table=block&id=962c0f53-441c-48ef-86aa-322c58a96116&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n### Living room\n\n![Living room](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F26b1e2c2-55be-4c06-8d3b-e3be9c1dd091%2Fliving.jpg?table=block&id=3fa0a7f3-8535-49a0-a4bb-6f060913a5f7&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Living room detail](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F693ca849-a67c-40c5-8f2d-b768616d2980%2Fliving_detalle.jpg?table=block&id=e6dc791a-2fd8-41ea-bae5-70662fa26d90&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Dining area detail](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F68598226-f5b6-4983-ae68-1799d5adba8f%2Fcomedor_detalle.jpg?table=block&id=96dbdebd-ddf2-4aa6-a4bb-1e30d25d1c1a&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Dining area](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2Faea7b845-47ff-4350-bd1e-b160dc37f468%2Fcomedor.jpg?table=block&id=a81812b4-9531-4fc3-936a-2850753a43ca&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n### Rooftop terrace\n\n![Rooftop terrace detail](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F388951aa-f6fd-40d1-9dc0-5b8b68637fe4%2Fexterior_detalle.jpg?table=block&id=a02b8a58-6724-43e9-b6db-66bf27b9e60d&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Rooftop terrace living area](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F7b188fca-1266-4a4b-81ff-681d7ea059f0%2Fexterior-living.jpg?table=block&id=b7ff84fd-12ca-40b4-b735-c65d188f223d&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n### Elevator, stairs and entrance\n\n![Elevator](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F5841d439-98b6-40ac-ae57-683ebb2551e5%2Fascensor.jpg?table=block&id=c5f9799a-8a2b-4ae8-b892-702ce85118e0&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Stairs](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2F21448f11-50c5-4e40-b39b-25687db9659f%2Fescalera.jpg?table=block&id=b3ca3a66-f67d-475a-a23b-75c62a5c29fd&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Entrance hall](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2Ffe5d99ad-e66b-44c6-ac73-cda2ca00ef71%2Frecibidor.jpg?table=block&id=fcd1961f-02c1-4780-a5bb-c61ed80e4591&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n![Building entrance](https://notion.hendricksonserrano.com/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fecf29777-a8bd-4fc9-a24e-439cf060c453%2Fd87d9ecb-23fe-47d7-9864-9b5c00ef532c%2Fportal.jpg?table=block&id=ac530e4f-cd54-41f6-b830-20acced261c1&spaceId=ecf29777-a8bd-4fc9-a24e-439cf060c453&width=660&userId=&cache=v2)\n\n## How to get here\n\nOur home is located in the residential neighborhood of Gràcia on a quiet side street near the center of Barcelona, just minutes walking from the famous [Sagrada Familia](https://sagradafamilia.org/).\n\nIf you arrive by plane, you can take a 20–30 minute taxi from the airport for an estimated €35–40.\n\nIf you arrive by car, there are various parking garages in the vicinity.\n\n## What's nearby\n\nThere are many sites to see within walking distance, with others conveniently accessible by bike, taxi or subway:\n\n- [Sagrada Familia](https://sagradafamilia.org/) (5 minute walk)\n- [Parc Güell](https://parkguell.barcelona/) (10 minute taxi or 27 minute walk)\n- Central Gràcia (12 minute walk)\n- Local farmers market (1 minute walk, open daily apart from Sunday)\n- Supermarkets Veritas (all organic) and others, 2 minute walk\n- Convenience store (1 minute walk)\n- Wide boulevard of Sant Joan with kids playgrounds (2 minutes walk)\n- Café with specialty coffee Bar La Camila (6 minute walk)\n- Bakeries Origo and Turris (6 minute walk)\n- Beach (15 minute taxi, 14 minute bike or 47 minute walk)\n- Central Park / Ciutadella (7 minute bike or 30 minute walk)\n- Gothic quarter / El Born / old city (8 minute bike or 27 minute walk)\n- Las Ramblas (10 minute taxi, 10 minute bike or 33 minute walk)\n- Passeig de Gràcia / luxury shopping district (6 minute bike or 15 minute walk)\n- Montjuïc / magic fountain (15 minute taxi or 27 minute subway)\n\nSee [Spain visitor information](https://www.spain.info/en/) for restaurants and other recommendations.\n",
      "heroImage": "barcelona-guest-floor-hero.jpg",
      "heroImageStyle": "keep-proportions",
      "createdDate": "2026-02-04",
      "updatedDate": "2026-02-04",
      "excludeFromListing": true
    },
    {
      "slug": "openclaw-and-the-truth-layer",
      "title": "OpenClaw is powerful at doing but could use some help remembering.",
      "excerpt": "OpenClaw runs on your machine, clears your inbox, and builds skills. The gap is memory you can verify, query, and fix. A truth layer under it gives you provenance, rollback, and the same result every time you ask.",
      "published": true,
      "publishedDate": "2026-02-04",
      "category": "essay",
      "readTime": 5,
      "tags": [],
      "body": "## What OpenClaw gets right\n\nOpenClaw (and the broader Claw/Clawdbot wave) is the first time a lot of people have felt like they have a real personal AI. It runs on your machine. It has persistent memory. It can read your texts, manage your calendar, browse the web, fill forms, and build skills that get better as you use it.\n\nBrandon Wang's [bull case](https://brandon.wang/2026/clawdbot) is a good read: promise extraction from texts into calendar events, price alerts with complex criteria (e.g. \"pullout bed OK if not in the same room as another bed\"), freezer inventory from photos into Notion, Resy booking that intersects your calendar with restaurant availability.\n\nThe agent *does* things. It also *remembers* things. Context accumulates. That's the \"sweet elixir of context\" he talks about.\n\nSo on the axis of \"can the agent act on my behalf and learn my preferences,\" the answer is yes. The gap I care about is the other axis: how that memory is stored and whether it's something you can trust, replay, and fix when it goes wrong.\n\n## Where \"more context\" hits the same ceiling\n\nI run a lot of my life through [one agent (Cursor plus MCP): email, tasks, finance, contacts, content](/posts/agentic-search-and-the-truth-layer). I've hit limits that aren't about retrieval or model size. They're about state.\n\n- **Overwrites with no undo.** The agent updates a contact or merges two tasks. The previous state is gone. There's no versioning, no rollback. Writes are in-place.\n- **No provenance.** When the agent gives a wrong number or a wrong total, I can't trace it back to a specific record or import. I don't know which observation led to which answer.\n- **No canonical identity.** \"Acme Corp\" in one session and \"ACME CORP\" in the next may or may not be treated as the same entity. The agent re-infers each time. There are no stable IDs or merge rules.\n- **Non-deterministic answers.** Same question (\"what's my total spend with vendor X?\"), different answer tomorrow. Missed files, truncated search, or different entity resolution. No way to reproduce or verify.\n- **Tool-bound memory.** What the agent \"knows\" lives inside that tool's memory or that provider's context. I can't use the same contacts and tasks from Claude.ai or ChatGPT. The memory isn't shared across the tools I use.\n\nThose limits don't go away when the agent gets *more* capable or *more* context. They get sharper. The more the agent does (calendar, contacts, tasks, transactions), the more you need a place where that state is first-class: identity, lineage, and the ability to query it deterministically and roll it back when something breaks.\n\n## What a truth layer adds under an agent like Claw\n\nA [truth layer](/posts/agentic-search-and-the-truth-layer) isn't a replacement for the agent. It's the layer *under* it. The agent keeps doing: reading texts, browsing, filling forms, making calendar events, building skills. The layer is where the resulting state lives and how it's queried.\n\n- **Persistent canonical identity.** Contacts, tasks, transactions, events get stable IDs. \"Acme Corp\" and \"ACME CORP\" resolve to one entity by rule, not by per-session inference.\n- **Provenance and audit.** Every record can be traced to a source (import, agent action, user edit) and a time. When a number is wrong, you can see where it came from.\n- **Deterministic queries.** \"Every transaction with vendor X in the last two years\" or \"all tasks for project Y\" hit a structured store. Same query, same result. No re-search, no truncation, no re-inference.\n- **Recoverability.** When the agent overwrites a contact or merges two tasks by mistake, you have versioning and an audit trail. You can see what changed and roll back. Mutations are explicit; they're not silent overwrites.\n- **Cross-tool truth.** The same contacts, tasks, and execution plans are available to Cursor, Claude, ChatGPT, or Claw, via something like MCP. One memory substrate, many agents.\n\nSo Claw (or any Claw-style agent) would still own the \"do\" part: interpret intent, browse, fill forms, create events, learn workflows. The truth layer would own the \"remember\" part: canonical entities, timelines, provenance, and idempotent, replayable queries. The agent writes into the layer and reads from it. You get the lift of an agent that does things *and* a memory that doesn't drift, overwrite without trace, or disagree across sessions or tools.\n\n## Concrete picture\n\nImagine Claw creating a follow-up task after you promise something in a text. Today that might live in the agent's memory or in a local list. With a truth layer, that task is a first-class entity: linked to the conversation that created it, to the contact if relevant, and to any project or execution plan. You can query \"all follow-ups from last week\" or \"tasks linked to this contact\" from any tool that talks to the layer. If the agent later merges two tasks by mistake, you have a history of changes and can revert.\n\nOr: Claw helps you track spending with a vendor. Without a structured store, it re-searches exports and emails each time and re-infers entity resolution. Totals can change. With a truth layer, transactions are normalized and tied to a canonical vendor ID. \"Total spend with vendor X\" is a query, not a one-off assembly. Same question, same answer. And if the agent \"corrects\" a transaction based on wrong inference, you have an audit trail and the option to roll back.\n\nBrandon mentions writing workflows to Notion so he can see what Claw has learned. That's visibility into behavior. A truth layer adds visibility into *state*: what entities exist, how they're linked, where they came from, and how they changed. That's the complement to \"the agent did something.\" \"The agent did something, and here's the state it wrote, with lineage and the ability to fix it.\"\n\n## Why I'm building Neotoma this way\n\nI'm building [Neotoma](https://github.com/markmhendrickson/neotoma) as a structured memory layer with those primitives: entity resolution, timelines, provenance, determinism, and cross-platform access via MCP. I'm dogfooding it in my own agentic stack to see where they matter. The lesson from that work is that [retrieval (embedding-based or agentic)](/posts/agentic-search-and-the-truth-layer) and \"more context\" don't by themselves give you stable identity, verifiable state, or recoverability. Something that does has to sit underneath. OpenClaw and its ecosystem are proving that agents can do a lot. I think the next step is making sure what they do is grounded in a memory layer that you can trust, query, and fix. That's the layer I'm building.\n",
      "heroImage": "openclaw-and-the-truth-layer-hero.png",
      "heroImageStyle": "keep-proportions",
      "createdDate": "2026-02-04",
      "updatedDate": "2026-02-09",
      "ogImage": "og/openclaw-and-the-truth-layer-1200x630.jpg",
      "summary": "- OpenClaw and Claw-style agents excel at doing (calendar, texts, browsing, forms, skills). The gap is remembering in a way that is verifiable, consistent, and recoverable.\n- More context does not fix overwrites, missing provenance, unstable canonical identity, non-deterministic answers, or tool-bound memory. Those limits are about state, not retrieval.\n- A truth layer sits under the agent: the agent keeps doing; the layer keeps state with canonical identity, provenance, deterministic queries, recoverability, and cross-tool access.\n- With a truth layer, tasks and transactions become first-class entities with lineage. Same query yields same result; bad writes can be rolled back; the same data is available to Cursor, Claude, ChatGPT, or Claw via MCP.\n- Visibility into what the agent learned (e.g. workflows in Notion) is behavior. A truth layer adds visibility into state: what entities exist, how they are linked, where they came from, and how they changed."
    },
    {
      "slug": "truth-layer-agent-memory",
      "title": "Building a truth layer for persistent agent memory",
      "excerpt": "Agent memory is forgetful. Why I'm building inspectable, structured memory for trustworthy agentic systems.",
      "published": true,
      "publishedDate": "2026-02-02",
      "category": "essay",
      "readTime": 6,
      "tags": [
        "neotoma",
        "agent-memory",
        "truth-layer",
        "determinism"
      ],
      "body": "I've been working on something called **[Neotoma](https://github.com/markmhendrickson/neotoma)**.[^1]\n\nThere's nothing to try yet. This isn't a launch post, and I'm not announcing a product or asking for signups. The problem has been bothering me for a while, and more importantly, it's been actively getting in the way of work I've been trying to do.\n\nOver the past year, I've spent a lot of time experimenting with agentic systems: automating workflows, delegating tasks to agents, letting systems operate across sessions instead of starting from scratch each time. Again and again, I've run into the same wall. The systems were capable, often impressively so, but I couldn't trust them with real, ongoing state.\n\nThat limitation hasn't just been theoretical. It's been a practical blocker to automation.\n\n## AI systems are quietly changing roles\n\nThey used to be something you just consulted: you asked a question, got an answer, and moved on. Increasingly, they act. They write files and documents, call tools and APIs, refer to past conversations across sessions, and chain decisions over time without being explicitly prompted for each step.\n\nAt that point, personal data stops being reference material and starts becoming *state*.\n\nAnd state has different requirements.\n\n## The thing that keeps breaking is not intelligence, but trust\n\nCurrent AI memory systems are built around convenience. They optimize for recall, speed, and fluency, and for whether the system *feels* like it remembers you. None are built around provenance, inspectability, replay, or clear causality.\n\nIn practice, that means I can get an agent to do something once, but I hesitate to let it do something *again*. Memory changes implicitly. Context drifts. Assumptions accrete. And when something goes wrong, I can't answer what changed, why it changed, or whether the system would make the same decision if I reran it from scratch.\n\nThis is tolerable when AI is advisory but not when it's operational.\n\n## Part of the problem is a category mismatch\n\nWe still treat personal data like notes, text blobs, or loose context. Agents, meanwhile, treat that same data like inputs, constraints, triggers, and long-lived state. You cannot safely automate against data you can't inspect, diff, audit, or replay.\n\nThis isn't a UX problem. It's a systems problem.\n\n## What feels missing is a basic primitive\n\nExplicit, inspectable, replayable personal state.\n\nOther domains solved this long ago. Databases made application state reliable. Event logs made distributed systems understandable. Ledgers made financial history auditable. Personal data never needed that level of rigor before, because humans could carry context in their heads or reconstruct it by reviewing records manually.\n\nAgents change that assumption.\n\n## The uncomfortable implication is that doing this correctly adds friction\n\nState changes can't be implicit.\n\nMemory updates have to be named operations rather than side effects. Inputs have to be visible rather than inferred. History has to be reconstructable rather than hand-waved.\n\nYou give up some magic and accept more ceremony. Otherwise you and your agents will end up living together unreliably through divergent lenses of reality.\n\nThere isn't a shortcut around this tradeoff. Convenience-first systems and agent-safe systems pull in opposite directions.\n\n## I'm treating personal data the way production systems treat state\n\nThat leads to some unavoidable consequences. Behavior has to be contract-first: state changes are explicit, typed operations, not ad hoc updates. Mutations have to be explicit. Nothing \"just updates memory.\"\n\nIf agents are going to act, they need constrained, auditable interfaces rather than opaque prompts or embeddings. Replay matters as much as the current answer: being able to explain how you got here is part of the truth.\n\nSame input always produces the same output since the memory layer is deterministic and agents have a reliable substrate. Changes are immutable and queryable so you can see entity state at any point in time.\n\nMemory comes from both documents you upload and data agents write during conversations, one structured graph unifying entities and events so agents can reason across all of it.\n\nThese aren't aesthetic preferences. They fall directly out of trying, and repeatedly failing, to automate real workflows without losing trust in the system doing the work.\n\n## Why I'm designing it this way\n\nI'm keeping it MCP and CLI-first. There's no web UI and no hidden memory. It's local-first by default, with explicit interfaces for agents. I'm ingesting only what I explicitly provide with no automatic scanning or background ingestion. Those aren't omissions, they're guardrails. They make it harder, accidentally or otherwise, to lie about what the system knows and how it got there.\n\nI'm also making it cross-platform and privacy-first by design. It works with ChatGPT, Claude, and Cursor via MCP, not locked to a single provider. Your data remains yours, user-controlled, never used for training. Those aren't conveniences; they're prerequisites for trust.\n\n## What it's not\n\nIt's not a note-taking app or a \"second brain\"; it's a structured memory substrate for agents.\n\nIt's not provider-controlled ChatGPT Memory or Claude Projects; it's your own substrate, exposed via MCP so any agent can use it.\n\nIt's not a vector store or RAG layer; it's schema-first, structured memory with provenance.\n\nIt's not an autonomous agent or workflow engine or AI assistant with invisible memory; it's the memory layer agents read and write, and you control.\n\n\nAnd it's not something I'd call reliable yet. I'm trying to build the foundation layer before pretending guarantees exist.\n\n## Why now\n\nWe're normalizing systems that take actions on our behalf, persist beliefs, and accumulate decisions over time. When those systems fail, and they will, the first question will be, \"How did this happen?\"\n\nRight now, most tools won't be able to answer that. And over the past year, that inability has been the main thing preventing me from trusting agents with anything that matters. That problem is about to scale.\n\nThe agentic web is emerging. We need one where users remain in control of memory, not one where we hand it to centralized platforms and agents act on our behalf using opaque, unreliable methods. I'm building Neotoma to provide that: a substrate that is inspectable, replayable, and user-controlled as the agentic web grows.\n\n## Upcoming developer preview\n\nI'm working on releasing a developer preview for my own usage and public testing. It will be rough and explicitly unreliable (e.g. APIs may change). Its purpose will be to pressure-test these ideas in real use, not to sell anything.\n\nHow I'm approaching the build: I'm dogfooding it first in my own agentic stack so I can see where determinism and provenance actually help and where they get in the way. Use cases include:\n\n- **Tasks and execution** — Tasks, plans, projects, and outcomes with due dates and follow-up reminders\n- **Contacts and relationships** — Contact records and relationship graph linked to communications, tasks, and events\n- **Communications** — Email triage, workflow-triggered processing, and conversation tracking\n- **Finance** — Transactions, flows, income, holdings, transfer and cost recording\n- **Record keeping** — Purchases, accounts, property and one-off analysis reports\n- **Content** — Posts, personal history, favorite media and consumption sources\n- **Health** — Habits, exercises, and ongoing tracking\n\nI'm prioritizing MCP stability and a minimal CLI before adding more surface area, stress-testing entity and relationship resolution and timeline queries as usage scales.\n\nIf this framing resonates, the work is happening in the open here:\n[https://github.com/markmhendrickson/neotoma](https://github.com/markmhendrickson/neotoma)\n\nStarring the repo is the simplest way to keep track of it as it evolves. Input from people thinking about agentic systems and scalable state is always welcome.\n\n[^1]: Named after the genus *Neotoma* (packrats), known for collecting and preserving material.\n",
      "heroImage": "truth-layer-agent-memory-hero.png",
      "heroImageStyle": "keep-proportions",
      "heroImageSquare": "truth-layer-agent-memory-hero-square.png",
      "createdDate": "2026-02-02",
      "updatedDate": "2026-02-04",
      "summary": "- What keeps breaking AI-based automation is trust, not intelligence. Memory changes implicitly, context drifts, and you can't see what changed or replay it.\n- When agents act, personal data becomes state. The missing primitive is explicit, inspectable, replayable personal state. Other domains have it (DBs, event logs, ledgers). Personal data didn't need it until agents.\n- Doing it right adds friction: named operations, visible inputs, reconstructable history. Convenience-first and agent-safe pull in opposite directions.\n- Neotoma is the \"truth\" layer I'm building. Contract-first, deterministic (same input, same output), immutable queryable state, one graph for documents and agent-written data.\n- The agentic web is emerging. We need user-controlled, inspectable memory, not opaque platforms. I'm building Neotoma as that substrate.\n- MCP and CLI-first, local-first, explicit ingestion only. Cross-platform and privacy-first. Not a note-taking app, provider memory, or vector store. Not reliable yet.\n- Developer preview next. I'm dogfooding it in my own agentic operating system.",
      "ogImage": "og/truth-layer-agent-memory-1200x630.jpg"
    },
    {
      "slug": "professional-mission",
      "title": "Mark Hendrickson",
      "excerpt": "Building sovereign systems at the intersection of crypto and AI",
      "published": true,
      "publishedDate": "2025-01-01",
      "category": "essay",
      "readTime": 2,
      "tags": [],
      "body": "I build systems that restore sovereignty, clarity, and long-range capability to individuals in a world defined by complexity, centralized control, and cognitive overload.\n\nMy technology transforms chaos into structure, volatility into signal, and information abundance into actionable leverage, empowering people to operate with more agency, creativity, and strategic independence.\n\nMy work centers on designing personal infrastructure that is open, privacy-preserving, and fundamentally user-owned. Through data ingestion, contextual modeling, and automation, I eliminate friction and restore the time, mental bandwidth, and autonomy lost to modern digital and institutional systems. This gives people the structural foundation to think more deeply, act more decisively, and build more freely.\n\nI take an antifragile approach: systems grow stronger through disruption, not weaker. The tools I build help people thrive under uncertainty, adapt intelligently to changing conditions, and make decisions from clarity rather than reactivity.\n",
      "heroImage": "profile.jpg",
      "heroImageStyle": "float-right",
      "excludeFromListing": true,
      "showMetadata": 0,
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "the-flip-side",
      "title": "The Flip Side",
      "excerpt": "I've been reading The Flip Side since October and find it's an incredibly effective and efficient way to encounter and digest thoughtful opinions...",
      "published": true,
      "publishedDate": "2019-03-22",
      "category": "essay",
      "readTime": 1,
      "tags": [],
      "body": "I've been reading The Flip Side since October and find it's an incredibly effective and efficient way to encounter and digest thoughtful opinions daily from across the left-right divide in the US. I highly recommend it for bursting your political bubble: [theflipside.io](https://www.theflipside.io)",
      "excludeFromListing": 1,
      "showMetadata": 0,
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "yc-w19-web-developer-contract",
      "title": "Yc W19 Web Developer Contract",
      "excerpt": "Web developers: The founder of a YC W19 company I know is looking to contract immediately for a project involving React, Node, and MongoDB with...",
      "published": true,
      "publishedDate": "2018-12-05",
      "category": "essay",
      "readTime": 1,
      "tags": [],
      "body": "Web developers: The founder of a YC W19 company I know is looking to contract immediately for a project involving React, Node, and MongoDB with hosting on Digital Ocean. If you're interested or know someone who might be, please contact him directly at [jusduan@gmail.com](mailto:jusduan@gmail.com).",
      "showMetadata": 1,
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "facebook-police-state",
      "title": "The Facebook police state",
      "excerpt": "Should Facebook really be responsible for policing the misuse of user datahttps://newsroom.fb.com/news/2018/03/forensic-audits-cambridge-analytica/...",
      "published": true,
      "publishedDate": "2018-03-20",
      "category": "technical",
      "readTime": 2,
      "tags": [],
      "body": "Should Facebook really be responsible for [policing the misuse of user data](https://newsroom.fb.com/news/2018/03/forensic-audits-cambridge-analytica/) by companies that acquire it through legitimate means [then subsequently resell it to other entities](https://www.nytimes.com/2018/03/17/us/politics/cambridge-analytica-trump-campaign.html), for whatever purpose?\n\nFacebook is a broker that can directly control only how users and companies exchange data using its tools. Expecting the company to know – or even try to know – and respond to what happens to that data once data has been exchanged with its tools and taken off-platform is an impossible expectation for it to fulfill. We’re setting ourselves up as a society for continual disappointment and disillusionment having expected too much from a single organization.\n\nWouldn’t it be better to educate the general public about the risk of data reuse and resale should they opt to hand it over to whatever game or app developer through Facebook? Individuals could then decide if they want to assume that risk.\n\nFacebook already gives individuals clear and granular controls on what to share. And journalism is currently raising awareness of what could possibly go wrong should people give away their data thoughtlessly. The mature response to these learnings isn’t to blame Facebook and delete your account; it’s to realize that Facebook has given you great power over your personal data and while many may have shared it unwisely, that doesn’t mean you have to as well.\n\nFacebook and other monolithic networks aren’t social problems because they can’t control what goes on in or around them but rather because we expect and demand that they do.\n\nWe risk characterizing them as omnipotent parental figures that we should benevolently guide the multiplicity and complexity of our interactions while somehow resolving disputes with a fair hand.\n\nBut these aren’t governmental bodies and we’d be unwise to push them into that role by demanding they police data and behavior on their own or as agents of actual governments. They are international, capital-seeking enterprises, not representative bodies established, managed and adapted by referenda and all the protections of republican democracy. It’s hard enough to run a government responsive to the social needs of a specific population in an ever-globalized world. It’s madness to expect a company with nine board members and a user base of billions that spans the entire Earth to draw lines of social acceptability and benevolence let alone try to enforce them.\n\nIf we want paternalistically to prevent companies from exchanging personal data by applying categorical regulation, we already have real governments, locally, nationally and internationally available to do that. They can pass laws and enforce them, no matter how Facebook decides to expand or restrict its APIs and no matter how informed or ignorant its users are.\n\nWant to prevent companies from reselling data without originator consent? Make it punishingly expensive to do so by weiding courts and class action lawsuits while taking care not to circumscribe the very concept of consent by infantilizing individuals and therefore assume consent itself is not pragmatically possible.\n\nDid a company resell your data after promising it wouldn’t? Sue them. Did they never make that promise? Sorry, you’re out of luck. Just as you wouldn’t tell a secret to someone you don’t trust, you shouldn’t give any app access to your sensitive data if you don’t get legally enforceable guarantees to privacy.\n\nThe biggest shame that could come from our crisis of faith in Facebook and other platform providers would be to pressure the company into a defensive posture about the free flow of data and communication in general, effectively becoming totalitarian about its policies for fear of market and government retribution that stems from our impatient imposition of moral and legal responsibility.\n\nUsers will suffer from being treated as increasingly untrusted to share their data freely and whatever the form, whether demographic information, photos, status updates or medical history. Platforms will limit functionality to produce and exchange that data both on-platform and off, and the utility of their software will drop just as we expect it to rise in correspondence with our inflated sense of the pace of technology.\n\nThis will result in a two-pronged, contorted revolt against both the company’s tyranny and its impotence, leading eventually to mass emigration despite network lock-in effects. If this results in the use of decentralized platforms, people will have no choice but to seek social remedy from actual governments – that’d be the best-case scenario of creation arising from destruction.\n\nIf the migration leads to subsequent adoption of other centralized platforms, we risk entering into a cycle of losing progressively more liberty over our digital lives in the name of social safety and due to our lack of faith in higher authorities. The very platforms that are so uniquely situated to give us that positive liberty will be quixotically tasked with limiting it and that faith will only further degrade.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "finding-flow-beyond-distraction",
      "title": "Finding flow beyond distraction",
      "excerpt": "One my highest priorities over the past several years has been to establish a more frequent state of flowhttps://en.wikipedia.org/wiki/Flowpsychology...",
      "published": true,
      "publishedDate": "2018-02-28",
      "category": "technical",
      "readTime": 8,
      "tags": [],
      "body": "One my highest priorities over the past several years has been to establish a more frequent state of [flow](https://en.wikipedia.org/wiki/Flow_(psychology)) in my life.\n\nFlow has become increasingly important to me as I've internalized the belief that the most durable peace and satisfaction derives from an active concentration in the present moment, whatever it may contain. The emphasis on this experiential value is in contrast with my past preoccupation with the pursuit of future achievements, which after thirty-odd years, have proven to be emotionally all too fleeting.\n\nWhile in principle this switch sounds easy enough, its practice requires a constant tending to several conditions, not the least of which is a thorough reduction of distraction. And given that distraction is near constant factor for many of us, the question is whether to reduce it and how.\n\nThere are two main ways I've found that I get distracted – **by external interruptions and by internal ones**.\n\nExternal interruptions are the most obvious in that they're usually seen or heard. However, internal ones are just as pernicious, even if they're often discarded as thoughts that can't or shouldn't be helped.\n\nThe ultimate objective to minimizing both types of distractions is finding myself in a state of mind where I can focus on only one thing at a time and with pleasure instead of struggle. That thing could be a conversation with a good friend, the process of designing a new application, or the writing of a blog post. It could even be just walking down the street of a busy city and enjoying one observation at a time, not passively but through an active engagement with my thoughts, senses and feelings.\n\nThe ways I seek to decrease *external distractions* mainly involve practices to break smartphone addiction and maintain a clean work environment:\n\n- **I turn off all phone notifications, entirely**. In 2018, almost all of us have adopted a crazy habit of allowing any action with any level of importance related to our digital lives interrupt us with a sudden vibration or ding.\n\n  This is simply madness in the name of connectivity. I have an iPhone but have disabled all notifications so that at no point in time will my phone make a sound or vibration and interrupt me. If I want to check what I've missed, I can always open it up and pull down the notification center, which serves more rather like an imposing mailbox.\n\n- **I stop using my phone completely upon arriving home at night and until finishing my morning routine the next day**. Home is a place to recuperate, and if I check my messages or the news there (especially if I'm tired from the day or groggy from a night's sleep), I'm basically inviting the external world to interfere with that recuperation.\n\n  When I arrive at home, I plug the phone into the charger in my laundry room and resist the urge to take it out until I'm heading out the door again after breakfast the following day. If I'm heading to exercise first-thing, I resist checking it even until after I'm done and truly in a good position to react to anything I might see pop up in my digital life.\n\n- **I set up my workstation as neutrally as possible**. I love being around people while I work as a certain level of ambient noise actually helps me concentrate and feel emotionally connected. But it's just as important that I can focus for long stretches of time without distraction, either from my coworking peers, my friends digitally, or the entropy that results from moving between tasks.\n \n  Physically that means situating myself somewhere where people won't interrupt my work sessions often. \n\n  Digitally that means closing all windows and tabs that could possibly provide an avenue to interruption, such as email or Facebook. It also means maintaining inbox zero across all messaging and email interfaces (Gmail, WhatsApp, Facebook, etc), cleaning all files off my desktop and even setting the desktop color and system interface to a neutral dark grey.\n\n  A simple app called [Divvy](http://mizage.com/divvy/) helps me maintain perfectly divided windows, reducing cognitive friction even further by keeping everything in sight with the right proportions.\n\nI've found the key to minimizing *internal distractions* lies in creating well-organized places to tuck away concerns for the moment, as well as structuring time to ignore competing ones without constant ambivalence:\n\n- **I use [Asana](https://asana.com) religiously to track anything I feel I \"ought\" to do**. Instead of carrying various points of obligation around in my head and struggling to remember them at the right time, I organize any personal or professional tasks in Asana and seek to assign most of them due dates, which correspond to when I'll actually address them. This lets me temporarily forget that they even exist, since in a way, they really don't exist until they're actionable.\n\n- **I apply a modified [Pomodoro technique](https://en.wikipedia.org/wiki/Pomodoro_Technique) with [Focuslist](http://focuslist.co/)**. It's often hard to give a specific task my full attention because I'm actively doubting whether I should actually be focused on another priority.\n\n  But I've learned that I can temporarily supress that doubt by setting up 55-minute work intervals wherein I decide upfront the single thing I want to accomplish and commit to focusing on just that until the timer goes off.\n\n  During a subsequent 10-minute break, I not only give myself permission to indulge in any form of distraction but even force myself to do so, creating a sort of rest and reward cycle for myself.\n\n- **I'm an organizational freak about my finances**. Money can be one of the main drivers of stress and distraction, both explicitly through worrying about how to make ends meet and implicitly through the fretting of office politics that arises from feeling beholden to any given employment option.\n\n  For me, having a comprehensive picture of – and plan for – my money reduces that stress even when savings are low. That means obsessing over the details of just how much money I have and how I expect it to change in the foreseeable future in the context of my upcoming needs.\n\n  I use an app called [Foreceipt](http://www.foreceipt.com/) to track every purchase I make manually and map them to expense categories that I want to budget, such as dining and discretionary purchases. At the end of the month, category totals enable me to review precisely just whether I've met or exceeded those budgets and adjust accordingly.\n\n  On a monthly basis, I update a many-tabbed spreadsheet to record the current state of my assets and track recurring changes to them due to income, expenses and savings. Specifically, I allot a percentage of all income to several types of savings accounts (e.g. 10% for \"travel savings\") to automate my financial cushion.\n\n  This gives me an immense amount of peace of mind about how to sustain myself financially and prevents financial worries from arising when I'm not explicitly sitting down to manage them.\n\n- **Everything else gets purged onto paper**. Sometimes none of the above helps me get worries out of my head since they're too abstract or confusing to address proactively, at least yet.\n\n  In such cases, I simply take out a pen and piece of paper to jot down a rough outline of what keeps robbing my attention. The notes can take any form and they're not primarily about deciding what to actually *do* about the thoughts. This simple therapy hinges mainly on the act of getting it all out my head in the first place.\n\n  But after I complete the brain dump, I go through my outline and decide which are thoughts that I want to address and which I simply want to let run their course without any action at all, which is a surprisingly effective way to minimize them when done decisively.\n\n  I meditate on those I do want to address with action until coming up with at least one task that'd concretely help if not resolve the worry completely. That task then, of course, goes into Asana above.\n\n  I find that this paper-based exercise almost always reduces and many times resolves the distraction caused by scattered thoughts by moving them either firmly into my locus of control or out entirely.\n\nApplying all of the above won't, of course, necessarily result in a state of flow. I find it's also contigent on a base level of rest and physical health and often aided by a comfortable yet stimulating relationship with the task or experience at hand.\n\nHowever, in a connected world where many considerations vie for my attention at any given moment, these practices have been invaluable in helping me find that flow and enjoy the autotelic experiences that result with greater regularity.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "alps-snowboarding-recommendations",
      "title": "Alps Snowboarding Recommendations",
      "excerpt": "Any recommendations on the best snowboarding destinations in the Alps with easy access from Barcelona and London? 🏂",
      "published": true,
      "publishedDate": "2018-02-09",
      "category": "essay",
      "readTime": 1,
      "tags": [],
      "body": "Any recommendations on the best snowboarding destinations in the Alps with easy access from Barcelona and London? 🏂",
      "showMetadata": 1,
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "the-outline",
      "title": "The Outline",
      "excerpt": "This is the most impressive media website I've seen in a while, from a creative design and construction quality point of view:...",
      "published": true,
      "publishedDate": "2017-12-10",
      "category": "technical",
      "readTime": 1,
      "tags": [],
      "body": "This is the most impressive media website I've seen in a while, from a creative design and construction quality point of view: [https://theoutline.com](https://theoutline.com)",
      "showMetadata": 1,
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "tutor-catala",
      "title": "Tutor Catala",
      "excerpt": "Algú coneix a un tutor de català bo? Estic buscant un per a fer classes d'una hora i mitja a La Dreta per els matis de dimarts i dijous abans de la...",
      "published": true,
      "publishedDate": "2017-12-10",
      "category": "essay",
      "readTime": 1,
      "tags": [],
      "body": "Algú coneix a un tutor de català bo? Estic buscant un per a fer classes d'una hora i mitja a La Dreta per els matis de dimarts i dijous abans de la feina cada setmana. Tinc un nivell básic però vull parlar amb fluïdesa 🤓",
      "showMetadata": 1,
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "foursquare-swarm-checkins-sync",
      "title": "Syncing Foursquare / Swarm checkins to my website",
      "excerpt": "Neotoma now backs up all of my Foursquare / Swarm check-ins to my Dropbox account whereupon they're republished instantly to my website on a new check-ins page.",
      "published": true,
      "publishedDate": "2017-11-13",
      "category": "technical",
      "readTime": 2,
      "tags": [],
      "body": "[Neotoma](https://neotoma.io) ([on GitHub](https://github.com/neotoma)) now backs up all of my [Foursquare](http://foursquare.com) / [Swarm](https://www.swarmapp.com) check-ins to my Dropbox account whereupon they’re republished instantly to my website on {{#link-to 'checkins'}}a new check-ins page{{/link-to}}.\n\nThis setup relies on {{#link-to 'post' $post-dropbox-website-publishing.id}}the same publishing technique{{/link-to}} as my other website content as well as [recent changes to the Neotoma sync software](https://github.com/neotoma/sync-server/commit/fc5a2a2412ad405f5e1c670f1f6963c4300fe527#diff-6365ffb16fdc3a539e4cda9e40ab2a1cR825).\n\nThat software now transforms Foursquare check-ins copied initially in a proprietary JSON format from its API (e.g. [one from this past weekend](https://gist.github.com/markmhx/52d49a3ed4328c6141271b2640a25eea)) into a cleaner [JSON API](http://jsonapi.org/) format before saving them to Dropbox so [my website software](https://github.com/neotoma/personal-server) can easily understand and serve them as content:\n\n```\n{\n  data: {\n    id: \"foursquare-5a0719c1d4cc9849790606eb\",\n    type: \"check-ins\",\n    attributes: {\n      place-state: \"Catalonia\",\n      place-postal: \"43840\",\n      place-name: \"Corcega\",\n      place-longitude: 1.1382177519242145,\n      place-latitude: 41.076488193500616,\n      place-country-code: \"ES\",\n      place-country: \"Spain\",\n      place-city: \"Salou\",\n      place-category: \"Spanish Restaurant\",\n      place-address: \"C. Major, 31\",\n      photo-url: \"https://igx.4sqi.net/img/general/original/11437_gvUS2Nmh9bAaE7O2PP98sz5TZTzTbzr-wQlEShLGkmU.jpg\",\n      likes-count: 1,\n      foursquare-venue-id: \"4d0bce3d46bab60c9cc82990\",\n      description: \"Primera calçotada de l’any!\",\n      created-at: \"2017-11-11T15:39:45.000Z\"\n    }\n  }\n}\n```\n\nThe exact format I use here is definitely going to evolve as I build out functionality around this data on my website. For example, I plan to break out the places embedded in check-ins into their own files so I can rank the places I visit by frequency. But for now this format provides a quick and simple way to get the check-ins displayed reverse chronologically.\n\nAlso, Neotoma currently conducts a full historic backup of my check-ins when I connect my Dropbox and Foursquare accounts to it, but it doesn't keep watching for new check-ins.\n\nI'll be improving the system shortly to sync all new / future check-ins automatically so they appear on my website as well, both on the above check-ins page and on the homepage where I show my latest check-in up top.",
      "showMetadata": 1,
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "sync-server-dropbox-api-version-2-fix",
      "title": "Dropbox API v2 fix for Neotoma sync server",
      "excerpt": "As part of my working sprint at IndieWebCamp Berlin today, I managed to fix a show-stopping bug that's been in production for sync-server on Neotoma and run a backup job for my latest Foursquare / Swarm check-ins.",
      "published": true,
      "publishedDate": "2017-11-05",
      "category": "technical",
      "readTime": 2,
      "tags": [],
      "body": "As part of my working sprint at [IndieWebCamp Berlin](https://indieweb.org/2017/Berlin) today, I managed to fix a show-stopping bug that’s been in production for [sync-server](https://github.com/neotoma/sync-server) on [Neotoma.io](https://neotoma.io) apparently since September 28, 2017 when [Dropbox fully retired its API v1](https://blogs.dropbox.com/developers/2017/09/api-v1-shutdown-details/) in favor of API v2.\n\nI wasn’t aware of this bug until this week since error handling in production hasn’t been set to notify me (via email or otherwise), but setting up that notification is now [a prioritized task](https://github.com/neotoma/sync-server/issues/87) to avoid silent problems like this one in the future.\n\nAfter digging through the code, it turned out that the [Passport](http://www.passportjs.org/) implementation for Dropbox specifically was not passing an `apiVersion` parameter upon initialization of [its strategy](https://github.com/florianheinemann/passport-dropbox-oauth2), and as such, it was defaulting to Dropbox’s API v1 without my realization.\n\nI’ve added `apiVersion` [as a parameter here](https://github.com/neotoma/sync-server/commit/d9b1f15400201ef962a8dea79a121ad9d996c686#diff-25ac49459f3ccaa62fa691b8b449625cR69) and also as an attribute on the [storage model](https://github.com/neotoma/sync-server/commit/d9b1f15400201ef962a8dea79a121ad9d996c686#diff-430f49ef85b837131a35d1dd553659aeR23), specifically setting it to “2” for Dropbox’s storage document.\n\n*Note: This attribute apparently needs to be a string, not an integer, the latter of which failed to work for me when attempted.*\n\n```\nreq.strategy = new passportStrategy.Strategy({\n  apiVersion: document.apiVersion,\n  clientID: document.clientId,\n  clientSecret: document.clientSecret,\n  consumerKey: document.clientId,\n  consumerSecret: document.clientSecret,\n  callbackURL: `${req.protocol}://${req.get('host')}${path.resolve('/', Model.modelType(), document.slug, 'auth-callback')}`,\n  passReqToCallback: true,\n  profileFields: ['id', 'displayName', 'emails']\n}...\n```\n\nAs a result, Dropbox authentication now works again and I’ve been able to run a backup job for my Foursquare / Swarm check-ins, syncing the most recent ones to my Dropbox since last running backup earlier in the summer.",
      "showMetadata": 1,
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "dropbox-website-publishing",
      "title": "Publishing to my website instantly with Dropbox",
      "excerpt": "My website's content is now populated automatically via Dropbox using Neotoma personal server and publishing software, reducing the friction to publishing, keeping data in sync, and paving the way for content aggregation.",
      "published": true,
      "publishedDate": "2017-11-03",
      "category": "technical",
      "readTime": 6,
      "tags": [],
      "body": "I like files.\n\nWhen I was a kid just learning how to use the computer, I would organize all my files into folders by type: my [Kid Pix](https://en.wikipedia.org/wiki/Kid_Pix) drawings into one, my [SimCity](https://en.wikipedia.org/wiki/SimCity_(1989_video_game)) cities into another, etc.\n\nI still have distinct memories of thinking through the best ways to organize my files so that everything important to me was tucked away nicely and easy to find whenever I wanted it. This may have been an early sign of being a clean freak (or put more nicely, a minimalist) but file organization helped me maintain a peace of mind in an otherwise disorienting and scattered world of computing. It provided me a sanctuary of sorts within a broader realm of adventure.\n\nI remember [when Dropbox came out in 2009](https://techcrunch.com/2008/03/11/dropbox-the-online-storage-solution-weve-been-waiting-for/) that I realized immediately how this digital sanctuary could be extended to the cloud by turning one's strictly “local” collection of files into one that synced to \"remote\" storage on the internet, accessible wherever one went and with whatever device one may have on hand. The power of Dropbox was that I could simply drag all of my organized files into it and they would be instantly transformed from an isolated digital homebase to an omnipresent one, linked and yet independent from the physical reality of any one personal device.\n\nMeanwhile, however, I've built a career on developing websites and applications that are backed by all sorts of *databases*, not the so-called *flat files* I have come to love in my personal life – those that any normal human could open and read without special software. Databases have all sorts of advantages in making it easy to query and relate data, but fundamentally, they fall short in terms of transparency, transportation and transformation when compared to files. It's simply easier to see content, move it somewhere else, or change it when that content is stored regular files as opposed to databases.\n\nThe content we each produce as individuals in increasingly important to the world in which we live, and the nature of the Internet in particular. Each of us is a small publishing house and content producer, and every year we see the growing power of that role we play in the public realm. But we're terrible content *managers* and *crafters*, with either our public or private data. \n\nWe tend to throw our content out there to whatever distribution point makes it easiest to get it in front of other people, but that content ends up as rows in databases we hardly control and displayed on websites we hardly design. Even if you host your own blog on your own domain, chances are the content is stored in a database and on server for which you can't quite remember the password. Sure, it's out there *somewhere*, but it'd be a lot more powerful to put it all at the immediate control of your finger tips, just as you have control over the files on your laptop computer using [a wide range of learned techniques](https://www.youtube.com/watch?v=YtdWHFwmd2o).\n\nAbout a year ago I started to attempt a convergence of my two digital worlds – my sanctuary of private files and my slew of poorly managed public (or semi-public) content online – by releasing a new version of [my website](http://markmhendrickson.com), powered by some custom open-source software I created under the broader tent of [Neotoma](http://github.com/neotoma). The website is powered entirely by flat files that I edit directly on my MacBook, iMac or iPhone. These files are loaded by the software's [server](http://github.com/neotoma/personal-server) piece so that the [website](http://github.com/neotoma/personal-web) piece can load the data it needs to show without involving any sort of database. For example, if you ask the server directly for the content that makes up [this post](http://api.markmhendrickson.com/posts/), you'll see it provides exactly the same content stored as a file on my computer:\n\n![]()\n\nUntil this week, however, any of these files saved to my Dropbox that I wanted to publish to my website had to be copied manually to my website's server using [a script](https://github.com/neotoma/personal-server/blob/fefbdd6eb565958cafb79f94a973a3f6e9438d13/Gruntfile.js#L46), making the convergence I was looking for hardly seamless. My goal has been to hit the save button on my computer's text editor and immediately have any changes go live on my website, without any other extra step required. That way, there's never a question of my content being out of sync between my private and published worlds. I simply have to make the choice as to what content I want private versus public, and that decision is made simply by the organization of files into different folders on my computer. The rest is magic.\n\nI met that goal this week after noticing that Dropbox has [a handy Linux version](https://www.dropbox.com/install-linux) of their sync application that I could install on my website's server and configure to [selectively sync](https://www.dropbox.com/help/desktop-web/linux-commands) just the folders I want to make public. After setting the app up, I now just need to save any file into Dropbox that I want published and wait momentarily for my computer's Dropbox sync application to upload the changes to Dropbox then download them to my server where they'll be published instantly.\n\nI'm hardly the first person to think of this as a technique for personal website publishing. However, I hope to push the principle a lot further by automatically copying my content from a variety of database-driven, corporate services (such as Facebook, Twitter, and Foursquare) to my Dropbox in addition to my independently created content.\n\nThat copied content will then get automatically synced not only to my “local” devices (e.g. laptop) for backup as files but also to my website for republishing any way I choose. Aggregated republishing onto one's own website is one of the main use cases for the Neotoma sync service, and I've decided recently that I want to focus on developing the sync service primarily for this use case, dogfooding it on my own website and setting the system up for friends who want to self-publish in the same way as well.\n\nCopying data from a wide range of services into this system will take time and lots of work, so I'm looking to release support for them iteratively, starting with Foursquare since it's an app I use intensively. I'm currently focused on setting up Neotoma to sync my check-ins from Foursquare to my Dropbox and subsequently to my website where I can list them in reverse chronological order, creating a timeline of my life (with photos and other content related to my daily travels).\n\nThen I plan to sync my tips content from Foursquare as well to create custom-designed city guides on my website. These guides will automatically incorporate both the tips and check-ins data I generate on an ongoing basis to display the most up-to-date information about my favorite places in a way that's beautifully packaged, easy to use, and won't go stale.\n\nIf you're interested in setting up your personal website in a similar way, I'd be happy to chat about your needs and how my efforts here may help you as well. And if you're a developer interested in helping with any of the software involved, let's talk about Neotoma and where you could make an impact.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "park-ranger-grunt-hoist-proxy",
      "title": "New Node.js repos for environment management, deployment and proxying",
      "excerpt": "While working more or less full-time on Neotoma since last summer, I've created a few repositories that are hopefully useful as modules to other Node.js apps in general.",
      "published": true,
      "publishedDate": "2017-04-04",
      "category": "technical",
      "readTime": 3,
      "tags": [],
      "body": "As I’ve been working more or less full-time on [Neotoma](https://neotoma.io) (formerly Asheville) since last summer, I’ve had the opportunity to cut my teeth truly on open-source software for the first time.\n\nI’m happy to say that in addition to working on publicly available repositories specific to Neotoma, I’ve created a few repositories that are hopefully useful as modules to other Node.js apps in general:\n\n- [Park Ranger](https://github.com/markmhx/park-ranger): A manager for environment-specific dependencies such as environment variables, configuration files and SSL certificate files.\n\n  It’s called “park ranger” because a computer program always runs in a given environment, often determined by its device or a specific environment chosen within that device alongside other possible environments. And whom do you seek out when you’re enjoying the natural environment and have questions about it…? That’s right, a park ranger.\n\n  I basically kept rewriting the same utility code across my repositories to handle environment-based differences, mainly between my local development machine and deployment host. So, I refactored it all into this module to speed up code improvements and maintenance going forward. My starting point was [dotenv](https://github.com/motdotla/dotenv) but I quickly realized it was too simple for my needs.\n\n- [Hoist](https://github.com/markmhx/grunt-hoist): A suite of Grunt tasks to deploy Node.js apps to hosts and execute related remote procedures.\n\n  Similar to my experience with Park Ranger, I found myself rewriting slight variations of the same deployment routines across repositories (such as rsync'ing files, running \"npm install\" and restarting the remote server). So I created this set of tasks (which automatically make themselves available to parent projects as npm scripts) to standardize the way in which I approach this. They also greatly simplify my approach to continual development, even as I spin up new micro-services or make quick changes to local dependencies along the way.\n\n- [Proxy](https://github.com/neotoma/proxy): A proxy server for HTTP and HTTPS requests.\n\n  As I began hosting early versions of Neotoma for closed testing, I needed a simple way to support different servers running on the same host across protocols (HTTP vs. HTTPS), ports and subdomains. For example, the same host that uses HTTP to serve the landing page for Neotoma also serves HTTPS requests to its underlying API and both HTTP and HTTPS requests to the actual Neotoma web app running on a subdomain for testing.\n\n  While this repository lives under the Neotoma organization, it can be used by anyone who wants to do the same for their hosts that run multiple servers.\n\nThere are a number of other public repositories under development more directly related to Neotoma that I won’t list here but can be found on the [Neotoma GitHub organization](https://github.com/neotoma). While I haven’t actively sought contributions yet, all of the repositories listed above and in that organization are open to pull requests should you want to make any changes!",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "touch-bar-autosuggest",
      "title": "Touch Bar Autosuggest",
      "excerpt": "\"Hmmm what I've been doing is that I wanted to be a good friend to my life and my life is so far away\" - My Touch Bar autosuggest",
      "published": true,
      "publishedDate": "2017-03-02",
      "category": "essay",
      "readTime": 1,
      "tags": [],
      "body": "“Hmmm what I’ve been doing is that I wanted to be a good friend to my life and my life is so far away” - My Touch Bar autosuggest",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "spanish-non-lucrative-residence-visa",
      "title": "Applying for the Spanish non-lucrative residence visa",
      "excerpt": "Just over a year ago, I moved to Barcelona in Spain to live abroad for the first time. I booked a flight from San Francisco with little else in terms of preparation, an intentionally rash decision to experience what it would be like to land in a new country and improvise the establishment of a life there.",
      "published": true,
      "publishedDate": "2014-10-26",
      "category": "article",
      "readTime": 29,
      "tags": [],
      "body": "Just over a year ago, I moved to Barcelona in Spain to live abroad for the first time.\n\nI booked a flight from San Francisco with little else in terms of preparation, an intentionally rash decision to experience what it would be like to land in a new country and improvise the establishment of a life there. I brought just enough possessions to stuff a carry-on bag and backpack, with my American passport serving as my only legal documentation.\n\nThis was possible since American citizens are permitted three months out of every six to stay in the vast majority of Europe (anywhere covered by the [Schengen Area](http://en.wikipedia.org/wiki/Schengen_Area)) without needing a formal visa. They just stamp your passport when you arrive and (maybe) check it when you leave to see whether you broke that rule.\n\nJust a couple months into living in Barcelona, however, I realized that I wanted to live there for much longer and without frequent interruption. Discussing my options with other American expats, I learned that many opt to simply overstay the implicit three-month tourist visa and hope they don’t get caught, specifically when leaving Europe through customs.\n\nI ended up testing that method by overstaying a couple of months, pushing my initial stay to five. But when I flew out of Barcelona that first time, the customs officer stared at my passport for an anxiety-inducing long amount of time then lightly grilled me for having broken the rule. He let me off the hook without a penalty, but the specter of running into real trouble — perhaps a fine or ban, who knows — during future trips made me think more seriously about obtaining a proper longer-term visa.\n\n# The Options\n\nFast-forward to June during my second stay in Spain on the tourist visa when I finally got serious about doing some research. I started off by visiting an immigration lawyer who had been referred by a group of friends, several of whom consulted with him to manage their own immigration issues. I visited him for an introductory consulting session wherein he laid out a few visa options given my situation:\n\n- **Student visa:** If I were willing and interested in taking Spanish classes at a formal school (as opposed to self-teaching and working with a personal tutor, as I’d been doing), then I could apply for a student visa. The downside would be that I had to pay for and attend classes, somehow scheduling them alongside my other activities. Or, I could simply sign up and pay but not go, but that would mean burn money and risk getting caught, pretty much defeating the purpose.\n\n- **Work visa:** I could find employment with a Spanish company and they could sponsor me to live in Spain. However, I had little interest in working for a local company let alone having me ability to reside in Spain rely on one.\n\n- **Entrepreneur visa:** Apparently, Spanish law had been recently changed to permit entrepreneurs to obtain visas if they could whip up business plans and pitch decks that persuaded the government they would actually be profitable and employing Spaniards in short order. It seemed like a lot of work and, considering my predilection for speculative internet ventures, an exercise in bullshitting my way to a visa.\n\n- **Non-lucrative visa:** I had heard rumors of this type of visa, one in which rich or retired people could basically buy their way into Spain by showing they were financially very well off. But before visiting with the lawyer, my impression was that the minimally required amount would be prohibitively high given my finances. Luckily, he informed me that the bar was actually set at a level the government had determined would simply support a person for a year, to provide assurance that the visa holder would not end up taking a local job or weighing down the welfare system.\n\nThis last option jumped out at me as my path forward. Luckily, due to some decent saving over the previous few years, I had enough savings in the bank to cover the requirement of €25,560 (or about $32,660 at time of writing). And that was really the main requirement. As long as I wasn’t carrying any serious diseases or criminal history with me as baggage, I was looking at fairly smooth sailing.\n\nUnfortunately, I was about to get schooled by the world of government bureaucracy, learning just how frustrating it can be to piece together the procedural details of a visa application with minimal guidance from the Spanish government or anyone else for that matter (my lawyer turned out to be not-so-good at helping me get anything done so I struck out on my own). As such, I’ve learned through personal experience what it’s like to apply for this kind of visa, and I’d like to spare others some of the pain and mystery by relating that experience in detail.\n\n**A big caveat emptor**: I’m not a lawyer so none of this should be taken as authoritative legal advice. I also applied for the visa through the Spanish consulate in San Francisco during the fall of 2014. What follows comes from my single experience, so while it’ll hopefully guide you towards a similar outcome, it’s highly advised that you cross-check any information here with the most official sources you can find. As far as you or I know, the lessons I’ve drawn could be unique to my application, or they may have been dependent on changing protocol, now outdated by the time you read this.\n\n# The Application\n\nThe non-lucrative residence visa application is a [concise document](http://www.exteriores.gob.es/Consulados/SANFRANCISCO/en/ConsularServices/Documents/visas/NonLucrative.pdf), spanning a mere two pages. But therein lie a great number of details and potential pitfalls. Let’s start from the top and work our way down.\n\n> \"This visa allows you to reside in Spain without engaging in any type of lucrative activities.\"\n\nThis basically means you can live in Spain and do whatever you want as long as you don’t work for a Spanish company. \n\n> \"The process may take between 2 to 4 months from the day all documents are presented.\"\n\nThis was the part that gave me the most anxiety. The lawyer in Barcelona assured me it would take just a matter of days to apply and get approved. Others warned it may take a few weeks, still giving me the chance to swing through San Francisco for less than a month and return to Barcelona without missing a beat. But when I read _4 months_, my heart sunk and I spent weeks readjusting my expectations.\n\nThe time between submitting the application and receiving an approval notice ended up lasting only 19 days (just under three weeks). So, while the formal estimate is 2-4 months, that clearly wasn’t accurate guidance in my case and only served to stress me out. I should note, however, that the consulate official told me in person that it would probably take two months, with the quickest in recent memory having gotten processed in three weeks. Perhaps I got lucky or managed to prepare better than the average applicant. Either way, I’m glad it was a pleasant surprise.\n\nThe official also told me that the clock wouldn’t start ticking on my application process until I had returned to the consulate to deliver a final, missing document (the notarized criminal record clearance, which hadn’t been mailed back to me yet). This echoed the official guidance here in the application’s instructions. However, she ended up starting the process without that document, saving me about two weeks of waiting, since I think she was happy with how thoroughly everything else had been prepared. So, it pays to put your best foot forward even if can’t manage to bring everything ready in time for your application appointment.\n\n> \"Once your visa is authorized, we will contact you by email or mail, and you (and all your family members applying for a visa) will have to come in person to this Consulate General within a month with your passport and an itinerary of flight to Spain to obtain the visa.\"\n\nThis is pretty straightforward. I ended up receiving notice of my application approval by email. It stated that I had a month to come back to the consulate in person to pick it up, and that I needed to both book a flight to Spain before pickup. Additionally, because I had indicated I wanted to leave for Spain ASAP, I was asked to notify the official by email a few days before my visa pickup with the flight’s date. Presumably that information would be used to set an official start date for the visa’s one-year applicability.\n\n> \"For information about how to obtain the Apostille of the Hague Convention in the US, please visit: [http://travel.state.gov/law/judicial/judicial_2545.html](http://travel.state.gov/law/judicial/judicial_2545.html)\"\n\nOk, so this was a bit of a head-scratcher for me. \"What the hell is an ‘Apostille of the Hague Convention’ and is it going to be a pain to get it?\" were my first thoughts. The answers turned out to be both simpler and more complicated than expected.\n\n\"Apostille\" is French for…notarization or something? I don’t even know how to pronounce it, but basically it’s an internationally recognized form of notarization. You have to get it from the government, and you have to do so either by mailing it to them or dropping it off in person, then waiting weeks for return processing. In California, that means mailing it to the California Secretary of State or driving to Sacramento, the latter of which may save you a few days but ultimately, it’s a waiting game regardless.\n\nHere’s the catch: an Apostille (which comes in the form of a cover letter with a stamp that gets imprinted half on the original document and half on the letter) is required only for the criminal record clearance. And this clearance, as I detail below, must be obtained from the government as well. However (and this is important!), even though they’re both obtained from the government, you have to do so by processing paperwork with two separate government departments via mail. \n\nOnce you get the criminal record clearance back from one department, you have to mail it out to a different department, leading to more waiting. The department that produces the clearance can’t provide an Apostille for it as well, nor can that department mail the clearance directly to the department that _can_ provide an Apostille for it. Bureaucracy at its finest.\n\nI found [these exact instructions](http://www.sos.ca.gov/business/notary/authentication.htm) for obtaining an Apostille in California. They entail mailing the original clearance document, a basic cover letter, a check to pay $20 in processing fees, and a self-addressed envelope the department can use to send the notarized version back. It took a total of just over two weeks from when I mailed out the original document to when I received it back, notarized by the Secretary of State.\n\n> \"The Consulate of Spain in San Francisco will consider applications for Visas BY APPOINTMENT ONLY. One appointment per person. To schedule an appointment visit our website.\"\n\nThe best official information about the various visa options available, plus a direct link to making an appointment online, can be found [here](http://www.vfsglobal.com/Spain/usa/SanFrancisco/allaboutyourvisas.html).\n\nBook your appointment early. I found that the earliest available slot was three weeks out, so this isn’t something you can plan to do last minute. I also had to reschedule my appointment (something they say you can do only once) because I wasn’t ready  yet, and the earliest reschedule time was also a few weeks out. So while the online appointment-making process is straightforward enough (and free), do it early and reschedule quickly should the appointment date start creeping up and it becomes at all apparent that you won’t be prepared in time.\n\n> \"The consular administration has full authority to evaluate, and request more documents than those submitted by the applicant. The latter is hereby informed that submitting the aforementioned documents does NOT guarantee automatic issuance of the visa. The documents accepted in order to process the visa, will not be returned.\"\n\nIn my experience, the consulate never requested any documents not listed in the application, so I presume they put this caveat in here just to give a heads up that they might do so if things get complicated.\n\n> \"ORIGINAL and ONE PHOTOCOPY of each of the following items must be presented\"\n\nFor paranoia sake, I actually brought _three_ photocopies of every item to the appointment. And I believe that a second photocopy came in handy for at least one or two of them (I can’t remember which). So, spend the additional ink and paper to print out these extra copies and take them with you in an envelope of backups just in case. I brought four envelopes in total: one with the originals, one with the required copies, and one with the double sets of backup copies.\n\n# The Requirements\n\n> \"National visa application form: The application forms must be signed and filled out in print.\"\n\nThis form is fairly straightforward. The typical catch for most people is probably how it asks for a \"Postal address of applicant in Spain\". If you haven’t spent time in Spain and arranged an apartment or some other accommodation already, then this seems like a catch 22. I’ve read elsewhere that you can probably get away with just listing the city to which you intend to move. Fortunately, I had already arranged an apartment in Spain by the time I applied so I listed its address here.\n\nThe form also asks for how many entries are requested (one, two or more than two). My presumption with this visa was that it permits unlimited entries, so I was confused why it would be asking. I marked \"more than two\" and the topic didn’t come up at all during my appointment. I’ll have to see whether this becomes relevant once I start traveling to and from Spain with the visa.\n\nIt also asks for a \"Date of intended entry into Spain\". Given that the consulate encourages (rightly) that you shouldn’t book your flight to Spain before the visa is approved, and that approval takes an unpredictable amount of time, this is also a bit tricky. I simply put a date that was a month out from my scheduled appointment plus the phrase \"or once issued\". The official asked me about this during my appointment and I just emphasized that I wanted to leave for Spain ASAP. This may have helped get my application processed faster as well because they knew I wanted to go immediately.\n\nA note on signatures, for this form and all other documents: the official made a point during my appointment that they had to be hand-written. I have the habit of signing documents electronically with a digitized version of my signature. But she required that I sign each document again in person since this wasn’t adequate for the consulate’s legal purposes. I’d recommend simply signing all originals by hand before making copies to avoid this hassle during the appointment.\n\n> \"Form EX-01: Form must be signed and filled out in print. Only available in Spanish\"\n\nThis form is also easy enough (if you can read Spanish or use a translation dictionary). There are a few checkboxes that basically give legal consent for various things, and you’ll want to simply check these off. The biggest point of confusion was whether I needed to check off any box beneath the \"INICIAL\" section of \"4) TIPO DE AUTORIZACIÓN SOLICITADA\". I checked the first primary box for \"INICIAL\" but left the others below it blank, which the appointment official said was fine.\n\n> \"Original Passport: Valid passport for a minimum of 1 year, with at least one blank page to affix the visa.\"\n\nIf you already have a passport, just bring it with copies (like for everything else). But be sure to check the \"minimum of 1 year\" requirement. I realized that my passport was actually set to expire within a year, so I had to go through the weeks-long process of mailing it in to get a new one. You can do this pretty easily from your local post office. Some even have staff specifically trained for passports, which is useful. But it will add more waiting time that you need to budget properly ahead of your scheduled visa appointment.\n\n> \"Two passport size photos: (White Background, 2x2in) One per application form.\"\n\nI got my photo taken and had these printed easily at a local FedEx Office branch. However, I printed a total of eight since I knew I would need two to apply for my NIE (Número de Identidad de Extranjero) in Spain, which is essentially basic government registration that enables a number of things such as opening a bank account.\n\nI paper-clipped these to the upper-left corner of the two forms above plus their copies, using a total of four photos. During the appointment, though, the official removed the photos from the Form EX-01 and its copy, so I ended up submitting only two (for the national visa application form and its copy, to which she stapled the photos).\n\n> \"Notarized document explaining why you are requesting this visa, the purpose, the place and length of your stay in Spain and any other reasons you need to explain, with a certified translation into Spanish.\"\n\nHere’s what I wrote for my statement of purpose letter:\n\n> September 1, 2014\n> \n> Consulate General of Spain \n> 1405 Sutter Street\n> San Francisco, CA 94109\n> \n> Dear Consulate General of Spain,\n> \n> I am requesting a non-lucrative residence visa to live in Spain.\n> \n> The purpose of my stay will be to learn the Spanish language and culture while I take time off of work to focus on my hobbies and explore the country. I plan to live in Barcelona for a year.\n> \n> I have included bank statements totaling [redacted] in savings, which should satisfy the 25,560€ (or $33,849.11 at time of writing) minimum income requirement for this type of visa. I have also included medical and criminal records that demonstrate how I have no contagious diseases or criminal history.\n> \n> Additionally, I have included the certificate of a Spanish health insurance plan that has no deductible and covers me in excess of 30,000€.\n> \n> Thank you for your consideration.\n> \n> Sincerely,\n> Mark Hendrickson\n\nI wrote the letter in English since a \"certified translation\" into Spanish was required, for this and other documents. Unfortunately, the official instructions are generally very silent on what that means. I emailed the consulate about it and they sent me back a separate document (not found online, as far as I can tell) both explaining the general translation requirements and listing specific local, pre-approved translators I would have to use.\n\nI called around to several of them, which are a mix of seemingly corporate translators and individuals who translate part-time. After having some awkward phone calls with a few (imagine: calling someone’s home phone number and having their spouse pick up first), I found a woman who was very responsive and available to quickly translate this document and the others listed for translation. I emailed her digitized copies of the documents and she translated them in a matter of days. Then she handed off the translated versions (stapled to printed copies of the originals plus her own cover letters explaining the translations) in person in exchange for $250 total.\n\nJust be sure that you go with a translator approved by the consulate since there are others providing translation services that may not be considered \"certified\" by the consulate even if they have other certifications and reassure you they’re applicable.\n\nAs far as notarization for this letter, I simply went to a local UPS Store with notary services and had them perform a simple notarization in person wherein I signed the letter in the presence of the notary and they produced a cover letter with relevant information and stamp.\n\n> \"**Proof of enough periodic income** (investments, annuities, sabbaticals and any other source of income) to live in Spain without working. The minimum income required is 25,560 Euros annually plus 6,390 Euros per each additional family member. All documentation must be certified translated into Spanish.\"\n\nThis requirement contains the crux of this type of visa: finances. Basically, you have to demonstrate that you’ll have enough dough to live in Spain independently.\n\nThe term \"periodic income\" is quite ambiguous here even though they list a few types of income sources. The general theme seems to be _reliable money_ in that the Spanish government doesn’t appear inclined to accept any type of income that may fluctuate, such as a paycheck (oh right, remember you’re not supposed to be working anyway?).\n\nLuckily, basic cash sitting in a bank account counts as income here. It’s a bit of an unwritten rule, but if you can simply print out bank statements showing that you have (and perhaps importantly, have consistently had) enough money in a savings account to meet the minimum income amount, you’ll satisfy this requirement. That’s what I did and they didn’t question any of the record details.\n\n> \"**Police Criminal Record clearance** must be verified by fingerprints. It cannot be older than 3 months from the application date with a certified translation into Spanish. The certificate must be issue from either:\na) State Department of Justice. Original clearance letter form signed (from the States where you have lived during the past 5 years). It must be legalized with the Apostille of the Hague Convention from the corresponding Secretary of the State.\nb) FBI Records, issued by the US Department of Justice – F.B.I. It must be legalized with the Apostille of the Hague Convention from the US Department of State in Washington DC.\nYou must also get a police record from the countries where you have lived during the past 5 years.\"\n\nThis requirement led me on a costly, time-consuming goose hunt. I went with option A using a service called Live Scan, which is apparently the only viable option for requesting and obtaining this clearance.\n\nThe in-person Live Scan service is relatively straightforward once you know where to go and which form to bring. There is a list of [locations in California](http://ag.ca.gov/fingerprints/publications/contact.php) that support Live Scan, namely UPS Stores. You go with the relevant form filled out, they electronically scan your fingerprints, and off your application goes. About two weeks later, it should show up in the mail as a simple letter saying (hopefully!) you don’t have any criminal record on file.\n\nNote that when counting the roughly two-weeks wait for obtaining an Apostille for this letter (as detailed above), the total time for obtaining this clearance with Apostille amounted to just over a month. So, as with everything here, get the ball rolling early.\n\nUnfortunately, I had to go through the Live Scan process twice having screwed it up the first time. It turns out there are at least two different types of Live Scan applications you can submit: one for regular \"record review\" (intended for individuals who want to personally review their record status) and [another specifically intended for visa applications and other immigration-related purposes](http://oag.ca.gov/fingerprints/visaimmigration). \n\nI failed to notice this latter option even existed when first applying for Live Scan, so while the resulting document was perfectly accurate, it wasn’t written in a valid format for my visa application. I heard this bad news over a week into waiting for Live Scan to process, which delayed my overall preparation time and increased my costs. Fortunately, the California Department of Justice was responsive to my inquiry emails; otherwise, I would have waited a week longer before uncovering the mistake.\n\n> \"**Medical Certificate** with a certified translation into Spanish, which should be a doctor’s recent statement (not older than 3 months in doctor’s or medical center’s letterhead) indicating that ‘the patient has been examined and found free of any contagious diseases according to the International Health Regulation 2005’. Must be signed by a M.D.\"\n\nThis turned out to be another, costly requirement due to confusion about how to go about obtaining such a doctor’s letter.\n\nEssentially, all you should have to do is schedule an appointment with a normal doctor, who will ask you some simple questions about your health and perform a routine checkup. You can then have have them write, print and sign a simple letter to satisfy this requirement.\n\nHowever, only after going through an entire checkup with a doctor at my regular health clinic was I told that she couldn’t write this sort of letter for me. It had something to do with how she wasn’t certified to write anything related to international law, meaning she could write the bulk of the needed statement but not the presumably crucial part about \"according to the International Health Regulation 2005\".\n\nI discovered this at the tail end of my appointment even though I had specifically told the clinic about my need to have this letter produced as a result of the appointment, and both the scheduling staff member and doctor had assured me beforehand that it would be possible. I ended up going back to the same clinic to have an entirely new checkup performed by a different doctor who _could_ write the full letter. But unfortunately this meant more time and (luckily, insurance-covered) money. So be sure to get very explicit and considerate recognition ahead of time by your doctor that they can produce what you need before placing yourself on the hook for medical costs by going through the checkup first.\n\n> \"Proof of having international medical insurance while in Spain, with a certified translation into Spanish.\"\n\nThis is one of the more ambiguous requirements. We all know what medical insurance looks like (right?) but \"international\"? What does that mean exactly?\n\nI emailed the consulate to find out and they responded simply that the insurance had to have zero deductible and coverage up to 30,000€. On those bases alone, my individual, private American insurance plan was inadequate. And I was still wary of getting a separate _American_ plan for fear that it wouldn’t cover me properly in Spain.\n\nFortunately, I have [a friend in Barcelona](http://www.premier-spain.com/) who has helped other expats obtain local health insurance for their own immigration needs. I contacted her and she sent me application forms for a standard (?) health insurance plan from the Spanish company ASISA. Within a few days, the application was fully processed and I had a new Spanish insurance plan, which appears to have no deductible or limit to how much they’ll cover (it beats me how that’s possible for the modest monthly price they charge, but hey, I’m American).\n\nThe downside here is that I’m now paying for two individual health plans – one for the US and one for Spain – but using the confirmation letter of my ASISA coverage (sent to me digitally and by default after my application was approved) worked without a hitch. I plan to keep both plans since if I get moderately sick in Spain, I want the option to get fixed up at a local hospital. And if I get majorly sick, I want the option to come back to the US for more intensive treatment.\n\n> \"Authorization form M790 C052, plus the Authorization fee. Please visit our website to check the most recent visa fees. We only accept cash (exact change) or money order. No personal check, no credit cards. The processing fee will not be returned even if the visa is not granted or cancelled.\"\n\nThis form handily comes with precise instructions, in English no less, about how to fill it out. So, there’s not much to worry about here.\n\nWhat did catch me a bit off guard was the expected application fees to be paid during my appointment. I brought $140 in cash per [the outline of fees](http://www.exteriores.gob.es/Consulados/SANFRANCISCO/es/ServiciosConsulares/Documents/Tasas/Tasas.pdf) provided by the consulate, but I ended up paying $14 more since I hadn't noticed that second fee. This was okay since I brought extra cash, which you should as well. The consulate official luckily had change but I’d recommend bringing a handful of ones, fives and twenties just to ensure you can produce whatever extra, exact cash payment may arise.\n\n# The Preparation\n\nIn total, I spent about 4-5 months preparing all of the above items. It’s hard to calculate how many hours were involved exactly but it was a bunch spread out over sprints of a few hours here and there.\n\nMy recommendation would be to start a full half year before you intend to apply, just so you don’t have to panic as I did at various moments when I realized that certain parts would take longer or involve more details than expected. \n\nDecide when you’d ideally like to move to Spain and then work backwards from there. If it takes up to six months to prepare and possibly four months to get approved, that’s nearly a year (10 months) from start to finish. Granted, you could get approved quickly like I did, cutting the processing time to under a month and the total time to something more like five months. But unless you’re already in a time crunch, it’s best to avoid the stress of constantly praying for the best-case scenario.\n\nA big aide in my preparation was a spreadsheet in which I laid out all of the requirements in rows and tracked their status in columns.\n\nThe rows included:\n\n- National visa application form\n- Form EX-01\n- Passport\n- Two passport-sized photos\n- Statement of purpose\n- Income proof\n- Criminal record clearance\n- Medical certificate\n- Proof of medical insurance\n- Authorization form M790 C052\n- Authorization fee of $140 in cash\n- Appointment confirmation\n\nAnd the columns included:\n\n- Obtained?\n- Completed?\n- Translated?\n- Notarized?\n- Physical Original Ready?\n- 3 Copies Printed?\n- Packaged?\n- Ready?\n- Notes\n\nNot every item requires every column. Notarization, for example, only affects two (the statement of purpose and criminal record clearance). But most require the majority and it’s hard to track the exact status of each item unless broken down into this level of spreadsheeted detail.\n\nAlso to organize everything, I made a practice of digitizing every document regardless of whether it was completed yet. That empowered me to reference the various documents on the fly as I worked my way between them, without having to dig up physical records based on my location.\n\n# The Appointment\n\nMy appointment was scheduled for the start of the consulate’s work day (10am), so when I arrived, it was pretty quiet. The San Francisco consulate is a modestly sized building, one that feels as though it was probably converted from a residence at one time. So there was no confusion as to where I had to check in, wait or go to meet with my assigned review official.\n\nThe staff also spoke perfect English and was generally friendly, at least based on what you’d expect from a government office. So it never gave me the vibe that I was being interrogated by a faceless organization, which reassured me I’d be okay should I need to inquire about anything while waiting for my application to process.\n\nMy official accommodated the fact I’d showed up for my appointment missing one document (the criminal record clearance) since it hadn’t been mailed back to me with an Apostille by the Secretary of State yet. She told me to simply swing by the consulate to drop it off without an appointment once it was in my possession. I don’t know how many of these missing items she would have tolerated to accept my initial application, but I got the impression that this sort of piecemeal application is not unexpected.\n\nThe in-person application experience was smooth enough. I waited maybe 15 minutes then went upstairs to sit down at a desk across from the official in her office. We spent maybe 20 minutes total going through the two packets of documents that I handed her: one with originals and one with copies. She lightly reviewed them and asked basic questions as she went. It was nothing amounting to an interview or interrogation of any kind. \n\nOnce she had stapled and collated a bunch of the documents, she told me to come back with the final document then simply wait for the application to process. I got the impression that she’d have to send the paperwork off to another department (one, I later found out, was based in Barcelona) as part of the process and, once she heard back from them, would relay the result to me.\n\n# The Costs\n\nOverall, in addition to the considerable amount of time involved, **I spent roughly $928 during the application process**:\n\n- $33 for eight passport-sized photos and printing at FedEx Office (two for application, two for anticipated [NIE application](http://en.wikipedia.org/wiki/NIE_Number) back in Spain, plus four spares)\n- $110 in password renewal fees\n- $6 to mail passport renewal application\n- $50 for first Live Scan application to receive a review of my criminal record from the CA Department of Justice\n- $50 for second Live Scan application (whoops)\n- $17 to overnight mail my criminal record and corresponding materials to the Secretary of State for an Apostille\n- $20 in fees for Apostille\n- $197 for doctor visit to obtain basic health clearance letter\n- $11 for statement of purpose notarization\n- $280 for certified Spanish translations ($250 plus tip)\n- $154 in Spanish consulate application fees\n\nOn top of that, I’m now paying about 46€ per month for Spanish health insurance.\n\n# The Approval\n\nThe consulate official both called and emailed me once my application was approved, notifying me that I needed to stop by the consulate once more within a month to drop off my passport, have them paste the visa inside of it, and return it to me within the same day. She also requested that I send her basic information about my flight back to Spain so she could match the visa start date with it.\n\nOnce the visa was obtained, I had 90 days to return to Spain and use it at the local police department to obtain a residency card, which would serve as the actual documentation permitting me to stay an entire year.\n\nAfter the first year, I should be able to renew the visa twice from Spain, first after a year and for two additional years, then two years later for an additional two years. Theoretically,  that means I should be able to use this visa for up to five years, extending it as desired from Spain.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "bringing-it-all-back-home",
      "title": "Bringing it all back home",
      "excerpt": "> \"Now, I been in jail when all my mail showed, that a man can't give his address out to bad company\" - Bob Dylan, Absolutely Sweet Marie Last summer...",
      "published": true,
      "publishedDate": "2014-01-30",
      "category": "technical",
      "readTime": 6,
      "tags": [],
      "body": "> \"Now, I been in jail when all my mail showed, that a man can’t give his address out to bad company\" - Bob Dylan, Absolutely Sweet Marie\n\nLast summer I drove across the USA from New York City to San Francisco, mostly by myself. I took my time, stopping in many places along the way over the course of a relaxed month. I didn't have a predefined itinerary; I just wanted to discover and react to what I saw.\n\nThe trip granted me stretches of time to both reflect and record, which accentuated an increasing tendency in both my life and our culture in general. One moment I was ruminating about Civil War history, the other I was cropping a photo of a Robert E. Lee commemorative plaque and writing some commentary to share online.\n\nThe recording was not only an instantaneous way to share a bit of my journey with friends, family and followers. More importantly in the long run, it was my way of creating a breadcrumb trail of moments that would later help me remember and make sense of them.\n\nInternet-connected software, aided by portable devices, has given us an unprecedented ability to create such live journals. But while enabling the online production of historical identities, the companies behind this software (e.g. Facebook, Twitter, Instagram, Foursquare, etc.) present two substantial shortcomings:\n\n1. They keep our own data primarily on their own computers.\n\n2. They provide, support and permit software for accessing our data only when it aligns with their business needs.\n\nI have noodled on this set of problems before, but an epiphany of sorts hit me while walking the streets of Asheville, North Carolina just about a week and a half into my trip.\n\nI had just finished posting a photo to Instagram, tweeting an observation on Twitter, and checking into a restaurant on Foursquare. All three of these actions constituted different ways to reflect and preserve the moment. But I was giving my impressions away to three separate companies that would keep the memories in  fragments on disparate servers over which I had no control. \n\nI had no practical way of bringing all these moments into a single place on which I could depend over the next five, ten, and fifty years. Nor could I access or create a cohesive representation of them, tailored to my liking. These cherished moments were to remain fragments of my identity, adrift in the proverbial cloud unless something changed.\n\nThe epiphany presented a core of a solution: **what if I had copies of all these moments on my Dropbox, alongside all of the other files I store there already?**\n\nDropbox (as well as other sync-based cloud storage solutions that have emerged in its wake such as Google Drive) is a beautiful tool because it bridges the local and network-based data realms. Any files you put into it will gain a dual property: stored both locally on your device, such as a laptop or desktop computer, and accessibly on the internet (i.e. the \"cloud\").\n\nIf Dropbox were to disappear tomorrow, you would still have your files on your computer. If your computer were to get destroyed, you would still have them on Dropbox.\n\nBut most importantly for the epiphany, these cloud storage solutions have APIs that make it possible to add, remove and view files within Dropbox accounts from other software on the internet. They can be used to copy all of the data I am currently giving away to online software companies to my Dropbox account for safekeeping.\n\nFrom this idea was born [Asheville](http://asheville.io), a nascent open-source project on which I have been hacking over the past several months. The project's goal is not only to provide a user-friendly solution for syncing one's content (such as photos, status updates, checkins, blog posts, and reviews) over to a cloud storage account on a continual basis. It is also intended to help people do more with their data once it has been synced, by providing them with ways to make their data available to any number of third party software services (or their own).\n\nIn the purest sense, the project is all about helping people establish proper stakes on the web. We live in the digital age but as individuals, the vast majority of us do not have proper digital homes. Many companies are vying to provide them for us but ultimately, they cannot without playing a zero-sum game with our data. We should be empowered to own our personal online data, maintaining close control so we can use it and open it up to others as we please. Asheville is intended to help make that possible for non-technical and technical people alike.\n\nThere are a lot more details about the project on Github, and there is still much more to do before it will be ready for actual usage. But I have already made a lot of headway on the initial user experience, creating an [Ember](http://emberjs.com)-based web application that provides users with real-time updates on their sync status. Next comes tying this interface to backend software that does the work of actual copying the data from various social networks, publishing platforms and other online services to cloud storage accounts.\n\nI have also just relaunched my personal website in preparation for dogfooding Asheville and showcasing the ways in which it can extend the application of your online data. This website is now built in Ember as well and is therefore [a proper JavaScript app](https://github.com/markmhx/markmhendrickson), not just a flat set of files with no way of processing and displaying data from external sources. Currently, it still shows only a set of blog posts  I have written, but soon I expect to add photo galleries, maps of places I have been, status updates and more. All of this will be powered in large part by the data I have already posted elsewhere online.\n\nIf you are interested in getting involved with Asheville, please do not hesitate to get in touch with me. Thanks already to [Jack Pearkes](http://jack.ly/) and [Ryan Barrett](http://snarfed.org/) for helping out. And if you would simply like to try the product once ready, [leave us your contact information](https://docs.google.com/forms/d/1i2iHhLVcfhYIEHPS5G7iD0gC4z-K-2e535GLGrj_qNE/viewform) for further updates.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "11-reasons-why-i-wont-click-on-your-link-bait",
      "title": "11 Reasons Why I Won't Click On Your Link Bait",
      "excerpt": "1. It blatantly panders to my predispositions. 2. I presume it was written by an underpaid intern. 3. I already saw the content on Reddit. 4. I'm...",
      "published": true,
      "publishedDate": "2013-12-06",
      "category": "essay",
      "readTime": 6,
      "tags": [],
      "body": "1. It blatantly panders to my predispositions.\n\n2. I presume it was written by an underpaid intern.\n\n3. I already saw the content on Reddit.\n\n4. I'm sick of watching animated reaction GIFs.\n\n5. The format screams unoriginality and desperation for visibility.\n\n6. Your subject matter is either mind-numbingly trivial or frustratingly oversimplified.\n\n7. My same friends keep falling for your tricks.\n\n8. You keep planting irrelevant sexual photos as thumbnails.\n\n9. You're dumbing society down, list-by-list, training us to become impatient with content that doesn't come in PowerPoint format or isn't scannable for things with which we easily agree or disagree, because really, who has time for sentences with conjunctions that encourage us take some time to actually *think* when we have other websites to visit so we can waste more time instead of getting our work done and on to something meaningful with our lives?\n\n10. You're making the rest of media envious of your apparent success, infecting them with your ways and eroding their already tenuous motivation to market enriching content.\n\n11. It irritated me enough to write this stupid rant of a parody.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "kanban",
      "title": "Kanban",
      "excerpt": "One of the most important aspects of product design and management is prioritization. There are only so many resources at a company's disposal at any...",
      "published": true,
      "publishedDate": "2012-12-11",
      "category": "technical",
      "readTime": 6,
      "tags": [],
      "body": "One of the most important aspects of product design and management is prioritization. There are only so many resources at a company's disposal at any given time that product efforts must be prioritized vigilantly lest a team end up spending considerable time producing sub-optimal or worse, misguided, work. This is especially true at a startup where resources are extremely constrained and product efforts can either suffer or benefit greatly from acute path dependency.\n\nI've found that the simplest yet most effective tool to prioritize product developments is [kanban](http://en.wikipedia.org/wiki/Kanban), or at least my interpretation of it. A well-organized kanban board can help a team visualize what product ideas currently are or are not getting attention, by what team members, and why. It encourages a \"lean\" mentality by putting ideas into a sequence of design,  development and validation only when there is current capacity to do so. And yet it also provides a staging area for longer-standing ideas that await their turn for production.\n\nYou can use digital software to create a kanban, such as [Trello](http://trello.com), but I actually prefer old-fashioned notecards, tape and markers. To create one, you simply need these materials and a decent amount of wall space. You'll be writing down product ideas onto the cards and then taping them up on the wall under predefined columns and sections depending on where they belong. A big advantage of doing this on the wall is that you can very readily use the board during meetings to communicate product direction without having everyone burrow into their laptops. A wall-based kanban system is also more customizable on-the-fly.\n\nProduct ideas, corresponding to notecards that primarily display their label (e.g. \"Daily Digest Email\"), start off in the left-most \"Icebox\" column and move rightward through \"Design\", \"Develop\" and \"Validate\" columns as they proceed along the production process. Ideas start in the icebox because all ideas deserve justification before they get any significant attention from the team, and as the name suggests, the icebox is where ideas stay frozen (i.e. no one is working on them). Whenever you or anyone else on the team thinks of a clever new product idea, it should be put up on the board in the icebox because then it can be considered for production.\n\nI like to divide the icebox into sub-columns for the organization of ideas by their primary purposes. A primary purpose should be determined for every idea, even if that idea serves several purposes, because otherwise it's hard to justify its enactment and later validate its success. You may find that the purposes you outline on the board vary from product to product, but for many products, you can boil them down to mainly user/customer \"Acquisition\", \"Activation\", and \"Engagement\". For example, the daily digest email example above should probably go under \"Engagement\" because the main hypothesis behind such an email would be that it could increase the engagement frequency and long-term retention of users. A different idea, such as \"Send Invitations Page\", could go under \"Acquisition\" because it's intended to help with viral distribution.\n\nYou can order product ideas under each column based on your current sense of their appropriate prioritization, but any such prioritization should be tentative since ideas ought to be pulled from the icebox as the result of iterative assessments about what's worth resources (perhaps as frequently as on a weekly or biweekly basis). It's also ideal to identify what measurable impact on the product will indicate success for an idea before it's taken out of the icebox and put into production (i.e. you could hypothesize that long-term user retention will increase 5% with the introduction of daily digest emails). Kanban is intended to help you prioritize by product ideas by potential impact, but this shouldn't be interpreted as creating a \"waterfall\" situation where ideas are prioritized concretely and deeply into the icebox. Otherwise, you'll find that you aren't properly making incremental product decisions based on the most up-to-date information at your disposal, and you could get yourself locked into unproductive product directions for months or more at a time.\n\nOnce it's determined that an idea should go into production – because it contains the greatest potential for addressing the most-pressing business needs (such as user engagement or acquisition) – its card gets moved from the icebox and into the \"Design\" column. This column in divided into two rows: \"In Progress\" on the top and \"Done\" below. An idea first goes into the \"In Progress\" row, which indicates that designers have begun actively working on its design. Once the designers have finished all anticipated design work for the idea, it moves to \"Done\", which is where it ought to stay, however briefly as possible, before moving into the \"Develop\" column.\n\nSimilarly to the \"Design\" column, the \"Develop\" column indicates when ideas are getting attention from developers, or engineers. The \"Develop\" column has two rows: \"In Tracker\" on top, and \"In Progress\" on bottom. Because I've used kanban to visualize product prioritization in conjunction with [Pivotal Tracker](http://pivotaltracker.com) for engineering prioritization, the \"In Tracker\" row indicates when a given product idea has been picked up by the engineering team and added to their own pivotal tracker projects, which allows them to break it down into specific engineering tasks. The presence of an idea in the \"In Tracker\" section of \"Develop\" rather than the \"Done\" section of \"Design\" indicates that its aims and designs have been explained and delivered to developers, and the idea is therefore in their court to proceed with execution. When they do start working on an idea (which should be quickly in a well-regulated kanban system), it moves to \"In Progress\" until complete.\n\nSince kanban is best used with continuous deployment, there is no \"Done\" row for the \"Develop\" column; cards simply move to the \"Validate\" column, which indicates that their ideas have gotten deployed to actual users and are awaiting validation. The \"Validate\" column can be split into two rows if you have both a beta and live environment (i.e. when ideas get pushed into beta testing, they should go under a \"Beta\" row, and when they go fully live, a \"Live\" row). Notice that cards aren't simply taken off the board once they've been deployed, since the validation column is intended to remind a team that they need to follow up on whether the idea they built actually had its intended effect.\n\nDeployed ideas stay in the validation column until it's determined that they succeeded according to plan, at which point they're removed from the board. But if they (perhaps more likely) failed or fell short, they are moved backwards through the board to whatever stage is deemed necessary (\"Icebox\" if the team gives up on the idea for now, \"Design\" if its failure is deemed a design shortcoming, or \"Develop\" if it's found buggy or incorrectly implemented). Because it's easy to come up with low standards for validating ideas post-production, you'd do best to use the exact hypothesis you stated originally for an idea when determining whether it validated successfully, even if that means leaving a card in the \"Validate\" column for an extended period of time to gauge its effect.\n\nWhen you begin using a kanban board to track the execution of product ideas, you'll notice the bottlenecks in your overall production process. Cards may begin to pile up in the design, develop or validate columns, indicating that you need more resources in those areas if you want to produce so much at a given time. And if you can't add the required resources, you'll know you need to scale back how much you bite off or break down ideas into smaller chunks, with the goal of keeping each of your resources at full (but no greater) capacity.\n\nIf you begin using such a kanban board – hopefully to discuss product direction at regularly scheduled meetings – you'll inevitably find ways to customize it according to the needs of your particular team and product. And these customizations should be made with an eye towards augmenting the board's ability to prioritize the highest-impact product ideas and speed up the process by which they go from design to validation. If you manage to do this, you'll gain peace of mind that you're making the best product choices possible and following through on them effectively.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "success-projection-for-startups",
      "title": "Success projection for startups",
      "excerpt": "Startups often get so consumed by their day-to-day challenges that they do a poor job actually projecting their longer-term success or lack thereof....",
      "published": true,
      "publishedDate": "2012-12-05",
      "category": "essay",
      "readTime": 6,
      "tags": [],
      "body": "Startups often get so consumed by their day-to-day challenges that they do a poor job actually projecting their longer-term success or lack thereof. As a result, many toil without knowing whether they're truly living up to the expectations they've set for themselves or not.\n\nEven though there is a lot of uncertainty surrounding early-stage companies, they'd do well to build working projection models that give them a sense of whether they're on a road to success or growing too slowly per business metrics that matter to them most.\n\nAll startup stakeholders (founders, employees, investors, etc) ultimately gauge their long-term success by the company's anticipated valuation at the future point of time when they decide to liquidate their equity holdings, perhaps several years or more after they begin working on it. Therefore, any projection of success should place this valuation as its end goal and work backwards from there to derive the more immediate factors that go into achieving that success and whether those factors are on the right course.\n\nA company's valuation is derived (at least theoretically in an efficient market) by the total amount of profit it will accumulate forever into the future, discounted to a present value. So, the first derivation from valuation is long-term profit, and since this profit consists of the company's expected revenues and costs, those come next.\n\nCosts can be derived by projecting headcount and other operational expenditures, perhaps by studying public information about other companies that have developed in the same way as you expect your company to do so, providing rough guidance as to how your particular and total costs may pan out over the years.\n\nRevenue can be projected through comparables as well, especially if you anticipate deploying a business model similar to that of a public company (e.g. you could study companies that make most of their money from display ads if that's what you also intend to do). However, you will likely learn the most from them about what kinds of per-unit rates they get from various monetization schemes (such as subscriptions, e-commerce fees, and advertising CPMs), leaving it to you to determine what kind of product usage volume you anticipate mapping against such expected rates.\n\nIf you are, like many consumer startups, intending to generate revenue from advertising, then the model's key questions are consequently: 1) how many active users do you expect to attain by a given date in the future, and 2) how active will they be per day, week, month or year. And this is because you'll need to estimate the size of an active audience that'll be at your disposal for advertisers. You may come up with innovative ways to increase ad rates, but your future revenue will be mainly tied to how large or small that audience becomes.\n\nWith this being the case, you need to focus on how quickly you are accumulating active users and increasing their engagement, and this is where the model starts to get concrete for even the newest of startups. The number of active users for a product at any given point, now and into the future, is determined primarily by its user acquisition rate (how many people are signing up or otherwise first engaging with a product per a given unit of time), its activation rate (what percentage of those people reach a point of appreciation for the product), and its retention rate (what percentage of those who activate continue to use the product repeatedly).\n\nEach of these factors (as well as others that correspond to non-advertising-based business models) is unique to a given product, and eventually you'll need to project them all if you want to complete the model. However, even if you're at a beta stage with only 50 testers, you *can* start projecting them one-by-one, making assumptions about the rest. You won't have a lot of statistical significance with such a small user base, and it's probable that your key metrics will change as you address a larger market. But it'll at least give you a *baseline* from which you can judge movements towards or away from your ultimate definition of success (i.e. how valuable you want the company to become and how quickly). And it'll keep you honest about whether you truly have enough data to establish knowledge about the business's momentum, and if you do, whether that data reinforce or contradict your more subjective intuitions about how well the startup is doing.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "three-types-of-design",
      "title": "Three types of design",
      "excerpt": "People tend to use the word \"design\" very loosely when talking about product, but it's actually quite important to distinguish between the types of...",
      "published": true,
      "publishedDate": "2012-11-29",
      "category": "technical",
      "readTime": 6,
      "tags": [],
      "body": "People tend to use the word \"design\" very loosely when talking about product, but it's actually quite important to distinguish between the types of design (and their respective designers) when seeking to understand what someone means by the term in any given context, as well as how they apply to the overall product development process.\n\nI tend to divide design into three main types: **product**, **interface**, and **visual**.\n\n# Product Design\n\nThe goal of product design is to generate and prioritize functionality that could potentially deliver value to users in correspondence with the product's stated purpose, or to modify that stated purpose when no such functionality has sufficient potential.\n\nA product designer spends their time mainly thinking about user flows and experiences, which is to say, how users ought to encounter the product at various points in their lifecycles, what they are enabled to do upon those encounters, and how that enablement provides users with additional value.\n\nSuch a design involves the least amount of illustration of the three types, but those in the form of low-resolution diagrams, flow charts, and even rough interfaces can help get the point across about how the functionality should work. Often the output of product design consists of verbal materials, such as outlines and essays that convey how the functionality will suit users' needs and psychological profiles.\n\nA good product designer is aware that prioritization is key to their work because there isn't enough time or resources for all promising ideas and the ones with the most promise must be tackled first. And the product designer must continually map this product prioritization to the company's most pressing business objectives.\n\n# Interface Design\n\nThe goal of interface design is to translate the conceptual functionality conveyed by the product designer and articulate how the user actually experiences  and manages to understand that functionality in the product, on a step-by-step basis.\n\nIf the product is a website, the focus is on arranging and defining various elements on each page that provides the user with information and input. If the product is a mobile application, then the medium is screen-by-screen, and if physical, its available materials.\n\nThe interface designer is most responsible for making the product as intuitively usable as possible so that the highest percentage of users derive the value promised by it. A good interface designer understands the constraints and opportunities afforded by their medium and plays the very empathetic role of envisioning and studying how people of all targeted backgrounds will learn (or fail to learn) how to use the product. And they're intent on ensuring that the interface elements come together in a cohesive whole that makes sense to users architecturally, delivering those elements as wireframes or other medium-resolution materials to the visual designer.\n\n# Visual Design\n\nThe goal of visual design is to ensure that the product conveys a sense of quality and elicits the proper emotional response from its users.\n\nVisual design is the most aesthetic and subjective design type, but it's also the most immediately recognizable one. While visual designers take their cues from product and interface designers, they are responsible for crafting and delivering an ethos for the product. They spend most of their time making interface elements both attractive and appropriately toned so as to reinforce the purpose and value of the product for users, and a good visual designer knows how to make a product pleasurable without making assets that are overly conspicuous.\n\nA visual designer spends the most time on detail, since they sit closest to the user's actual experience. And they deliver high-resolution images, animations or other user-ready elements that can be incorporated directly into the product.\n\n# Interrelation of types\n\nThese types can be treated as a hierarchy, in the sense that product design mainly informs interface design, and interface design mainly informs visual design. And as such, it's more important to execute successfully on the product design front than the other two, because decisions (both good and bad) made in that tier will cascade to the others. And it's hard, if not impossible, to make up for shortcomings in product design with amazing interface or visual design (or, likewise, to make up for poor interface design with a compelling visual design).\n\nHowever, it's not desirable nor even theoretically possible to focus exclusively on product design without investing in the other two types. There's no way to actually manifest your product, let alone in a way that consumers find appealing, without spending a decent amount of time actually thinking through its interface and visual considerations and generating outputs from them. In an extreme yet feasible scenario, you could have barebones interface and visual design paired with solid product design and you might eek out traction in a marketplace, but you'll be making life a lot more difficult for yourself.\n\nThe practical question that startups often face, then, is how much attention to give each of these types of design, especially when their general hierarchy is recognized. Does interface design get 50% of the attention as product design, and does visual get 25% of that? Or do they all get roughly equal treatment, or some other division? The answer basically boils down to how much usability friction users can be expected to tolerate (on the interface front) and how central the notions of quality and emotion are to the product's value proposition (on the visual front) at any given release point.\n\nIf you are distributing a product (perhaps a business tool) to people who will likely find value from even a poor user interface and don't care much about how their tools look and feel, then you probably don't have to spend as much time on interface and visuals. But if your product (perhaps a game) needs to sway inexperienced and skeptical new users that it'll dependably provide them with emotional satisfaction, then you may want to make a thorough investment in all three design types. The consideration, therefore, is ultimately a marketing one.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "the-postmvp-fork-in-the-road",
      "title": "The Post-MVP Fork in the Road",
      "excerpt": "A lot of startups rightly aim to release a minimum viable product MVP as their first initiative. The goal of an MVP helps a team rally around a...",
      "published": true,
      "publishedDate": "2012-11-28",
      "category": "technical",
      "readTime": 4,
      "tags": [],
      "body": "A lot of startups rightly aim to release a minimum viable product (MVP) as their first initiative. The goal of an MVP helps a team rally around a concrete product design that they can get out the door in a limited amount of time. And it's every startup's hope that once the MVP is released, early adopters will flock to it eagerly and tell their more mainstream friends by the millions.\n\nWhen most MVPs get released, however, this dream doesn't exactly pan out. Instead of experiencing healthy growth and engagement, the MVP gets tepid interest from the market. It's neither an entire flop, since there are people who adopt it and do seem to like it quite a bit. Nor is it a clear success, since these people number in the hundreds or thousands, and their numbers don't appear to grow very quickly.\n\nAt this point in the startup's lifecycle, it faces a fork in the road wherein it can decide just what to do about its MVP. Most startups think to themselves: \"OK, it looks like we have the beginning of something interesting even if it's not an instant hit. If we just iterate on the core product and make it incrementally better, we'll hopefully get it to the point where its value to the ordinary user reaches an inflection point and our active users will take off.\"\n\nThis is a very dangerous mindset to adopt post-MVP, because you're inherently conceding that your minimum viable product was, well, unviable. But you're also skirting that reality in practice. If the MVP contained the most important kernel of the experience you were hoping to provide users and that kernel itself didn't get most people who tried it excited (judged primarily by your retention rate), then your fundamental thesis was wrong. Adding extra features, design refinements, and performance enhancements on top of the MVP is not going to change the main lesson you've already learnt, which is that the core concept you've released does not resonate with the market you had in mind.\n\nThe other path, which far fewer startups take upon releasing their MVP to an unenthusiastic market, is to stop and honestly assess what they've already attempted with it. Instead of going head-first into a mode of iteration, startups on this path critique their MVP's core experience and develop hypotheses as to why it wasn't found compelling enough by the market that tried it. These startups either attempt the same MVP with a significantly different market, having decided that the one they initially approached didn't have matching needs. Or they stick to the same market and revamp the fundamentals of their product, essentially going back to the drawing board.\n\nInertia is the reason you don't see many startups make this hard decision to question the essence of their market or product. And this inertia is often created by a whole host of factors, such as founder vision, investor and employee buy-in, and press exposure. When you're running product for a startup, it's much easier to stay headstrong about your initial concept and ignore clear market feedback, since one perceives it as too difficult to reorient the entire game plan and messaging for the product (I was certainly guilty of this with Plancast). It's also relatively easy to rationalize that the market simply needs more time to come around, or that just a couple more features will make it \"click\" with people, especially if you have money in the bank and supportive advisors who don't want to dampen your enthusiasm for it.\n\nBut the bravest and most critical thing that product managers at startups can and must do is treat their products as the experiments they are, maintaining a critical and detached eye for what they're building, at least until those products reach a pace of undeniable growth and engagement that makes it clear a market clamors for them.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "does-your-product-provide-instant-gratification",
      "title": "Does your product provide instant gratification?",
      "excerpt": "Over the past year, and mostly as a result of my Plancast post-mortem, I've spoken  with dozens of entrepreneurs who are working on events-related...",
      "published": true,
      "publishedDate": "2012-11-27",
      "category": "technical",
      "readTime": 3,
      "tags": [],
      "body": "Over the past year, and mostly as a result of my Plancast post-mortem, I've spoken  with dozens of entrepreneurs who are working on events-related internet products.\n\nI had a call with another such entrepreneur today who has actually built and released a considerably robust offering, at least from a feature standpoint. The service makes it possible to interact with event data in myriad ways, from personal sharing of plans among close friends to impersonal broadcasting of events to a wide audience. It consists of not only a website but a mobile client and calendar client synchronization to boot.\n\nThe problem, however, was that any given person who encounters the service will be at a loss as to why they should use it, and this was the case for two reasons: \n\n1. **It's not clear which of the features is primary**. The product doesn't sell itself as useful for any particular use case; rather, it leaves it up to the user to figure a main one out for themselves, which they're unlikely to do unless they already have a clear, burning need.\n\n2. **No one feature in particular provides instant gratification**. Even if the new user does manage to determine their use case, none of the ones available provides value immediately. They all demand that the user invest considerable time, energy and open-mindedness before they can likely get any substantial value back.\n\nThese are problems I see often with events services because they can take many different (and often nuanced) forms. For example, they can be about delivering invitations, discovering social opportunities, interacting real-time, networking with other attendees, and more. As a result, product designers often try to stuff several of these forms into one product.\n\nEvent services also wrangle with the issue of delayed gratification, because if the event data (likely) refer to something in the future, you won't fully appreciate the service until you actually attend the event. It's hard to accelerate the value you get, especially since anticipating events also imposes a good deal of psychological overhead, which isn't pleasurable.\n\nHowever, these are problems from which I see many other types of products suffer as well. If you are building something and you can't pinpoint the point in time when many, if not most, new users can reliably achieve gratification upon their first usage (and by that I mean their first time visiting or registering for your website or downloading your app), then you probably have a structural problem with your value proposition. Especially in this day and age, when people mostly treat new internet services as nice-to-haves rather than needs, your product's value needs to be singular and immediately available or you probably won't get a second look.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "a-postmortem-for-plancast",
      "title": "A Post-Mortem for Plancast",
      "excerpt": "Nearly three years ago, I left my position at TechCrunchhttp://techcrunch.com/2009/03/10/hendrickson-were-gonna-miss-you/ to start my own Internet...",
      "published": true,
      "publishedDate": "2012-01-22",
      "category": "technical",
      "readTime": 13,
      "tags": [],
      "body": "Nearly three years ago, [I left my position at TechCrunch](http://techcrunch.com/2009/03/10/hendrickson-were-gonna-miss-you/) to start my own Internet business, with the idea of creating a web application that'd help people get together in real-life rather than simply helping them connect online as most social networking applications had done.\n\nPlancast was the service conceived a few months later from that basic inclination. Its approach was to provide a really easy way for people to take whatever interesting plans they had in their calendars and share them openly with friends, with the rationale that greater social transparency for this particular type of personal information would facilitate serendipitous get-togethers and enable a greater awareness of relevant events. Personally, I figured that knowing more about the events my friends and peers were attending would lead to a more fulfilling social and professional life because I could join them or at least learn about how they spent their time around town.\n\nAlong the way my team built a minimum viable product, [launched from obscurity here on TechCrunch](http://techcrunch.com/2009/11/30/plancast/), [raised a seed round of funding](http://techcrunch.com/2010/03/08/plancast-funding/) from local venture capitalists and angel investors, and worked like mad to translate our initial success into long-term growth, engagement and monetization.\n\nAlas, our efforts began to stall after several months post-launch, and we were never able to scale beyond a small early adopter community and into critical, mainstream usage. While the initial launch and traction proved extremely exciting, it misled us into believing there was a larger market ready to adopt our product. Over the subsequent year and a half, we struggled to refine the product's purpose and bolster its central value proposition with better functionality and design, but we were ultimately unable to make it work (with user registration growth and engagement being our two main high-level metrics).\n\nThis post-mortem is an attempt to describe the fundamental flaws in our product model and, in particular, the difficulties presented by events as a content type. It's my hope that other product designers can learn a thing or two from our experience, especially if they are designing services that rely on user-generated content. The challenges I describe here apply directly to events, but they can be used collectively as a case study to advance one's thinking about other content types as well, since all types demand serious analysis along these lines should one seek to design a network that facilitates their exchange.\n\n#Sharing Frequency\n\nSocial networks (by my general definition and among which I count Plancast) are essentially systems for distributing content among people who care about each other, and the frequency at which its users can share that content on a particular network is critical to how much value it'll provide them on an ongoing basis.\n\nUnlike other, more frequent content types such as status updates and photos (which can be shared numerous times per day), plans are suitable for only occasional sharing. Most people simply don't go to that many events, and of those they do attend, many are not anticipated with a high degree of certainty. As a result, users don't tend to develop a strong daily or weekly habit of contributing content. And the content that does accrue through spontaneous submissions and aggregation from other services is too small to provide most users with a repeatedly compelling experience discovering events.\n\nI run the service, and even I currently have only five upcoming plans listed on my profile, with a total of 500 plans shared over the last couple of years, in contrast to almost 2,800 tweets on Twitter over the same period of time. People often tell me \"I like Plancast, but I never have any plans to share\". With social networks, this is sometimes a case of self-awareness (such as when people say they don't know what to tweet), but often they're simply telling the truth; many Plancast users don't have any interesting plans on their calendars.\n\n#Consumption Frequency\n\nPeople also don't proactively seek out events to attend as you might suppose. I've gotten into the habit of thinking about people as divided into two camps: those who have lots of free time and those who don't.\n\nThose who do are often proactive about filling it, in part by seeking out interesting events to attend in advance. They are generally more inquisitive about social opportunities, and they will take concrete steps to discover new opportunities and evaluate them.\n\nThose who don't have much free time often desire to conserve it, so rather than seeking out or welcoming additional opportunities, they view them as mentally taxing impositions on a limited resource. For them, planning is a higher-risk endeavor, and usually they'd rather not plan anything at all, since if they're busy, they likely have a preference to keep their free time just that – free.\n\nIt's hard to generalize by saying most people are in one camp or the other, but suffice to say, there are many people in the latter. And for them, it's hard to get them excited about a service that will give them more options on how to use their time.\n\n#Tendency to Procrastinate\n\nEven putting this bifurcation aside, most people resist making advanced commitments before they absolutely need to make them. People fear missing out on worthwhile events but don't actually like to take the deliberate initiative to avoid such missed chances, which requires planning.\n\nThis can be attributed primarily to people's desire to keep their options open in case other conflicting opportunities emerge as the date and time of an event approaches. If they can afford to wait and see, they will. Therefore, their commitment will be secured and shared in advance only when they're particularly confident they'll attend an event, if they need to reserve a spot before it fills up, or if there's some other similar prerogative.\n\n#Incentives to Share\n\nReturning to the topic of sharing plans, it's not only a matter of having interesting plans to share but being compelled to actually share them. And unfortunately, people don't submit information to social networks because they love data set integrity or altruistically believe in giving as much as possible. They do it because the act of contribution selfishly results in something for them in return.\n\nMost social networks feed primarily on vanity, in that they allow people to share and tailor online content that makes them look good. They can help people communicate to others that they've attended impressive schools, built amazing careers, attended cool parties, dated attractive people, thought deep thoughts, or reared cute kids. The top-level goal for most people is to convince others they are the individuals they want to be, whether that includes being happy, attractive, smart, fun or anything else.\n\nThis vanity compels folks to share content about themselves (or things they've encountered) most strongly when there's an audience ready and able to generate validating feedback. When you post a clever photo on Instagram, you're telling the world \"I'm creative!\" and sharing evidence to boot. Those who follow you validate that expression by liking the photo and commenting positively about it. The psychological rush of first posting the photo and then receiving positive feedback drives you to post more photos in the hope of subsequent highs.\n\nSharing plans, unfortunately, doesn't present the same opportunity to show off and incur the same subsequent happy feelings. Some plans are suitable for widespread consumption and can make a person look good, such as attending an awesome concert or savvy conference. But, frustratingly, the vainest events are exclusive and not appropriate for sharing with others, especially in detail.\n\nThe feedback mechanisms aren't nearly as potent either, since coming up with a worthy comment for an event is harder than commenting on a photo, and \"liking\" a plan is confusing when there's also an option to join. The positive feedback of having friends join is itself unlikely since those friends have considerations to make before they can commit, and they'll tend to defer that commitment for practical purposes, per above.\n\nAdditionally, if a user wants to show off the fact they're at a cool event, there is little additional benefit to doing so before the event rather than simply tweeting or posting photos about it while at the event. An important exception is to be made for professionals who style themselves as influencers and want to be instrumental parts of how their peers discover events. This exception has indeed been responsible for much of our attendee-contributed event data among an early-adopter community of technology professionals.\n\n#Selectivity & Privacy Concerns\n\nVanity, of course, is not the only possible incentive for users to share their plans. There's also utility to getting others to join you for an event you'll be attending, but this turns out to be a weak incentive for broadcasting since most people prefer to be rather picky about who they solicit to join them for real-life encounters.\n\nWhile event promoters have a financial interest in attracting attendees far and wide, the attendees themselves mainly turn to their closer circle of friends and reach out to them individually. You don't see a lot of longer-tail plans in particular (such as nights out on the town and trips) because people are both wary of party crashers and usually uninterested in sourcing participants from a wide network.\n\n#The Importance of an Invitation\n\nOn the flip-side of this reluctance to share plans far and wide is the psychological need for people to get personally invited to events.\n\nPlancast and other social event sharing applications are rooted in an idealistic notion that people would feel confident inviting themselves to their friends' events if only they knew about them. But the informational need here is not only one of event details (such as what's going to happen, when, where and with whom). People often also need to know through a personal invitation that at least one friend wants them to join.\n\nWhen you have a service that helps spread personal event information but doesn't concurrently satisfy that need, you have a situation where many people feel awkwardly aware of events to which they don't feel welcome. As a result, the most engaging events on Plancast are those that are open in principle and don't solicit attendees primarily through invitations, such as conferences and concerts, where the attendance of one's friends and peers is a much less important consideration for their own.\n\n#Content Lifespan\n\nGetting content into a social network is not enough to ensure its adequate value; there's also an importance of preserving that content's value over time, especially if it just trickles in.\n\nUnfortunately, plans don't have a long shelf life. Before an event transpires, a user's plan for it provides social value by notifying others of the opportunity. But afterwards, its value to the network drops precipitously to virtually nothing. And since most users don't have enough confidence to share most plans more than one or two weeks in advance, plans are typically rendered useless after that length of time.\n\nContrast this expiration tendency with more \"evergreen\" content types, such as profiles and photos. Other people can get value out of your Facebook profile for years after you set it up, and the photos you posted in college appear to have even increased in value. Nostalgia doesn't even have to play a part; people's hearts will melt upon viewing [this puppy](http://pinterest.com/pin/62065301084425706/) on Pinterest, Tumblr, and other visually-heavy content networks for a long time to come. But how much do you care that [I attended a tech meetup](http://plancast.com/p/7crb/october-2011-ny-tech-meetup) in New York last October, even if you're my friend?\n\n#Geographic Limitations\n\nGeographic specificity is another inherent limitation to a plan's value. Unlike virtually all other content types (with the exception of check-ins), plans provide most of their value to others when those users live or can travel near enough to join.\n\nI may share plans for a ton of great events in San Francisco, but few to none of my friends who live outside of the Bay Area are going to care. In fact, they'll find it annoying to witness something they'll miss out on. Sure, they might appreciate simply knowing what I'm up to, but the value to that kind of surveillance is rather modest all by itself.\n\nThis is especially problematic when trying to expand the service into new locations. New users will have a hard time finding enough local friends who are either on the service and sharing their plans already, or those who are willing to join them on a new service upon invitation. People who encounter the service from non-urban locations have the hardest time, since there aren't many events going on in their area in general, let alone posted to Plancast. Trying to view all events simply listed within their location or categories of interest yields little for them to enjoy.\n\n#Looking Forward\n\nDespite all of these challenges, I still believe someone will eventually figure out how to make and market a viable service that fulfills our aims, namely to help people share and discover events more socially. There's simply too much unearthed value to knowing about much of what our friends plan to do to leave information about it so restricted to personal calendars and individuals' heads.\n\nAnother startup may come along that develops insight into an angle of attack we missed. Or, perhaps more likely, an established company with an existing event or calendaring product will progressively provide users with a greater ability to share their personal information contained within. On the calendaring side, Google is possibly the best-situated with Google Calendar and Google+, which together could make for a very seamless event sharing experience (one of the things we considered seriously for Plancast was deep personal calendar integration, but a sufficient platform for it simply wasn't available). On the events side, companies like Eventbrite, Meetup and Facebook have services that are primarily compelling for event organizers but already contain useful data sets that could be leveraged to create their own social event discovery and sharing experiences for attendees.\n\nPlancast managed to attract a niche audience of early adopters who found it to be among the most efficient ways to share and hear about events (thanks, users! you know who you are). Over 100,000 have registered and over 230,000 people visit each month, not to mention enjoy the event digests we send out by email each day. For that reason alone, and despite its growth challenges, we're going to keep it up and running for as long as possible and are hopeful we'll find it a home that can turn it into something bigger. It's my expectation that one day mainstream society will take for granted the type of interpersonal sharing it currently enables for just this small community, and I look forward to seeing how technological advancements overcome the aforementioned challenges to get us there.\n\n*This post was originally [published on TechCrunch](http://techcrunch.com/2012/01/22/post-mortem-for-plancast/) in January 2012.*",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "share-frequency",
      "title": "Share frequency",
      "excerpt": "When designing a social network that depends on users to contribute content from which they'll collectively derive value, one must consider certain...",
      "published": true,
      "publishedDate": "2011-08-12",
      "category": "technical",
      "readTime": 4,
      "tags": [],
      "body": "When designing a social network that depends on users to contribute content from which they'll collectively derive value, one must consider certain qualities of its supported content types to determine whether those types can provide enough ongoing value to keep users engaged.\n\nAmong these important qualities is the frequency with which people are compelled to create and share a given type of content. People are interested in sharing some types on a seemingly continual basis, spacing out contributions mere hours, minutes or even seconds apart. Conversely, there are types that make sense to share only occasionally when unique opportunities or needs arise.\n\nWhile the suitable frequency of each type varies between individuals, generalizations can be made for evaluation and comparison purposes. For example, status updates, which a given person might find him or herself compelled to produce several times per day, lend themselves to a greater frequency than blog posts, which the same person might publish only every few weeks.\n\nThe general frequency of a particular content type results from numerous factors that affect the costs and benefits of sharing it. All else being equal, types that are easy to produce, such as check-ins or one-off photos, enjoy a greater frequency than those that take more time and consideration, such as restaurant reviews or entire photo albums. Types that return more value to the producer, such a thoughtful answer to a question that earns social acclaim, also enjoy greater frequency than less beneficial types that require the same investment.\n\nIt also seems clear that people, due to their impatience, have a greater cost elasticity than benefit elasticity, in that a little less effort makes a bigger positive impact on frequency than a little more benefit. This asymmetry might help to explain why we've seen smaller, bite-sized types of sharing emerge, whereas we haven't seen as many new services that target types with higher costs yet higher yields.\n\nIt's also possible that with current feedback mechanisms (which provide superficial doses of social validation rather than more impactful, long-standing personal gains), there are simply more apparent opportunities to reduce costs than increase benefits, even if that results in a downward movement of publisher value (and likely consumer value) per share.\n\nEvery type of content has its own set of reasons for why it presents people with higher or lower costs and benefits, and a study of each is necessary to understand their resulting frequencies. When choosing one or more types for a new service, it's important to conduct this study to determine whether they'd yield a high enough frequency to engage users on a continual basis.\n\nHigher frequency generally leads to greater engagement if only because it enables the production of more content within a given period of time and, after all, content is the lifeblood of any social network and needs to accrue. If the value of content is also dependent at least partly on its recency (as is the case with virtually all types, to varying degrees), frequency is even more important because there must be enough new content available at any given time that users decide to engage with the service. The depreciation of existing content essentially needs to be counteracted by fresh content at a sufficient enough rate.\n\nThe need for a relatively high sharing frequency is particularly acute due to an increasing number of services vying for consumers time and attention. Each additional service drives up the minimum value users demand from the next, either as producers or consumers of content. An important question for social network designers, then, is what are the types of content that will provide enough net publishing value that they elicit frequent contributions from their target demographics, especially as their opportunity costs rise.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "homesteading-on-the-indie-web",
      "title": "Homesteading on the indie web",
      "excerpt": "I had the pleasure of attending IndieWebCamphttp://indiewebcamp.com/ in Portland last month, a BarCamp-style conference where techies get together to...",
      "published": true,
      "publishedDate": "2011-07-30",
      "category": "technical",
      "readTime": 10,
      "tags": [],
      "body": "I had the pleasure of attending [IndieWebCamp](http://indiewebcamp.com/) in Portland last month, a BarCamp-style conference where techies get together to brainstorm ideas about how they can help people own and control their online identities.\n\nThe so-called indie web movement, a spiritual cousin to the open source and standards movements, is rooted in a desire for digital freedom, primarily from monopolies that threaten to restrict and violate the common Internet user's online existence. It calls for practical means to protect this existence by preventing or disrupting the control that any one company has over a person's online identity, either from a functionality or data point of view.\n\nIt's a thought-provoking movement for a number of reasons, not least because it finds itself screaming into the wind, so to speak. Most Internet users, with the proliferation of social networks, increasingly place their digital lives in the hands of proprietary services run by mostly private — and always self-interested — companies. These users don't own the identity and content they publish to these services in a way that insulates them from their vague terms of service and the application thereof. Nor can they continue to enjoy those services (at least in the same manner) if the companies shut them down, redesign them undesirably or fail to improve them. Yet, only a small minority of users actively worry about these problems and usually only once they've been stung by account deactivation, incessant downtime, censorship, privacy leaks, or critical design shortcomings.\n\nThere's a moral tone to the indie web movement, not just an insistence that users ought to control their online identities for the practical purpose of avoiding conflicts with their service providers. Proponents argue that the Internet needs to maintain its decentralized nature and resist consolidations of power lest technological progress gets stymied, data gets lost, hoarded or corrupted, and users get disenfranchised en masse. There's a tension here, since private companies that treat their users as [virtual sharecroppers](http://nomoresharecropping.org/) are clearly responsible for much of the progress occurring on the web today, and their services are making it dramatically easier for everyone, including the technically illiterate, to participate online.\n\nThere were two particular challenges to the indie web movement that struck me while attending the conference. The first had to do with identifying the relevant and recognizable needs of the average Internet user to obtain better control over their online identity. Indie web proponents lodge a disparate number of valid complaints against proprietary services, each with its own merit but none that would be recognized by mainstream audiences as a massive, immediate problem on its own.\n\n[Tantek Çelik](http://tantek.com/), the conference's lead organizer and my gracious host, cited the famous downtime of services like Twitter and Tumblr as reason for decentralization, as well as the tendency of acquired services to get shut down. Others cited the desire to more easily export and manage the content they post to services so it can be used on their personal computers and published elsewhere on the web. For others still, it was primarily an issue of personalization and the ability to interact with numerous online services and their respective functionality with more flexibility and fluidity.\n\nAll of these are pain points that are best articulated by technologists who take the time to understand them but are surely felt by \"normals\" as well. They don't, however, seem top of mind enough to compel millions of ordinary Internet users to take concrete steps to address them, at least with today's solutions. Downtime is frustrating but most people learn to work around it; shuttered services disappoint loyal users but most likely faced their demise due to popular disinterest; and most people don't know what else they want out of the services they use, at least substantially enough to seek alternative solutions.\n\nThis complacency poses a critical motivational problem for the primary decentralization scenario proposed by those in the indie web movement, wherein users (both early- and late-adopter alike) take the initiative to host their identity and personal content independently of any proprietary service. The idea here is that everyone should register their own [second-level domain](http://en.wikipedia.org/wiki/Domain_name) and put up a personal website of some sort, just as I've registered markmhendrickson.com and centralized my online identity there. This site could be a simple, static presence or advanced enough to exchange information with proprietary services so that interactions can take place with friends or followers. Theoretically, these proprietary services could get cut out entirely over time, and independent personal websites could begin communicating with each other directly, effectively mapping social networking relationships onto the Internet in a distributed, peer-to-peer fashion.\n\nIn addition to the marketing challenge of compelling individuals to establish these independent sites, there's the technical challenge of bringing this distributed system to life and making it possible for normal people to get involved. The technical challenge can be divided on one side into the infrastructural issues of decentralizing the real-time communications that currently take place within centralized services (such as forging social relationships, posting content to streams, and interacting with that content). On the other side, there are the technical issues of setting each user up within the decentralized system and making sure they have the tools needed to participate without getting tied to any single provider.\n\nEach IndieWebCamp attendee spent the second day of the conference working on a self-chosen project that would aid the movement. I took it upon myself to devise a tool that would perhaps solve the second half of this technical challenge while also communicating to mainstream users why they ought to set up their own domains. My project was primarily user-centric, since it deferred many of decentralization's intricate engineering decisions and instead focused on motivating users to overcome their default complacency and break ground on their own online homestead.\n\nI established several main requirements for this tool:\n\n- It had to simplify for users the process of registering a domain name and a basic web host, both of which had to be treated as commodities and substitutable at any time. While it's not possible or feasible for users to literally own their domain and hosting, the next best thing is to minimize the differentiation power of these services by abstracting them away.\n\n- It had to automate the process of setting up an initial website, or homestead, on the newly registered domain and host, as well as to automate the processes of updating or extending it later on. While the software for the website had to be fully hosted by the user and open-sourced for maximum control, it could be assisted by the tool on an ongoing basis through code and data pushes.\n\n- The user couldn't be expected to use FTP, a command line interface, a file system, or any other technologies beyond the browser because doing so would severely limit its accessibility. User interactions had to be limited to filling out web forms and clicking on things.\n\n- The financial and time burden of using the tool to both set up and maintain a homestead needed to be minimized as much as possible.\n\n- Users couldn't be required to reenter their personal information or manually upload content they've already shared elsewhere.\n\n![]()\n\nThe tool's initial user experience is outlined by the wireframe above. The marketing appeals directly to a person's need for control, since that's ultimately what users are expected to obtain in a decentralized system, it likely resonates with an underlying fear that their current online identity may be in disarray, and it's a vague enough proposition to allow many solution details.\n\nThe page then addresses four of the most identifiable needs under the tent of controlling one's online identity. Obtaining a personal URL allows a user to more easily point people to their information online; ranking well-curated personal information highly on Google allows a user to control what people find out about them when searching their name; listing all of a user's social networking profiles in one place brings order to identity fragmentation; and backing up a user's online content from numerous sources provides peace of mind. The area at the bottom that lists other people's websites is meant to provide social validation for these propositions.\n\nTo get started, the user needs to enter just their desired URL, an email address and a password (with the desired URL checked against a domain registrar's API, assuming one exists). Requests for other values, such as the user's name, are omitted since they can be gathered from the user later on. The goal here is to have them engage with the setup process as painlessly as possible.\n\n![]()\n\nUpon entering this basic information, the user is prompted to connect their new homestead to any number of their online services. A link to each of these services, once connected, will show up on the user's homestead. Content posted to them can also be pulled, either once or continually, for redisplay or simply backup on the user's homestead, depending on what kind of service it is.\n\nFor example, when a user connects their Facebook account, they can choose to have all of their photos and status updates automatically republished to their homestead. Not shown are possible options to simply back up these but not republish them. By connecting with any of these services, the tool can also automatically determine the user's name, portrait and any other details to display on the homestead.\n\n![]()\n\nThe final setup step consists of actually paying for the desired URL, with the assumption that the tool could arrange for free hosting. This part of the mockup isn't fleshed out much, but basically the page would show the appropriate form once the user has chosen their preferred payment method.\n\n![]()\n\nThe result is a profile page not terribly unlike those you'd find on most social networking sites but hosted on the user's own domain and consisting of information about and from the user from a variety of sources. Their service profiles show up on the left along with their portrait and bio, and content they've decided to import into their homestead shows up aggregated on the right.\n\nThis is meant to be just a start. There are a number of ways the design and functionality of a given user's homestead could be advanced. The layout and theme could be customizable. The user could add the ability to post content directly to their homestead and then have it syndicate out to other services. They could even start creating connections with other homesteaders by adding them as friends or the like, all referenced by their own URLs.\n\nPerhaps an open-source ecosystem could even emerge that provided plugins and other modifications to the core software package, eventually enabling social experiences that rival those of proprietary services, with feeds, messages, tags and more. The central accomplishment here would be in enabling large numbers of people to claim independent online presences with the potential to play increasing roles in their online lives. Once enough people have done so, it'll be much easier to weave a indie web between their homesteads and insulate them from the decisions or fate of any particular company.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "content",
      "title": "Content",
      "excerpt": "People are particular about the forms of communication they employ when expressing themselves, through social networks or any other media, because...",
      "published": true,
      "publishedDate": "2011-07-07",
      "category": "technical",
      "readTime": 5,
      "tags": [],
      "body": "People are particular about the forms of communication they employ when expressing themselves, through social networks or any other media, because different forms possess different powers of conveying information.\n\nWhen in the physical presence of others, we can communicate verbally, visually or tactilely with our words, gestures or touch. Words are usually chosen to communicate abstract concepts, finger pointing is best suited to convey direction, and hugs provide the quickest route to imparting fondness.\n\nWhen afar, we can send each other letters, speak to each other over the phone or route a message through a friend. The message might ostensibly be the same despite the form it takes, but a letter will likely impress a greater sense of consideration, a phone call will impart nuances by way of intonation, and a routed message will include the implicit validation of its intermediary. The transmitter must choose their form carefully if they want to get the intended message across because each form has its own abilities and disabilities to deliver information.\n\nLikewise, social networks are constructed around particular forms of communication and consequently limited to the characteristics of those forms. The various forms can be considered types of *content*, because shared information persists within a given network and is intended to benefit its consumers by entertaining or edifying them. As such, it's important to consider the types of content people can share within a network as key to its communicative value.\n\nAll social networks collect identification information from their members and publish it back out as static content. Usually this consists of a member's name and portrait as well as their location and one-line biography. Particularly identity-centric networks collect a lot more static, or evergreen, information such as employment and educational history, music and movie interests, and contact details. The sum of this content is displayed primarily on a single page, which serves to anchor the user's identity within a network and provide a reference point to others. Therefore, networks share the profile as a fundamental content type.\n\nSocial networks almost universally publish some manner of relationship content, too. Friendships, follows, subscriptions, and the like indicate that pairs of people have a relationship between each other that's worth recording and making known. And the types of relationships that can be captured depend on the model a given network has implemented and how that model has been communicated throughout the service. This content – which is often showcased on profile pages but importantly delivered through notifications as well – constitutes yet another fundamental type that varies only in implementation.\n\nThe content differences between social networks, however, mainly come from the types of information that users are able and encouraged to submit as discrete objects. These types are manifold: photos, videos, graphics, status updates, blog posts, articles, documents, books, events, travel plans, travel advice, questions, answers, bookmarks, pokes, reviews, deals, goods for sale, money, vital stats, purchases, gadgets, badges, check-ins, short-form messages, gifts, songs, audio clips, polls, webpages, brands, applications and more. This content is posted proactively by users and its immediate destination is often a feed or profile page. It will likely be repurposed for other consumption points, such as search or syndication, too.\n\nThere is also a host of reactive content types that social networks variably support. These include, most commonly, comments or replies and gestures that indicate approval or disapproval of shared content, such as likes, reposts, favorites or votes. These reactive types are designed to permit direct interaction around pieces of content, allowing the publisher and any other established participant to garner feedback and increase the impact of their contributions. Furthermore, reactive content can be generated in response to other reactive content, thereby extending chains of interaction to deeper levels.\n\nSome social networks support many of these proactive and reactive content types while others specialize in just one or a few. Support may also differ in subtle yet important ways between two or more networks, allowing those networks to convey substantially different information and consequently present dramatically different value propositions to their members.\n\nComparisons aside, every network must be designed around a combination of content types that can be used to fulfill the identifiable communication needs of its producers and consumers. On one side of the equation, a sufficient number of people must be interested in producing a given type of content because it allows them to express themselves in a way they find valuable. On the other, a sufficient (and most likely larger) number of people must be interested in consuming that content because it benefits them in a recognizable way.\n\nSocial network designers must identify not only certain communication needs and their corresponding content types but the frequency and size of those needs as well. Network participation requires commitment on the part of its members, lest they forget or resist leveraging it when their needs arise. And the only way to earn that commitment is to satisfy members' content needs either frequently in small ways or occasionally in big ways.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "relationship",
      "title": "Relationship",
      "excerpt": "When people connect with others on a given social network, they are conscientious about whom they will connect with, because an exchange of...",
      "published": true,
      "publishedDate": "2011-06-04",
      "category": "technical",
      "readTime": 5,
      "tags": [],
      "body": "When people connect with others on a given social network, they are conscientious about whom they will connect with, because an exchange of information, both immediate and ongoing, will result from the connection.\n\nJust as in offline life, people don't like to send and receive information to and from random people; their relationship with those people is crucial. The things you say to those you encounter on the street will differ from the things you say to familiar people in your own home. Conversely, your interest in what strangers have to say will differ from your interest in what your friends can tell you.\n\nThe types of relationships that people experience aren't simply divided between friends and strangers; they are manifold and impossible to label with complete precision. Strictly speaking, no particular relationship gets formed between two pairs of people because nuances invariably come into play. You may be office mates with both Tim and Joe, but you're a bit fonder of Joe because he invites you to lunch.\n\nRelationships also aren't perfectly symmetrical. While you think warmly of Joe, he might think you're kind of a jerk and only asks you to join him because he's interested in your sister. Consequently, any label and assumption of symmetry you assign to a given relationship will constitute an approximation at best.\n\nHowever, approximations are useful when trying to identify the type of relationships a given social network should or does facilitate, because individuals themselves map their relationships to approximate groups. And despite the efforts of designers to diversify the types of relationships that thrive on their networks, consumers tend to view each social network as primarily suitable for only one of their groups.\n\nUnderstanding a group to be simply a set of people who share the same approximate relationship to each other, we can identify an array of such groups that might be facilitated by social networks.  On a high level, there are expansive groups of people you've met and people with whom you've simply communicated. There are also people you admire and people you want to impress.\n\nMore specifically, there are acquaintances from colleges, companies and organizations. There are peers in your industry and collaborators on your specific projects. There are close friends whom you see weekly as well as old friends from high school you see once a year. There are family members and teammates. And there are folks you may or may never have met but who share the same interests as you.\n\nWhatever the group and however specific, it needs to have enough members who both find the group important and desire better ways to share information with each other to warrant a dedicated network. And its importance is often tied to the group's size and its predominance in members' lives. Facebook initially took off among college (and then high school) students because it intensified the already intense relationships that existed within academic communities. Likewise, Twitter and LinkedIn initially thrived by bolstering professionally important relationships within the Silicon-Valley-centric tech scene.\n\nFurthermore, when someone encounters a new network, it's important that they can actually identify which of their relationships it will facilitate and how they will benefit as a result. Otherwise, they are presented with the communications equivalent of a hammer without a nail; they won't know what to do with the social network and it will seem pointless. Similarly, if you signal that the network is meant for a particular type of relationship they don't have, want or care for - or if they feel as though they don't have an unaddressed communication need for that relationship - they won't feel compelled to participate.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "three-pillars",
      "title": "Three pillars of social networking",
      "excerpt": "Social networking is a precondition for new modes of information exchange, not an end in and of itself. By definition, a social network is simply a...",
      "published": true,
      "publishedDate": "2011-05-02",
      "category": "essay",
      "readTime": 5,
      "tags": [],
      "body": "Social networking is a precondition for new modes of information exchange, not an end in and of itself.\n\nBy definition, a social network is simply a set of connections between different people represented by a computer system. These representations wouldn't provide value to anyone if they didn't enable the exchange of information in novel ways. When you friend someone on Facebook, connect with them on LinkedIn, or follow them on Twitter, you aren't doing it for academic purposes; you're doing it to communicate. You don't care about the improved integrity of the social network; you care about the ways in which you can use it to interact with people you care about.\n\nThese networks share important characteristics with one another but crucially differ in even more important ones. The differences are more important, if also more poorly understood, because they allow each of them to present unique ways of exchanging information. If that weren't the case, society wouldn't need more than one social network.\n\nThere are three categories into which these differences can be broken down to better understand market demand for various social networks. First, there is the question of **relationship**, or the significance of the people forging connections to each other on a given network. Second, there's the question of **content**, or the type of information that can be shared across the network. And third, there's the question of **mechanism**, or how that information can be published or consumed, which affects not only its production and distribution but its meaning as well.\n\nThese categories form three pillars of effective social networks. Weaknesses can and inevitably will be tolerated within any given pillar, but each must be strong overall or the network won't constitute a compelling way to exchange information. Additionally, new social networks must differentiate themselves from existing ones by establishing at least one (but not necessarily all) of these pillars differently, thereby giving people a reason to adopt another network.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "how-to-pitch-a-tech-blogger",
      "title": "How to pitch a tech blogger",
      "excerpt": "I've been asked privately quite a few times over the last couple of years how one should pitch their startup to a tech blog like...",
      "published": true,
      "publishedDate": "2010-09-26",
      "category": "technical",
      "readTime": 5,
      "tags": [],
      "body": "I've been asked privately quite a few times over the last couple of years how one should pitch their startup to a tech blog like [TechCrunch](http://techcrunch.com), [GigaOm](http://gigaom.com), [VentureBeat](http://venturebeat.com) or [ReadWriteWeb](http://readwriteweb.com). So I've decided (quite selfishly) to write a post about the subject instead of repeating myself or re-forwarding emails.\n\nThis comes from my experience as both a tech writer (for TechCrunch, ~1.5 years) and internet startup entrepreneur (for [Plancast](http://plancast.com), also ~1.5 years), so I've been able to see things from both sides of the table, particularly when it comes to PR for newly founded startups. As such, these are principles that I primarily recommend to unproven entrepreneurs with unknown companies who want to launch publicly for the first time. Once an entrepreneur or their company gains visibility, their approach to PR will evolve and the press may end up coming to them for news instead of the other way around.\n\n# The Story Is Key\n\nWhen you pitch a blogger – or any writer for that matter, whether they work for The New York Times or your local paper – it's crucial to recognize their desire to identify and then write a *story*. And by story, I mean something that starts, continues, completes or encapsulates a narrative. Bloggers have no interest in merely reporting facts detached from meaning. And they certainly don't want to report facts that actually have insufficient significance to their readers. Bloggers dread the idea of someone coming along and justifiably saying \"so what?\". Good narratives prevent that. Great narratives are thought-provoking and get further developed in readers' minds.\n\nNow, you obviously don't have the power to directly dictate which narrative a blogger will craft as the result of your pitch (no matter how many pay-for-publish conspiracies you've heard). But it's important to think about a narrative for your company or product, because you *can* and *should* steer the blogger towards it. Why? Because bloggers are strapped for time and don't possess the same depth of domain expertise as you. Lay out a narrative that jibes well with their preconceptions and they'll likely run with some form of it.\n\nIt helps to recognize some of the more common types of narratives. If you read through the headlines on [Techmeme](http://techmeme.com/), you'll find that most fit into at least one of the following:\n\n- **Competitive or Political Drama** - aka \"company X releases product Y to kill company Z\"\n\n- **Gossip** - \"CEO of company X gets tangled up in Y\"\n\n- **Insight** - \"trend X will change the world because of A, B, and C\"\n\n- **Evolution & Confluence** - \"service Y is like X for Z, capitalizing on the recent developments of A and B\"\n\n- **Success** - \"company X has created super impressive technology Y, is growing fast, or has made lots of money\"\n\n- **Failure** - \"company X is dying or has messed something up\"The idea is to figure out which type you want to adopt and then craft the facts of your announcement into a compelling and succinct narrative that conforms to it. You'll likely opt for type #4 or #5, but don't hesitate to spice it up with a bit of #1 or #3 (the story can have sub-narratives, but expect the blogger to lead with only one). This isn't an exercise in stretching the truth or making stuff up; there's a reason why you've built what you've built or done whatever you're announcing. Weave that reason into a bigger story while avoiding as many buzzwords as possible.\n\nWhen framing your narrative, you'll do well to remember that bloggers are creatures of comparison. They'll immediately try to compare your product or announcement to another they've already seen, and if they find a close match, they'll pass on it. You should get out in front of this reaction by emphasizing the characteristics of your announcement that [make it unique](http://www.sethgodin.com/purple/). But don't insist that it is incomparable; on the contrary, be forward about drawing comparisons that will highlight the significance of its uniqueness. The writer should come away from your pitch thinking \"I've seen cows before, and this is indeed a cow, but it's purple! All of the other ones I've seen are only black and white\" **not** \"This guy insists this purple thing is not a cow but it obviously is. It might be worth writing about the fact that it's purple but I'm not sure; it feels as though I'm being pitched another cow\".\n\n# Relationships Matter\n\nThis may sound like psychological manipulation directed towards selfish ends (i.e. sales) but if that's how it feels, you're doing it wrong. The goal here is to help the blogger, not exploit them. When you help them (with well-articulated material for a story), they help you (with a story that will publicize your business). As with all transactions, it relies on a relationship, however temporary. And the success of that relationship will depend on how much trust and rapport you've established.\n\nA lot of times when entrepreneurs are ready to pitch, they go looking for a friend who knows and can refer them to a writer. The idea here is to leverage someone else's relationship to validate themselves transitively. This is all fine and good, and it's certainly better than submitting a story to a writer cold. However, it's much better to begin building a direct relationship with them well before the pitch.\n\nOne of the beautiful things about the internet is that you can develop relationships with people without ever meeting them. Get on your favorite bloggers' radars by commenting thoughtfully on their posts, retweeting and replying to them on Twitter, and submitting promising tips to them for stories that have nothing to do with your company. If you blog, take the time to write pieces that link to their pieces; they'll most likely read them and take note of your name. If you happen to live in their area, introduce yourself and chat with them casually at an industry event without giving an elevator pitch unless they ask.\n\nThe point is to achieve some level of familiarity and validation before ever pitching them on a story, not to become their best friend. In fact, you don't want to be too overeager or complimentary, otherwise they'll perceive you (rightfully) as a suck-up.\n\nWhen you're ready to pitch, make sure you're not wasting their time with material that can't be delivered as an interesting story. A litmus test is whether you'd honestly be interested in reading about your announcement if you weren't the one behind it. And when presenting the story, keep it real. Certainly don't embellish or lie about anything. Build trust by throwing in a few facts that, if published, might not make you look so good. If you must, just ask the blogger to please not publish them and they won't, but you'll gain credibility in their eyes.\n\n# A Straightforward Procedure\n\nAs far as the mechanics of delivering a pitch, it's best to ping a blogger about the announcement you'd like to make about a week beforehand. Describe it in one paragraph (no more, no less), suggest the time you'd like them to write about it, and ask them if they're interested and want to hear more. If they respond in the affirmative, send them a few more paragraphs with details and some visuals (e.g. screenshots or demo video) or private access to an alpha product, if relevant. **Do not send them a press release; it will only insult their intelligence**. \n\nTry to be flexible on the timing if they're busy, and if you must pitch the same announcement to more than one blogger (not advisable for unknown startups who should bolster the value of their story with exclusivity), be completely forthright about it and your reasons for doing so. Resist the urge to propose an embargo; [they only cause frustration](http://techcrunch.com/2008/12/17/death-to-the-embargo/).\n\nOnce a blogger has written about you, don't embarrass them by being the first to comment with \"thank you for writing about us!\". Do your part in promoting the piece by getting friends and family to retweet, post to Facebook, etc. And space things out before pitching them again so they don't grow tired of you or the subject. \n\n# Your Company's Best Representative\n\nIf this procedure sounds simple enough, you can craft the most compelling story for your company or product, and you have the time necessary to build these relationships, then you shouldn't hire anyone else to handle PR for you. It'll only be a waste of money, and you'll get less than optimal results. In any case, bloggers much prefer to work directly with executive-level representatives than PR firms, so you'll be doing them a favor.\n\nAs you scale your business, or if you find any of this particularly daunting, then perhaps you should seek professional guidance. But otherwise take this as an opportunity to develop a new skill set and relationships that'll serve you well even beyond your current startup.\n\nGood luck!",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "some-latenight-ideas-for-blippy",
      "title": "Some late-night ideas for Blippy",
      "excerpt": "I feel a certain kinship towards the founders of Blippyhttp://blippy.com/. Not because I know them well I've met Philip Kaplan aka Pud only once but...",
      "published": true,
      "publishedDate": "2010-01-18",
      "category": "essay",
      "readTime": 5,
      "tags": [],
      "body": "I feel a certain kinship towards the founders of [Blippy](http://blippy.com/). Not because I know them well (I've met Philip Kaplan aka Pud only once) but because they're pushing the limits of what people are willing to share about themselves online. While we at [Plancast](http://plancast.com/) are encouraging folks to be more open about their future whereabouts, the team behind Blippy is hoping that people are ready to share their purchases with the world.\n\nBoth of our services are also very new, and as to be expected with new web services, there's still lots of work to be done on both. In the spirit of tech camaraderie, I thought I'd offer up a few (unsolicited) suggestions for Blippy.\n\n- **Give us digests**. The current user experience is primary centered around a mostly reverse chronological, FriendFeed-like stream of purchases. This is okay but I'd prefer to check Blippy as often as I check Mint (which is to say, once a month). And when I do, I'd like to see an overview of sorts that breaks my friends' spending habits down. Tell me what their biggest and smallest purchases were; their strangest purchases; their spending habits (have they been splurging on clothes? buying a lot of airline tickets?); and overlap in their spending (who's buying the same things? what are the trends among my friends?). Pretty graphs might help. Maybe incorporate some maps so I see where about town people are spending their money. Who knows, I might visit once a day if the data updates constantly with new trends.\n\n- **Provide more info about the purchases**. Right now each purchased item is displayed in tiny blue type. Blow that up if it's available and give me context (a URL to where I can buy/view more info), an image, and a description. Show me who else I care about has also bought it.\n\n- **Let us condense/hide comments**. I realize that much of the interaction onsite right now is around the comments people make on purchases. But I'd personally rather locate an interesting purchase *then* choose to view the comments around it.\n\n- **Add a \"Want\" button**. The \"like\" button is a step in the right direction, but perhaps a \"I Want This\" button would be more valuable. It signals a higher level of interest in the purchase, leaving simple \"oh that's cool\" expressions for the comments. You could have profiles list not only purchases users have made but the items of their friends they want. Analyze this data in aggregate to see who starts purchasing trends (fashion being an obvious area).\n\nMy 2 cents.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "plancast-in-public-beta",
      "title": "Plancast in public beta",
      "excerpt": "I'm happy to announce that the site I've been working on for the past half year or so - Plancasthttp://plancast.com/ - is now available in public...",
      "published": true,
      "publishedDate": "2009-11-15",
      "category": "technical",
      "readTime": 2,
      "tags": [],
      "body": "I'm happy to announce that the site I've been working on for the past half year or so - [Plancast](http://plancast.com/) - is now available in public beta. If you haven't tried it out yet, please give it a whirl and [send us your thoughts](http://plancast.com/contact). I suggest you get started by posting a few plans that may be tucked away in your personal calendar. Share them on Plancast and you'll be surprised by how positively other people respond after hearing about them.\n\nWhat does Plancast do, you ask? It helps you share your upcoming plans with friends and learn about what others will be doing in the future. Imagine how awesome it would be if we all had a better idea of what everyone was up to in the next few hours, days, weeks, or months. Thinking about grabbing drinks with friends tonight? Going to a concert tomorrow? Heading to a conference next week? Taking a trip next month? Great, it takes just seconds to share each of these plans. Your plans will reach not only your subscribers on Plancast, but your friends on Twitter and Facebook as well, if you so desire. The service a great way to spread the word about the informal social activities you do every week, whether or not you're looking for people to join you.\n\nIt's been an amazingly fruitful journey getting to this point, and things are especially exciting now that the site is finally live and in such good shape. I launched an \"alpha\" version at the beginning of September, but since it was so rudimentary, I only sent it to a handful of people for testing. With this more functional beta version (released just this past weekend), I'm encouraging everyone to check it out and invite some friends along. We're already seeing a broad range of people take a liking to it.\n\nI'm also happy to announce that I've brought aboard [Jay Marcyes](http://marcyes.com/) as a co-founder. Jay is a programming beast, not to mention a thoroughly nice guy, who was previously working full-time on another consumer internet app called [Noopsi](http://noopsi.com/). I'm truly lucky to have him, and I encourage you to get acquainted [on Twitter](http://twitter.com/jaymon). For ongoing updates about Plancast in general, you can follow [the official Twitter account](http://twitter.com/plancast) as well.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "facebooks-social-graph",
      "title": "Facebook's social graph",
      "excerpt": "The social graph on Facebook has been the company's biggest asset, but over time it has become perhaps its biggest liability as well. When users want...",
      "published": true,
      "publishedDate": "2009-07-31",
      "category": "technical",
      "readTime": 2,
      "tags": [],
      "body": "The social graph on Facebook has been the company's biggest asset, but over time it has become perhaps its biggest liability as well.\n\nWhen users want to find their friends online, they think of Facebook first. For many users, \"Facebook\" is nearly synonymous with \"social networking\". They wouldn't think of using any other \"social\" service because, after all, their friends are all on Facebook. As far as the social networking industry is concerned, this dedication constitutes a massive customer lock-in, because no matter how much better you can make a social application, you'll start off not only without the preestablished connections enjoyed by Facebook; you'll also be fighting against the reluctance of Facebook users to try an application outside of the Facebook ecosystem in uncharted territory where most of their friends do not exist.\n\nThe Facebook developer platform (which includes the ability to write widget-like applications for placement on Facebook.com, as well as the ability to extract data about users for integration into applications on other domains) narrows this gap only slightly. For all of Facebook's talk about wanting to open up, its platforms APIs and policies empower third-party developers with only so much data and user access. Compared to the power that Facebook wields as chief overseer of its data and users, outside developers can query just a sliver of its social graph. And of that sliver, they can only store certain data in certain ways for certain periods of time. The restrictions add up so that Facebook integration delivers but minor, complementary benefits to most third-party sites.\n\nTo break things down a bit, the platform can be divided into push and pull components. Many of the APIs are designed to let you pull data about Facebook's users and leverage that data in your applications. Others are designed to let you push data from your application back to Facebook, usually for sharing user activity with friends there. These push mechanisms are the most critical for most third-party developers, because users want to retain contact with their Facebook friends and share activity with them. The data you pull from Facebook about users is generally less interesting, if only because it's pretty generic. Unfortunately, the push mechanisms are pretty weak since they don't let you reliably send data to individual friends of users, whether through Facebook's proprietary messaging system or email notifications. Your best bet is to rather bluntly dump something into the homepage stream and pray that it catches enough friends' eyes to make an impact.\n\nAll of this is to say that Facebook still has a huge competitive advantage over other social networking companies (whether on-platform or off) because it controls a valuable social graph – and particularly the email addresses that come along with it. However, the social graph is not a divinely produced thing. And it's not a permanent, exclusive good. On the contrary, I believe the social graph is deteriorating on Facebook and starting to be reproduced elsewhere in better form.\n\nThe main problem is that people's real-world social graphs change often and automatically, while their virtual representations on Facebook change mostly uni-directionally and manually. In other words, friends come and go in real life; but on Facebook, they usually just come. Friend lists tend to get bloated over time because users have a harder time defriending each other virtually than in real life. And even if they are going to defriend each other virtually, it has to be a deliberative effort, unlike in real-life when you just stop seeing certain people.\n\nThis problem is particularly acute for Facebook, because its earliest adopters were college students or high school students who have undergone significant changes in their lives over the last few years. They no longer see many of the people who they once friended in school. And they aren't inclined to remove these friendships from Facebook because they're lazy, fatigued or simply too polite.\n\nThe ill effects of this discrepancy would have been tempered had Facebook stuck to its original value proposition of static profiles. However, Facebook has undergone a major shift from a static directory to a dynamic communication channel. This shift is embodied by its decision to remake its homepage into a Twitter-like stream of directly published content. When you open up Facebook these days, you're bombarded with little bits of information about your Facebook friends' lives. It's no longer primarily a place to browse people's profiles (and associated photos) like Wikipedia pages.\n\nDon't get me wrong, I love the \"real-time web\" as spawned by Twitter and advanced by FriendFeed. But Facebook has hoisted this dynamic paradigm onto a user base that didn't expect it, didn't ask for it, didn't prepare for it, and perhaps doesn't want it. \n\nI've already [discussed](http://www.techcrunch.com/2009/02/07/why-facebook-isnt-poised-to-steal-twitters-thunder/) why this last factor is such an issue. But assuming the idea of micro-sharing does grow on Facebook users, they haven't established the right audiences for it. Friendships haven't been made on the basis of content consumption; they were made first to simply acknowledge your friends and later to gain access to their profiles (once Facebook opened up for non-students and became a less trusting environment). Sure, the news feed was introduced rather early on and aggregated information about those who users decided to friend. But the inability to post content directly and immediately to all of your friends' news feeds created an important sense of distance between you and them – and made it easier to coexist on the site with those friends who weren't really your friends anymore, or those who you didn't ever care to hear from much.\n\nAs a content producer, my predefined social graph on Facebook makes me reluctant to publish there, because I don't feel as though my friends have indicated an interest to see my constant updates. The problem I have as a content consumer is just the flip-side: when I load up Facebook, I see content produced by people who I don't particularly want to hear about or from. \n\nFacebook has provided various ways to sort friends into lists and hide individuals from your stream, but these tools are daunting and perhaps ultimately futile. I spent 20 minutes alone last night organizing just my friends with first names that start with letters A-C. With almost 800 friends, I'm reluctant to keep going. And I imagine that most Facebook users don't even have the wherewithal to try in the first place.\n\nFacebook may try to address this content audience problem by introducing a Twitter-like follower model. The site already asks you when friending someone new whether you want to see that person's updates in your home stream. But users won't be doing this retroactively, and it adds complexity to an already complex site. Privacy and distribution controls simply aren't going to solve the problems of an over-encompassing social graph.\n\nWhat does this all mean? Well, Facebook's golden goose (the social graph) may not be so golden after all. It changes as users change. And it's not really even a singular thing. People have multiple social graphs; Facebook just tries to roughly represent them all by clumping them together. When it comes to profile access, you may want to leverage a different set of connections than when it comes to status message streams. Facebook may have to make a decision as to which particular social graph it wants to represent for its (constantly growing and diversifying) user base. It may not work for the company to be all things social for all people.\n\nIt also means that there's a massive opportunity for other social sites to give Facebook users a fresh start with fresh new social connections. I'm biased here, of course, since I'm working on social software. But this opportunity is seen in the rise of Twitter, which can attribute much of its success to the mere fact that it's *not* Facebook. When you sign up for Twitter, you can determine anew who you care about - whether that's your new friends or coworkers, or celebrities, businesses and media outlets. Facebook will no doubt remain a dominant social network for quite sometime, but it's dominance does not preclude the rise of other, independent social applications and services.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "worldly-developments",
      "title": "Worldly Developments",
      "excerpt": "I'm happy to announce some exciting news regarding my startup. I've officially incorporated as Worldly Developmentshttp://worldlydevelopments.com/...",
      "published": true,
      "publishedDate": "2009-06-21",
      "category": "technical",
      "readTime": 2,
      "tags": [],
      "body": "I'm happy to announce some exciting news regarding my startup. I've officially incorporated as [Worldly Developments](http://worldlydevelopments.com/) (yay for Delaware). Just a teaser of a website currently, but something I can print on my new business cards.\n\nThe reason for incorporation? I've raised some (micro-)seed funding from [fbFund](http://developers.facebook.com/fbFund.php). Since that requires a legit corporate bank account and the filing of other important paperwork, my mom has also assumed the role of Chief Financial Officer (employee #1! thanks Mom). \n\nFbFund isn't just an investment vehicle; starting this summer, it's also an incubation program in Palo Alto. So while I planned on moving to San Francisco earlier this month, that's been put off until the end of summer so I can enjoy the office space they've provided for us just off University Ave. It's one of the old Facebook offices, and all of the participating startups (~20) started moving their stuff in there just a few days ago.\n\nFrom now until mid-August I'll be participating in the fbFund program, which basically means hacking away at my application as I would anyway, except with additional support/mentorship/resources provided by Facebook and others. I've already met a fair number of the participants and organizers, and I must say, it's a refreshing change of pace to work around like-minded people again instead of coding solo in my bedroom or at the cafe.\n\nI also just got back from a quick trip to Japan and China as part of [GeeksOnAPlane](http://geeksonaplane.com/). Thanks to [Dave McClure](http://500hats.com) for bringing me along as a media partner of sorts (I relayed what we learned as a return guest writer for TechCrunch; see [these](http://www.techcrunch.com/2009/06/14/geeksaplane-briefing-on-the-chinese-tech-industry-at-startonomics-beijing/) [posts](http://www.techcrunch.com/2009/06/10/geeksonaplane-learnings-from-tokyo/) in particular). The people in the traveling group were amazing, both personally and professionally, as were the people we met along the way. If you haven't visited East Asia, I highly encourage you to do so.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "the-web-needs-its-own-app-store",
      "title": "The web needs its own App Store",
      "excerpt": "I've been thinking a bit about the Apple App Store recently, perhaps because I'm starting to think more about developing a mobile application that...",
      "published": true,
      "publishedDate": "2009-05-19",
      "category": "technical",
      "readTime": 2,
      "tags": [],
      "body": "I've been thinking a bit about the Apple App Store recently, perhaps because I'm starting to think more about developing a mobile application that will complement my site.\n\nOne of the amazing things about the App Store – perhaps the most amazing thing – is how easy Apple makes it for users to pay for apps with small amounts of money (micro-transactions, if you will). Are you a developer that wants to sell an application for just $.99 a pop? Easy. Just create your app, submit it to the store, and start earning money. You don't even have to worry about going the advertising route. The strategy instead is primarily about providing a free version of your app that then gets users warmed up for your paid version.\n\nApps in the App Store aren't that different from web apps. They are both internet-enabled at their core and provide rich communication and data retrieval services for their users. Sure, iPhone apps run in Objective C instead of Javascript, etc. But that doesn't matter to the regular user who decides to pay for an app on their iPhone. Both are apps either based, or quickly retrieved, from the cloud.\n\nOne of the problems that most internet startups face is the looming question of \"how the hell am I going to make money on my site even if it becomes successful?\" The internet industry is often mocked because it doesn't readily provide dependable answers, at least not ones that are not easily undermined by general economic factors.\n\nNo web developer wants to deface their sites with Adsense or banner ads. Online advertising is (with few exceptions) anti-consumer and difficult to do \"right\" (as in, design to really capture users' attention and generate modest rates of return). Wouldn't it be great if we could just design sites that sell like iPhone apps?\n\nImagine visiting a site's landing page, getting a description and some screenshots, and then seeing a button that says \"$1.99 for full access\". If, as a user, you could click on that button, enter a password, and then immediately have lifelong access to the site – I bet you'd do it. $1.99 is a price lots of people are willing to pay, even if there's a risk involved with trying out the site and then finding out you don't like it.\n\nI don't know nearly enough to understand why this hasn't happened, even though I've read a fair amount about how online micropayments for sites have been heralded for years but have never materialized. Apple has made things happen on the iPhone because they've created a closed system that makes it really easy for end users and developers alike. As a user, you give your credit card information to Apple once and they only charge it when it makes sense (since charging every micropayment individually would drive up transaction fees).\n\nWhy can't someone do this for the web? It seems to me we just need a company that a) people trust with their credit card information and transactions, and b) knows how to build the right distributed technology that will get adopted by developers. Google comes to mind since it's tight with developers and very trusted among users. Maybe browsers like Mozilla are better situated, however, since they control the \"device\" within which users browse the internet (just as Apple controls the iPhone). Then again, Google has its own shiny new browser (Chrome) so they'd have that going for them as well.\n\nSuch a payment system could revolutionize how websites are monetized - and consequently, how they are made. Businesses and their users would both stand to benefit greatly. Here's to hoping this happens as the web matures.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "narrowing-scope",
      "title": "Narrowing Scope",
      "excerpt": "Since my last post, I've narrowed the scope of my project down a great deal. The main reason was to make it more accessible to first-time users who...",
      "published": true,
      "publishedDate": "2009-05-18",
      "category": "article",
      "readTime": 2,
      "tags": [],
      "body": "Since my last post, I've narrowed the scope of my project down a great deal. The main reason was to make it more accessible to first-time users who would probably otherwise be overwhelmed by the number of things you could do on the site. There are related problems associated with trying to bite off too much from the very beginning:\n\n- It's hard to communicate the immediate value to users, especially ones that join the site before any of their friends\n\n- It's extremely difficult to predict what kind of service users are going to respond to, and which features in particular are necessary. Better to release something straightforward, see how users like it, and evolve by coupling user feedback with theory\n\n- When there's only one developer (me), it can feel overwhelming to build out a complicated system all at once. After awhile, there's a strong desire to launch *something* so it feels like you're making tangible progress. Milestones are key when you're the only one laying out your priorities\n\n- Once you've released something - even if it's only a smaller part of your broader vision - it's easier to explain to other people what you're building. It's no longer just about pie in the sky ideas - you can send them a link, have them see what it does, and then explain where you plan to take it.\n\n- There's an intrinsic value to building something simple. Users don't want to think too hard about how to use applications, and if your application only does one thing - and it does it well - they're more likely to come back.\n\n- When you release a web app these days, you need to market it effectively, otherwise you'll get drowned out by all the other options people have online. One effective way to market is to make your site compatible with the places people already visit, namely social networks like Twitter and Facebook (at least in my case). It's easiest to piggyback off of these sites if you create something particular that enhances them; then you can go from there and develop something more sophisticated that stands on its own.\n\nLast week I launched the narrowed site into private beta, inviting only a dozen or so people. I've already received a laundry list of features/change requests, and I'll start opening the site up more broadly when I feel as though I've addressed them adequately.\n\nI'm also happy to report that I've been chosen as a finalist for fbFund, which affords me some money to advertise on Facebook. More details and a list of the other finalists [here](http://www.techcrunch.com/2009/05/18/facebooks-fbfund-09-names-first-batch-of-winners).",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "the-unfolding-legacy-of-twitter-for-software-design",
      "title": "The unfolding legacy of Twitter for software design",
      "excerpt": "The immense amount of ink spilled for Twitter these days signals two main things. First, that web innovation in general is going through a...",
      "published": true,
      "publishedDate": "2009-03-30",
      "category": "technical",
      "readTime": 4,
      "tags": [],
      "body": "The immense amount of ink spilled for Twitter these days signals two main things. First, that web innovation in general is going through a transformative period, one in which we don't see a lot of breakout technologies because the industry is struggling to redefine itself in the wake of economic collapse and the exhaustion of innovation. Twitter stands out because it's a counter example to this trend, a company that's going mainstream and perplexing people all at the same time. It's a paradigm changer; people are simultaneously obsessed *with it* and confused *by it*.\n\nBoth the confusion and the obsession will pass in time, just as it did for Facebook, the last internet rockstar to emerge before Twitter. Facebook is no longer the buzz maker it was about two years ago and Twitter will no longer intrigue us two years from now. And like Facebook today, it's primary mark on the web landscape will have been made, even though it will remain a powerful force on the web and continue to innovate.\n\nThe making of this mark is the second reason why there's so much attention lavished on Twitter right now, especially by technology pundits who pay constant attention to the effects of breakout services on their peers and descendants. The mark is both simple and profound, and it consists of demonstrating the potency of so-called \"microblogging\" for the distribution of social information.\n\nFacebook may be credited with popularizing the \"news feed\" - a continually updated stream of information about people you care about - but Twitter boiled the news feed down to its essence. On Twitter, the news feed doesn't extract changes from secondary profiles and associated applications. It's not deducing news about your friends by passively monitoring their activity elsewhere, as the Facebook news feed did almost exclusively until very recently.\n\nNo, on Twitter, the users contribute directly into the news feed itself. The news feed is the main feature, not a method of surfacing the most contemporary information in a system. And the content that users add is very basic: simple strings of text no longer than 140-characters in length. Sure, Twitter could have allowed users to post images, movies and other types of data into the feed. But it's creators - partly restricted by the desire for all tweets to be SMS-compatible, and partly influenced by the legacy of blogging - kept things stripped down to their basics.\n\nTwitter remains a stunningly simple application. That's its strength, but the simplicity also creates an opportunity for other services to apply Twitter's model to other ends. Facebook most notably just appropriated Twitter's user experience with the redesign of its homepage. Apparently, Facebook thinks that the Twitter model (combined with the related FriendFeed model) is the best way for friends to exchange information of all types - not only status updates but links, images, videos, and more. And months prior to that, Yammer did something similar for the workplace by releasing an enterprise microblogging service.\n\nThis is just the start. Over the next few years, we are going to see social services across the spectrum appropriate and expand upon the basic functionality of Twitter, because there are needs that Twitter doesn't (and can't) fulfill, either onsite or through its API. Since all software is becoming social, expect the Twitterification of software in general.\n\nWhy is the Twitter way of communicating so powerful - and consequently, why will others borrow from it? Microblogging is passive, it's distributed, and it's easy. In other words, people can digest and respond to tweets as they please. There's no technological or sociological pressure for them to consume or act on information in ways that are disproportionate to their interest level. When you post a tweet, it gets blasted out to many recipients all at once, unlike email which is architecturally designed for a limited audience. And each tweet demands very little from its users - just a simple thought or observation.\n\nSo, Twitter has set the standard. It's currently proving that its model can appeal to mainstream audiences, who actually appear capable of grokking its utility (which was not always a given). But this is just the beginning - just as \"social networking\" features pervade services of all kinds these days, microblogging will also become ubiquitous - and it will assume different forms depending on the various needs at hand.",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "my-first-week-as-an-entrepreneur",
      "title": "My first week as an entrepreneur",
      "excerpt": "Today marks the completion of my first week working on my first startup/getting-the-lay-of-the-land. So, I thought it would be a good time to share...",
      "published": true,
      "publishedDate": "2009-03-22",
      "category": "technical",
      "readTime": 4,
      "tags": [],
      "body": "Today marks the completion of my first week working on [my first startup](/getting-the-lay-of-the-land). So, I thought it would be a good time to share some of the things I've learned already about what it's like to build your own web service from the ground up and as a full-time job. \n\nI'm going to try to provide this type of update on a regular basis for two main reasons: 1) so others can get a sense of what it's like to start a dot com these days, especially if they are thinking about doing it themselves; and 2) so I have a record to look back on later to see just where I've been and how my thoughts may have changed along the way.\n\nWhen I decided to strike out on my own, I expected that the hardest thing would be to keep up morale, both for myself and for anyone who joined me. It seems as though a lot of startups wither away because their founders give up hope on their ambitions. The threat of demoralization appears the greatest for startups that are trying to do something really new and innovative, something for which the market can't even indicate demand yet. Even with less radical ideas, there are always naysaying thoughts nagging away at the back of the entrepreneur's mind; for example, \"if this was such a good idea, why isn't someone else already doing it?\", \"if this were *even possible*, wouldn't others be already doing it?\", \"hasn't X and Y companies already kind of tried this?\", and \"couldn't company Z easily move into this space and wipe out the market opportunity for my fledgling startup?\".\n\nEven though it's only been a week, these are thoughts that I have had to handle carefully. On the one hand, it's necessary to constantly question the core value proposition of your startup. If you don't, then you're bound to build something that people don't need or want. Whether I'm in the shower or walking down the street, I'm frequently turning ideas over in my head, looking for weaknesses in them, and trying to expose poorly made assumptions that could turn into Achilles' heels. \n\nOn the other hand, a startup founder needs to be stubbornly optimistic, lest he or she succumb to the overwhelming number of (legitimate and illegitimate) doubts that may arise. As [the debate](http://www.techmeme.com/090322/p4#a090322p4) from this week over Zuckerberg's decision to look beyond user feedback brings into focus, its important for a founder to say \"yes\" even when (many) others say \"no\". If the man on the street dictated product design, [we'd all be driving Volvos](http://scobleizer.com/2009/03/21/why-facebook-has-never-listened-and-why-it-definitely-wont-start-now/). Great ideas (perhaps by definition if not just in general) shouldn't be easily appreciated until they've been executed, and sometimes not even in the short-run after they've been executed. An entrepreneur needs to internalize this belief and learn how to endure resistance and skepticism from those who don't readily share it.\n\nCurrently, I have a user base of just one, so I don't have to defy millions of faithful users when making product decisions. Mostly doubts arise when I've shared my ideas with friends, family and basically anyone willing to listen. Some of the time people get the value proposition instantly and it clearly resonates with them. It's immensely satisfying (despite what I just said above) when people respond with \"wow cool, that sounds exactly like something I would use.\" \n\nOther times, people scratch their heads and almost reflexively assume the role of devil's advocate: \"so this is kind of like a cross between X and Y websites...right?\" or \"shouldn't this just be a facebook app?\" (the modern day equivalent of calling someone's website idea a gizmo). It's important to listen to those who still need convincing, because there will be a lot more of them and their concerns usually inform your concerns. But it's equally important not to let their skepticism deflate your enthusiasm for the project. I've found that the best way to reassure myself in these situations is to think about what it might have been like for the founders of Google, Facebook, or Twitter to sit down at the beginning of their projects and get feedback from friends (\"why do we need another search engine?\", \"why would I want to put my personal information online and then tell a site who my friends are?\", and \"who cares if I'm brushing my teeth or watching the basketball game?\").\n\nKeeping up morale is particularly important when you're flying solo, because you don't have a cofounder who will constantly reassure you of your decision to follow a path to no certain end. Also, when you're at a regular job, you may not know if the company will succeed but you can be fairly certain that you will succeed in your delegated role. Your projects are usually well-defined and limited in scope, and as long as you get them done well, you have something to put on your resume and feel good about when you head home at night. \n\nBut when you're starting a company, the goals are not defined for you and you're wrapped up in the success or failure of the enterprise as a whole. If you spend two years working on a startup that ultimately falls through, I imagine it's a much greater personal burden than spending two years working at someone else's company that goes belly up.\n\nSo far, morale has been good for me. I've already experienced a bit of the \"rollercoaster\" effect that I've heard others describe, where emotions swing from high to low, and back to high – sometimes on an hourly basis. But my project is getting increasingly more exciting as it develops, even though the first weeks are all about baby steps (drawing mockups, conducting general research, checking out developer documentation, hacking together the first pieces of code, etc.).\n\nThere are also definite benefits to working for yourself - and from home, as I currently am. There's none of that (often unnecessary) pressure to please anyone but yourself, and I suffer less from *unhealthy* stress. Sometimes it feels a bit like I'm on vacation, but then I remember that I'm actually working longer and on weekends now. It just feels like vacation because the work is thoroughly pleasurable, at least so far. I also don't have to deal with the formalities of a normal job, like going into an office or taking breaks only when it makes sense for the organization as a whole. When you work for yourself, you can roll out of bed and immediately start getting stuff done. And if you feel like 2:30pm is the perfect time to take a break and hit the gym, you can do it without feeling like anyone's judging you for leaving in the middle of the day.\n\nSo that, in a rather large nutshell, is what I've experience so far. The prototype (codenamed \"Magellan\" - thanks [Jason](http://jasonnazar.com)) is coming together, and I hope to have the first version ready to share with friends and family by sometime in June. I also plan to start inviting people off the waiting list before soon after that.\n\nFor related reading from someone with much more experience than me, check out Paul Graham's essays, especially [this one](http://www.paulgraham.com/13sentences.html).",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    },
    {
      "slug": "getting-the-lay-of-the-land",
      "title": "Getting the lay of the land",
      "excerpt": "As Erick Schonfeld was kind enough to announce in a post on TechCrunch last week, I've left my position at TechCrunch to build and run my own startup.",
      "published": true,
      "publishedDate": "2009-03-17",
      "category": "technical",
      "readTime": 4,
      "tags": [],
      "body": "As Erick Schonfeld was kind enough to announce in [a post on TechCrunch](http://www.techcrunch.com/2009/03/10/hendrickson-were-gonna-miss-you/) last week, I've left my position at TechCrunch to build and run my own startup.\n\nIn many ways, this is what I intended to do all along, at least ever since I graduated from college almost two years ago. After joining TechCrunch in 2007 (as an intern anticipating that I'd only be there for a couple of months), I quickly realized that it would be a great place to lay the groundwork for my own venture. Working there keeps you in constant connection with the consumer internet technology scene, since you're always reading news and analysis (especially as a writer) and meeting people from all parts of the industry (PR, management, investment, development, etc.). Of course, it doesn't hurt to work for a brand that has great name recognition in the Valley, especially when you're returning from four years in Maine - basically Siberia, as far as people around here are concerned.\n\nI learned *a lot* about new media, internet technology, the culture around internet technology, and the inner workings of a startup during my time at TechCrunch - lessons that perhaps I'll explore in a later post. But now my sights are set on building a viable web service (and later, a profitable business) in a down economy...something that with any luck won't fall to [the wayside](http://www.techcrunch.com/tag/deadpool/) like so many of the startups I witnessed at TechCrunch. It's not going to be easy; in fact, I'm sure it's going to be one of the toughest things I ever try to pull off. The reassurance is that even if I fail, I will have learned and experienced much along the way.\n\nOk, so enough sappy reflection and introspection. What am I actually trying to build? Or as my friends and family keep asking, \"What's your website about?\"\n\nLet me start to answer that question with a description of how *the idea* for my startup came about. When I moved back to the Bay Area after living in a tightly knit community at Bowdoin, I had a new set of needs - most of them social. And like many needs, they could only be fulfilled by gathering information, not just once but on a continual basis. For example, I wanted to know:\n\n- Which of my friends now live in the Bay Area, and where exactly do they live and work? Do any of them live together?\n- Who do my friends know in the area that I might like to know? Do they live near me, and what kind of people are they looking to meet?\n- Where do my friends and their friends like to hang out on the weekends? Where do they go out to eat, and where do they do other things like going to the gym or perhaps volunteering in their free time?\n- What kind of plans do my friends and their friends have coming up in the near future? Are they thinking about going somewhere fun in San Francisco, or do any of them plan to go running around Palo Alto?\n- Are any of my friends actively involved in particular interest groups? Maybe a few get together with others to, say, play tennis or hit up the farmers market every weekend?\n\nQuestions like these are just begging to be answered by web services - especially by the type of those we've seen sprouting up in the past few years - because they all call for social information. Unfortunately, no web service adequately answered them in 2007, and still none does today. Sure, we have a plethora of sites intended to help you figure out what to do and where to go in your area. But those with the most data are not personal enough (i.e. they don't help you see the world through your existing connections), and those that *are* personal lack data, and the proper architecture for that data.\n\nSo, on a high level, I've set out to build a service that will answer the questions above and many others, a service that will help you engage more actively in your community. Call it a city or location-based social network if you want, but hopefully you'll see that those terms tend to misrepresent what I have in mind. I'm not looking to set up a site where you simply post a profile for others around you to view and write things on. I'm looking to set up a site that makes it easy for you to share information about who and what you know, and what you do, around the area in which you live. And conversely, a site that makes it uber-easy to digest useful local information shared by others.\n\nNo service does this to my satisfaction yet, but there are many related sites out there. After all, the desire to meet people and learn about what's going on around you isn't new. Here's a list of the names currently scribbled on my whiteboard:\n\n- **Loopt, Brightkite, et al.** - Services that detect your current location via a mobile device and then broadcast that location to your friends are all the rage right now. Perhaps we'll encroach more on each other's territory down the line, but I don't really care about helping users find out that their buddy is in the bar next door. I care more about providing you with social information from and about the area in which you live.\n\n- **Yelp, Goodrec, Citysearch** - Local review sites are great since they have a ton of information. But unfortunately, the information comes mainly from the public at large. Goodrec is a step towards personalization and simplification, but reviews and recommendations need to be even more socially focused (Whrrl had the right idea but [didn't execute successfully](http://www.pelago.com/blog/announcements/2009/03/whrrl-v20-has-arrived/).\n\n- **Facebook, MySpace, TheScene** - \"Traditional\" social networks, no matter how innovative, define themselves broadly. They aren't interested, for the most part, in local discovery. Look at how Facebook abandoned network pages. And new sites like TheScene ostensibly help you go local but simply aren't innovating much in how people publish and share information (for this, just look at the ripples that a deceptively simple service like Twitter has made).\n\n- **Match.com, Okcupid, Mixtt, Engage, etc.** - Dating sites are still holding down the fort when it comes to local discovery services. One problem - they suck, and they only serve one particular need (ok ok, it's a good need to serve, but even that need can be served better). It's great to see sites like Engage and Mixtt try to innovate by making things more social, but so far their efforts haven't worked out all that well.\n\n- **MeetUp, Upcoming** - Sites that help you meet up with interest groups and attend local events. Good. But who are all these strangers?\n\n- **Outside.in and other local news sites** - Local news is also good, but if there's anything that's easy to find online, it's news. And local news is often far more boring than national news, so it's an uphill battle to build a service that just revolves around this.\n\n- **Craigslist** - Amazingly great and amazingly bad at the same time. I'd like to think that this site isn't a testament to how the last 10+ years of web technology advancements don't matter when it comes to local classifieds.\n\nThose are the services on my mind as I start the process of creating something new and improved. What did I miss?",
      "createdDate": "2026-01-19",
      "updatedDate": "2026-01-19"
    }
  ]
}
